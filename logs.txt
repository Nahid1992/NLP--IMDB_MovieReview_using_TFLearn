Run id: IMDBreview_tflearn_run04
Log directory: /tmp/tflearn_logs/
---------------------------------
Training samples: 22500
Validation samples: 2500
--
Training Step: 1  | time: 5.156s
| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 00064/22500
Training Step: 2  | total loss: [1m[32m0.62387[0m[0m | time: 5.539s
| Adam | epoch: 001 | loss: 0.62387 - acc: 0.5344 -- iter: 00128/22500
Training Step: 3  | total loss: [1m[32m0.68078[0m[0m | time: 5.931s
| Adam | epoch: 001 | loss: 0.68078 - acc: 0.4551 -- iter: 00192/22500
Training Step: 4  | total loss: [1m[32m0.69002[0m[0m | time: 6.324s
| Adam | epoch: 001 | loss: 0.69002 - acc: 0.5122 -- iter: 00256/22500
Training Step: 5  | total loss: [1m[32m0.69225[0m[0m | time: 6.714s
| Adam | epoch: 001 | loss: 0.69225 - acc: 0.4929 -- iter: 00320/22500
Training Step: 6  | total loss: [1m[32m0.69278[0m[0m | time: 7.096s
| Adam | epoch: 001 | loss: 0.69278 - acc: 0.4874 -- iter: 00384/22500
Training Step: 7  | total loss: [1m[32m0.69307[0m[0m | time: 7.473s
| Adam | epoch: 001 | loss: 0.69307 - acc: 0.4856 -- iter: 00448/22500
Training Step: 8  | total loss: [1m[32m0.69302[0m[0m | time: 7.861s
| Adam | epoch: 001 | loss: 0.69302 - acc: 0.4849 -- iter: 00512/22500
Training Step: 9  | total loss: [1m[32m0.69304[0m[0m | time: 8.248s
| Adam | epoch: 001 | loss: 0.69304 - acc: 0.5094 -- iter: 00576/22500
Training Step: 10  | total loss: [1m[32m0.69349[0m[0m | time: 8.608s
| Adam | epoch: 001 | loss: 0.69349 - acc: 0.4422 -- iter: 00640/22500
Training Step: 11  | total loss: [1m[32m0.69330[0m[0m | time: 8.959s
| Adam | epoch: 001 | loss: 0.69330 - acc: 0.4770 -- iter: 00704/22500
Training Step: 12  | total loss: [1m[32m0.69334[0m[0m | time: 9.317s
| Adam | epoch: 001 | loss: 0.69334 - acc: 0.4803 -- iter: 00768/22500
Training Step: 13  | total loss: [1m[32m0.69314[0m[0m | time: 9.682s
| Adam | epoch: 001 | loss: 0.69314 - acc: 0.5155 -- iter: 00832/22500
Training Step: 14  | total loss: [1m[32m0.69332[0m[0m | time: 10.031s
| Adam | epoch: 001 | loss: 0.69332 - acc: 0.4517 -- iter: 00896/22500
Training Step: 15  | total loss: [1m[32m0.69323[0m[0m | time: 10.382s
| Adam | epoch: 001 | loss: 0.69323 - acc: 0.4767 -- iter: 00960/22500
Training Step: 16  | total loss: [1m[32m0.69300[0m[0m | time: 10.718s
| Adam | epoch: 001 | loss: 0.69300 - acc: 0.5147 -- iter: 01024/22500
Training Step: 17  | total loss: [1m[32m0.69321[0m[0m | time: 11.075s
| Adam | epoch: 001 | loss: 0.69321 - acc: 0.4757 -- iter: 01088/22500
Training Step: 18  | total loss: [1m[32m0.69306[0m[0m | time: 11.442s
| Adam | epoch: 001 | loss: 0.69306 - acc: 0.4733 -- iter: 01152/22500
Training Step: 19  | total loss: [1m[32m0.69292[0m[0m | time: 11.787s
| Adam | epoch: 001 | loss: 0.69292 - acc: 0.4770 -- iter: 01216/22500
Training Step: 20  | total loss: [1m[32m0.69277[0m[0m | time: 12.127s
| Adam | epoch: 001 | loss: 0.69277 - acc: 0.4844 -- iter: 01280/22500
Training Step: 21  | total loss: [1m[32m0.69266[0m[0m | time: 12.470s
| Adam | epoch: 001 | loss: 0.69266 - acc: 0.5086 -- iter: 01344/22500
Training Step: 22  | total loss: [1m[32m0.69225[0m[0m | time: 12.800s
| Adam | epoch: 001 | loss: 0.69225 - acc: 0.5248 -- iter: 01408/22500
Training Step: 23  | total loss: [1m[32m0.69206[0m[0m | time: 13.139s
| Adam | epoch: 001 | loss: 0.69206 - acc: 0.5221 -- iter: 01472/22500
Training Step: 24  | total loss: [1m[32m0.69300[0m[0m | time: 13.479s
| Adam | epoch: 001 | loss: 0.69300 - acc: 0.5159 -- iter: 01536/22500
Training Step: 25  | total loss: [1m[32m0.69290[0m[0m | time: 13.818s
| Adam | epoch: 001 | loss: 0.69290 - acc: 0.5201 -- iter: 01600/22500
Training Step: 26  | total loss: [1m[32m0.69083[0m[0m | time: 14.154s
| Adam | epoch: 001 | loss: 0.69083 - acc: 0.5396 -- iter: 01664/22500
Training Step: 27  | total loss: [1m[32m0.68806[0m[0m | time: 14.490s
| Adam | epoch: 001 | loss: 0.68806 - acc: 0.5415 -- iter: 01728/22500
Training Step: 28  | total loss: [1m[32m0.68518[0m[0m | time: 14.823s
| Adam | epoch: 001 | loss: 0.68518 - acc: 0.5819 -- iter: 01792/22500
Training Step: 29  | total loss: [1m[32m0.68113[0m[0m | time: 15.178s
| Adam | epoch: 001 | loss: 0.68113 - acc: 0.5810 -- iter: 01856/22500
Training Step: 30  | total loss: [1m[32m0.68842[0m[0m | time: 15.543s
| Adam | epoch: 001 | loss: 0.68842 - acc: 0.5840 -- iter: 01920/22500
Training Step: 31  | total loss: [1m[32m0.66559[0m[0m | time: 15.875s
| Adam | epoch: 001 | loss: 0.66559 - acc: 0.6115 -- iter: 01984/22500
Training Step: 32  | total loss: [1m[32m0.65095[0m[0m | time: 16.211s
| Adam | epoch: 001 | loss: 0.65095 - acc: 0.6391 -- iter: 02048/22500
Training Step: 33  | total loss: [1m[32m0.62386[0m[0m | time: 16.554s
| Adam | epoch: 001 | loss: 0.62386 - acc: 0.6772 -- iter: 02112/22500
Training Step: 34  | total loss: [1m[32m0.65944[0m[0m | time: 16.889s
| Adam | epoch: 001 | loss: 0.65944 - acc: 0.6493 -- iter: 02176/22500
Training Step: 35  | total loss: [1m[32m0.66469[0m[0m | time: 17.223s
| Adam | epoch: 001 | loss: 0.66469 - acc: 0.6442 -- iter: 02240/22500
Training Step: 36  | total loss: [1m[32m0.66932[0m[0m | time: 17.566s
| Adam | epoch: 001 | loss: 0.66932 - acc: 0.6339 -- iter: 02304/22500
Training Step: 37  | total loss: [1m[32m0.67246[0m[0m | time: 17.904s
| Adam | epoch: 001 | loss: 0.67246 - acc: 0.6227 -- iter: 02368/22500
Training Step: 38  | total loss: [1m[32m0.67618[0m[0m | time: 18.237s
| Adam | epoch: 001 | loss: 0.67618 - acc: 0.6171 -- iter: 02432/22500
Training Step: 39  | total loss: [1m[32m0.66815[0m[0m | time: 18.573s
| Adam | epoch: 001 | loss: 0.66815 - acc: 0.6216 -- iter: 02496/22500
Training Step: 40  | total loss: [1m[32m0.67146[0m[0m | time: 18.922s
| Adam | epoch: 001 | loss: 0.67146 - acc: 0.6076 -- iter: 02560/22500
Training Step: 41  | total loss: [1m[32m0.66902[0m[0m | time: 19.280s
| Adam | epoch: 001 | loss: 0.66902 - acc: 0.5964 -- iter: 02624/22500
Training Step: 42  | total loss: [1m[32m0.66988[0m[0m | time: 19.617s
| Adam | epoch: 001 | loss: 0.66988 - acc: 0.5847 -- iter: 02688/22500
Training Step: 43  | total loss: [1m[32m0.66841[0m[0m | time: 19.946s
| Adam | epoch: 001 | loss: 0.66841 - acc: 0.5835 -- iter: 02752/22500
Training Step: 44  | total loss: [1m[32m0.66579[0m[0m | time: 20.280s
| Adam | epoch: 001 | loss: 0.66579 - acc: 0.5826 -- iter: 02816/22500
Training Step: 45  | total loss: [1m[32m0.65742[0m[0m | time: 20.613s
| Adam | epoch: 001 | loss: 0.65742 - acc: 0.5951 -- iter: 02880/22500
Training Step: 46  | total loss: [1m[32m0.66806[0m[0m | time: 20.948s
| Adam | epoch: 001 | loss: 0.66806 - acc: 0.5897 -- iter: 02944/22500
Training Step: 47  | total loss: [1m[32m0.68948[0m[0m | time: 21.279s
| Adam | epoch: 001 | loss: 0.68948 - acc: 0.5750 -- iter: 03008/22500
Training Step: 48  | total loss: [1m[32m0.68149[0m[0m | time: 21.621s
| Adam | epoch: 001 | loss: 0.68149 - acc: 0.5956 -- iter: 03072/22500
Training Step: 49  | total loss: [1m[32m0.67186[0m[0m | time: 21.953s
| Adam | epoch: 001 | loss: 0.67186 - acc: 0.6200 -- iter: 03136/22500
Training Step: 50  | total loss: [1m[32m0.67277[0m[0m | time: 22.280s
| Adam | epoch: 001 | loss: 0.67277 - acc: 0.6207 -- iter: 03200/22500
Training Step: 51  | total loss: [1m[32m0.66825[0m[0m | time: 22.630s
| Adam | epoch: 001 | loss: 0.66825 - acc: 0.6238 -- iter: 03264/22500
Training Step: 52  | total loss: [1m[32m0.65772[0m[0m | time: 22.992s
| Adam | epoch: 001 | loss: 0.65772 - acc: 0.6333 -- iter: 03328/22500
Training Step: 53  | total loss: [1m[32m0.65623[0m[0m | time: 23.322s
| Adam | epoch: 001 | loss: 0.65623 - acc: 0.6344 -- iter: 03392/22500
Training Step: 54  | total loss: [1m[32m0.66088[0m[0m | time: 23.653s
| Adam | epoch: 001 | loss: 0.66088 - acc: 0.6376 -- iter: 03456/22500
Training Step: 55  | total loss: [1m[32m0.65753[0m[0m | time: 23.983s
| Adam | epoch: 001 | loss: 0.65753 - acc: 0.6447 -- iter: 03520/22500
Training Step: 56  | total loss: [1m[32m0.65874[0m[0m | time: 24.319s
| Adam | epoch: 001 | loss: 0.65874 - acc: 0.6441 -- iter: 03584/22500
Training Step: 57  | total loss: [1m[32m0.66178[0m[0m | time: 24.708s
| Adam | epoch: 001 | loss: 0.66178 - acc: 0.6350 -- iter: 03648/22500
Training Step: 58  | total loss: [1m[32m0.66018[0m[0m | time: 25.081s
| Adam | epoch: 001 | loss: 0.66018 - acc: 0.6443 -- iter: 03712/22500
Training Step: 59  | total loss: [1m[32m0.65018[0m[0m | time: 25.452s
| Adam | epoch: 001 | loss: 0.65018 - acc: 0.6564 -- iter: 03776/22500
Training Step: 60  | total loss: [1m[32m0.65202[0m[0m | time: 25.816s
| Adam | epoch: 001 | loss: 0.65202 - acc: 0.6646 -- iter: 03840/22500
Training Step: 61  | total loss: [1m[32m0.64864[0m[0m | time: 26.182s
| Adam | epoch: 001 | loss: 0.64864 - acc: 0.6635 -- iter: 03904/22500
Training Step: 62  | total loss: [1m[32m0.64586[0m[0m | time: 26.544s
| Adam | epoch: 001 | loss: 0.64586 - acc: 0.6747 -- iter: 03968/22500
Training Step: 63  | total loss: [1m[32m0.64295[0m[0m | time: 26.913s
| Adam | epoch: 001 | loss: 0.64295 - acc: 0.6723 -- iter: 04032/22500
Training Step: 64  | total loss: [1m[32m0.64235[0m[0m | time: 27.309s
| Adam | epoch: 001 | loss: 0.64235 - acc: 0.6781 -- iter: 04096/22500
Training Step: 65  | total loss: [1m[32m0.64179[0m[0m | time: 27.672s
| Adam | epoch: 001 | loss: 0.64179 - acc: 0.6754 -- iter: 04160/22500
Training Step: 66  | total loss: [1m[32m0.63785[0m[0m | time: 28.035s
| Adam | epoch: 001 | loss: 0.63785 - acc: 0.6769 -- iter: 04224/22500
Training Step: 67  | total loss: [1m[32m0.63419[0m[0m | time: 28.398s
| Adam | epoch: 001 | loss: 0.63419 - acc: 0.6838 -- iter: 04288/22500
Training Step: 68  | total loss: [1m[32m0.62991[0m[0m | time: 28.757s
| Adam | epoch: 001 | loss: 0.62991 - acc: 0.6898 -- iter: 04352/22500
Training Step: 69  | total loss: [1m[32m0.62141[0m[0m | time: 29.120s
| Adam | epoch: 001 | loss: 0.62141 - acc: 0.6950 -- iter: 04416/22500
Training Step: 70  | total loss: [1m[32m0.62601[0m[0m | time: 29.478s
| Adam | epoch: 001 | loss: 0.62601 - acc: 0.6905 -- iter: 04480/22500
Training Step: 71  | total loss: [1m[32m0.62062[0m[0m | time: 29.834s
| Adam | epoch: 001 | loss: 0.62062 - acc: 0.6955 -- iter: 04544/22500
Training Step: 72  | total loss: [1m[32m0.62518[0m[0m | time: 30.195s
| Adam | epoch: 001 | loss: 0.62518 - acc: 0.6911 -- iter: 04608/22500
Training Step: 73  | total loss: [1m[32m0.62577[0m[0m | time: 30.553s
| Adam | epoch: 001 | loss: 0.62577 - acc: 0.6838 -- iter: 04672/22500
Training Step: 74  | total loss: [1m[32m0.62580[0m[0m | time: 30.938s
| Adam | epoch: 001 | loss: 0.62580 - acc: 0.6910 -- iter: 04736/22500
Training Step: 75  | total loss: [1m[32m0.62641[0m[0m | time: 31.328s
| Adam | epoch: 001 | loss: 0.62641 - acc: 0.6906 -- iter: 04800/22500
Training Step: 76  | total loss: [1m[32m0.62416[0m[0m | time: 31.691s
| Adam | epoch: 001 | loss: 0.62416 - acc: 0.7037 -- iter: 04864/22500
Training Step: 77  | total loss: [1m[32m0.62199[0m[0m | time: 32.055s
| Adam | epoch: 001 | loss: 0.62199 - acc: 0.6954 -- iter: 04928/22500
Training Step: 78  | total loss: [1m[32m0.62659[0m[0m | time: 32.416s
| Adam | epoch: 001 | loss: 0.62659 - acc: 0.6831 -- iter: 04992/22500
Training Step: 79  | total loss: [1m[32m0.62457[0m[0m | time: 32.780s
| Adam | epoch: 001 | loss: 0.62457 - acc: 0.6884 -- iter: 05056/22500
Training Step: 80  | total loss: [1m[32m0.63370[0m[0m | time: 33.150s
| Adam | epoch: 001 | loss: 0.63370 - acc: 0.6787 -- iter: 05120/22500
Training Step: 81  | total loss: [1m[32m0.63309[0m[0m | time: 33.512s
| Adam | epoch: 001 | loss: 0.63309 - acc: 0.6812 -- iter: 05184/22500
Training Step: 82  | total loss: [1m[32m0.63520[0m[0m | time: 33.878s
| Adam | epoch: 001 | loss: 0.63520 - acc: 0.6803 -- iter: 05248/22500
Training Step: 83  | total loss: [1m[32m0.63293[0m[0m | time: 34.245s
| Adam | epoch: 001 | loss: 0.63293 - acc: 0.6825 -- iter: 05312/22500
Training Step: 84  | total loss: [1m[32m0.64255[0m[0m | time: 34.613s
| Adam | epoch: 001 | loss: 0.64255 - acc: 0.6784 -- iter: 05376/22500
Training Step: 85  | total loss: [1m[32m0.65297[0m[0m | time: 35.034s
| Adam | epoch: 001 | loss: 0.65297 - acc: 0.6715 -- iter: 05440/22500
Training Step: 86  | total loss: [1m[32m0.64859[0m[0m | time: 35.425s
| Adam | epoch: 001 | loss: 0.64859 - acc: 0.6746 -- iter: 05504/22500
Training Step: 87  | total loss: [1m[32m0.64858[0m[0m | time: 35.852s
| Adam | epoch: 001 | loss: 0.64858 - acc: 0.6743 -- iter: 05568/22500
Training Step: 88  | total loss: [1m[32m0.64498[0m[0m | time: 36.213s
| Adam | epoch: 001 | loss: 0.64498 - acc: 0.6757 -- iter: 05632/22500
Training Step: 89  | total loss: [1m[32m0.64264[0m[0m | time: 36.574s
| Adam | epoch: 001 | loss: 0.64264 - acc: 0.6784 -- iter: 05696/22500
Training Step: 90  | total loss: [1m[32m0.64823[0m[0m | time: 36.937s
| Adam | epoch: 001 | loss: 0.64823 - acc: 0.6684 -- iter: 05760/22500
Training Step: 91  | total loss: [1m[32m0.64439[0m[0m | time: 37.299s
| Adam | epoch: 001 | loss: 0.64439 - acc: 0.6719 -- iter: 05824/22500
Training Step: 92  | total loss: [1m[32m0.64499[0m[0m | time: 37.663s
| Adam | epoch: 001 | loss: 0.64499 - acc: 0.6656 -- iter: 05888/22500
Training Step: 93  | total loss: [1m[32m0.64264[0m[0m | time: 38.028s
| Adam | epoch: 001 | loss: 0.64264 - acc: 0.6678 -- iter: 05952/22500
Training Step: 94  | total loss: [1m[32m0.63657[0m[0m | time: 38.384s
| Adam | epoch: 001 | loss: 0.63657 - acc: 0.6807 -- iter: 06016/22500
Training Step: 95  | total loss: [1m[32m0.63139[0m[0m | time: 38.742s
| Adam | epoch: 001 | loss: 0.63139 - acc: 0.6908 -- iter: 06080/22500
Training Step: 96  | total loss: [1m[32m0.62712[0m[0m | time: 39.105s
| Adam | epoch: 001 | loss: 0.62712 - acc: 0.6998 -- iter: 06144/22500
Training Step: 97  | total loss: [1m[32m0.62210[0m[0m | time: 39.481s
| Adam | epoch: 001 | loss: 0.62210 - acc: 0.7064 -- iter: 06208/22500
Training Step: 98  | total loss: [1m[32m0.61857[0m[0m | time: 39.872s
| Adam | epoch: 001 | loss: 0.61857 - acc: 0.7076 -- iter: 06272/22500
Training Step: 99  | total loss: [1m[32m0.62413[0m[0m | time: 40.240s
| Adam | epoch: 001 | loss: 0.62413 - acc: 0.7009 -- iter: 06336/22500
Training Step: 100  | total loss: [1m[32m0.62050[0m[0m | time: 40.605s
| Adam | epoch: 001 | loss: 0.62050 - acc: 0.7043 -- iter: 06400/22500
Training Step: 101  | total loss: [1m[32m0.62062[0m[0m | time: 40.966s
| Adam | epoch: 001 | loss: 0.62062 - acc: 0.7057 -- iter: 06464/22500
Training Step: 102  | total loss: [1m[32m0.61203[0m[0m | time: 41.325s
| Adam | epoch: 001 | loss: 0.61203 - acc: 0.7133 -- iter: 06528/22500
Training Step: 103  | total loss: [1m[32m0.60370[0m[0m | time: 41.688s
| Adam | epoch: 001 | loss: 0.60370 - acc: 0.7216 -- iter: 06592/22500
Training Step: 104  | total loss: [1m[32m0.60076[0m[0m | time: 42.046s
| Adam | epoch: 001 | loss: 0.60076 - acc: 0.7260 -- iter: 06656/22500
Training Step: 105  | total loss: [1m[32m0.59819[0m[0m | time: 42.410s
| Adam | epoch: 001 | loss: 0.59819 - acc: 0.7237 -- iter: 06720/22500
Training Step: 106  | total loss: [1m[32m0.59921[0m[0m | time: 42.774s
| Adam | epoch: 001 | loss: 0.59921 - acc: 0.7217 -- iter: 06784/22500
Training Step: 107  | total loss: [1m[32m0.59966[0m[0m | time: 43.158s
| Adam | epoch: 001 | loss: 0.59966 - acc: 0.7214 -- iter: 06848/22500
Training Step: 108  | total loss: [1m[32m0.59750[0m[0m | time: 43.542s
| Adam | epoch: 001 | loss: 0.59750 - acc: 0.7274 -- iter: 06912/22500
Training Step: 109  | total loss: [1m[32m0.59560[0m[0m | time: 43.945s
| Adam | epoch: 001 | loss: 0.59560 - acc: 0.7234 -- iter: 06976/22500
Training Step: 110  | total loss: [1m[32m0.58292[0m[0m | time: 44.309s
| Adam | epoch: 001 | loss: 0.58292 - acc: 0.7370 -- iter: 07040/22500
Training Step: 111  | total loss: [1m[32m0.57990[0m[0m | time: 44.674s
| Adam | epoch: 001 | loss: 0.57990 - acc: 0.7367 -- iter: 07104/22500
Training Step: 112  | total loss: [1m[32m0.57193[0m[0m | time: 45.036s
| Adam | epoch: 001 | loss: 0.57193 - acc: 0.7412 -- iter: 07168/22500
Training Step: 113  | total loss: [1m[32m0.57103[0m[0m | time: 45.418s
| Adam | epoch: 001 | loss: 0.57103 - acc: 0.7405 -- iter: 07232/22500
Training Step: 114  | total loss: [1m[32m0.57304[0m[0m | time: 45.774s
| Adam | epoch: 001 | loss: 0.57304 - acc: 0.7399 -- iter: 07296/22500
Training Step: 115  | total loss: [1m[32m0.57147[0m[0m | time: 46.197s
| Adam | epoch: 001 | loss: 0.57147 - acc: 0.7393 -- iter: 07360/22500
Training Step: 116  | total loss: [1m[32m0.57256[0m[0m | time: 46.626s
| Adam | epoch: 001 | loss: 0.57256 - acc: 0.7420 -- iter: 07424/22500
Training Step: 117  | total loss: [1m[32m0.57279[0m[0m | time: 47.050s
| Adam | epoch: 001 | loss: 0.57279 - acc: 0.7412 -- iter: 07488/22500
Training Step: 118  | total loss: [1m[32m0.57150[0m[0m | time: 47.477s
| Adam | epoch: 001 | loss: 0.57150 - acc: 0.7452 -- iter: 07552/22500
Training Step: 119  | total loss: [1m[32m0.57223[0m[0m | time: 47.917s
| Adam | epoch: 001 | loss: 0.57223 - acc: 0.7379 -- iter: 07616/22500
Training Step: 120  | total loss: [1m[32m0.57771[0m[0m | time: 48.360s
| Adam | epoch: 001 | loss: 0.57771 - acc: 0.7297 -- iter: 07680/22500
Training Step: 121  | total loss: [1m[32m0.57746[0m[0m | time: 48.785s
| Adam | epoch: 001 | loss: 0.57746 - acc: 0.7286 -- iter: 07744/22500
Training Step: 122  | total loss: [1m[32m0.56647[0m[0m | time: 49.207s
| Adam | epoch: 001 | loss: 0.56647 - acc: 0.7339 -- iter: 07808/22500
Training Step: 123  | total loss: [1m[32m0.56474[0m[0m | time: 49.632s
| Adam | epoch: 001 | loss: 0.56474 - acc: 0.7386 -- iter: 07872/22500
Training Step: 124  | total loss: [1m[32m0.56279[0m[0m | time: 50.053s
| Adam | epoch: 001 | loss: 0.56279 - acc: 0.7444 -- iter: 07936/22500
Training Step: 125  | total loss: [1m[32m0.55442[0m[0m | time: 50.475s
| Adam | epoch: 001 | loss: 0.55442 - acc: 0.7512 -- iter: 08000/22500
Training Step: 126  | total loss: [1m[32m0.56593[0m[0m | time: 50.899s
| Adam | epoch: 001 | loss: 0.56593 - acc: 0.7464 -- iter: 08064/22500
Training Step: 127  | total loss: [1m[32m0.58728[0m[0m | time: 51.318s
| Adam | epoch: 001 | loss: 0.58728 - acc: 0.7390 -- iter: 08128/22500
Training Step: 128  | total loss: [1m[32m0.61016[0m[0m | time: 51.746s
| Adam | epoch: 001 | loss: 0.61016 - acc: 0.7307 -- iter: 08192/22500
Training Step: 129  | total loss: [1m[32m0.62321[0m[0m | time: 52.171s
| Adam | epoch: 001 | loss: 0.62321 - acc: 0.7264 -- iter: 08256/22500
Training Step: 130  | total loss: [1m[32m0.61932[0m[0m | time: 52.594s
| Adam | epoch: 001 | loss: 0.61932 - acc: 0.7287 -- iter: 08320/22500
Training Step: 131  | total loss: [1m[32m0.63060[0m[0m | time: 53.058s
| Adam | epoch: 001 | loss: 0.63060 - acc: 0.7231 -- iter: 08384/22500
Training Step: 132  | total loss: [1m[32m0.62849[0m[0m | time: 53.504s
| Adam | epoch: 001 | loss: 0.62849 - acc: 0.7258 -- iter: 08448/22500
Training Step: 133  | total loss: [1m[32m0.63696[0m[0m | time: 53.931s
| Adam | epoch: 001 | loss: 0.63696 - acc: 0.7204 -- iter: 08512/22500
Training Step: 134  | total loss: [1m[32m0.63466[0m[0m | time: 54.368s
| Adam | epoch: 001 | loss: 0.63466 - acc: 0.7202 -- iter: 08576/22500
Training Step: 135  | total loss: [1m[32m0.63469[0m[0m | time: 54.804s
| Adam | epoch: 001 | loss: 0.63469 - acc: 0.7185 -- iter: 08640/22500
Training Step: 136  | total loss: [1m[32m0.63986[0m[0m | time: 55.228s
| Adam | epoch: 001 | loss: 0.63986 - acc: 0.7123 -- iter: 08704/22500
Training Step: 137  | total loss: [1m[32m0.64912[0m[0m | time: 55.648s
| Adam | epoch: 001 | loss: 0.64912 - acc: 0.7020 -- iter: 08768/22500
Training Step: 138  | total loss: [1m[32m0.63956[0m[0m | time: 56.070s
| Adam | epoch: 001 | loss: 0.63956 - acc: 0.7068 -- iter: 08832/22500
Training Step: 139  | total loss: [1m[32m0.64666[0m[0m | time: 56.499s
| Adam | epoch: 001 | loss: 0.64666 - acc: 0.6955 -- iter: 08896/22500
Training Step: 140  | total loss: [1m[32m0.65239[0m[0m | time: 56.921s
| Adam | epoch: 001 | loss: 0.65239 - acc: 0.6853 -- iter: 08960/22500
Training Step: 141  | total loss: [1m[32m0.65556[0m[0m | time: 57.349s
| Adam | epoch: 001 | loss: 0.65556 - acc: 0.6746 -- iter: 09024/22500
Training Step: 142  | total loss: [1m[32m0.65569[0m[0m | time: 57.795s
| Adam | epoch: 001 | loss: 0.65569 - acc: 0.6681 -- iter: 09088/22500
Training Step: 143  | total loss: [1m[32m0.65279[0m[0m | time: 58.244s
| Adam | epoch: 001 | loss: 0.65279 - acc: 0.6684 -- iter: 09152/22500
Training Step: 144  | total loss: [1m[32m0.64995[0m[0m | time: 58.668s
| Adam | epoch: 001 | loss: 0.64995 - acc: 0.6735 -- iter: 09216/22500
Training Step: 145  | total loss: [1m[32m0.64544[0m[0m | time: 59.090s
| Adam | epoch: 001 | loss: 0.64544 - acc: 0.6811 -- iter: 09280/22500
Training Step: 146  | total loss: [1m[32m0.63668[0m[0m | time: 59.515s
| Adam | epoch: 001 | loss: 0.63668 - acc: 0.6974 -- iter: 09344/22500
Training Step: 147  | total loss: [1m[32m0.63537[0m[0m | time: 59.933s
| Adam | epoch: 001 | loss: 0.63537 - acc: 0.6980 -- iter: 09408/22500
Training Step: 148  | total loss: [1m[32m0.63109[0m[0m | time: 60.376s
| Adam | epoch: 001 | loss: 0.63109 - acc: 0.7094 -- iter: 09472/22500
Training Step: 149  | total loss: [1m[32m0.62577[0m[0m | time: 60.798s
| Adam | epoch: 001 | loss: 0.62577 - acc: 0.7119 -- iter: 09536/22500
Training Step: 150  | total loss: [1m[32m0.62267[0m[0m | time: 61.216s
| Adam | epoch: 001 | loss: 0.62267 - acc: 0.7110 -- iter: 09600/22500
Training Step: 151  | total loss: [1m[32m0.62668[0m[0m | time: 61.631s
| Adam | epoch: 001 | loss: 0.62668 - acc: 0.6993 -- iter: 09664/22500
Training Step: 152  | total loss: [1m[32m0.62255[0m[0m | time: 62.047s
| Adam | epoch: 001 | loss: 0.62255 - acc: 0.7028 -- iter: 09728/22500
Training Step: 153  | total loss: [1m[32m0.62006[0m[0m | time: 62.527s
| Adam | epoch: 001 | loss: 0.62006 - acc: 0.7091 -- iter: 09792/22500
Training Step: 154  | total loss: [1m[32m0.61796[0m[0m | time: 63.084s
| Adam | epoch: 001 | loss: 0.61796 - acc: 0.7132 -- iter: 09856/22500
Training Step: 155  | total loss: [1m[32m0.61447[0m[0m | time: 63.547s
| Adam | epoch: 001 | loss: 0.61447 - acc: 0.7184 -- iter: 09920/22500
Training Step: 156  | total loss: [1m[32m0.61280[0m[0m | time: 63.963s
| Adam | epoch: 001 | loss: 0.61280 - acc: 0.7216 -- iter: 09984/22500
Training Step: 157  | total loss: [1m[32m0.60674[0m[0m | time: 64.405s
| Adam | epoch: 001 | loss: 0.60674 - acc: 0.7260 -- iter: 10048/22500
Training Step: 158  | total loss: [1m[32m0.61554[0m[0m | time: 64.723s
| Adam | epoch: 001 | loss: 0.61554 - acc: 0.7143 -- iter: 10112/22500
Training Step: 159  | total loss: [1m[32m0.61384[0m[0m | time: 65.155s
| Adam | epoch: 001 | loss: 0.61384 - acc: 0.7179 -- iter: 10176/22500
Training Step: 160  | total loss: [1m[32m0.61200[0m[0m | time: 65.584s
| Adam | epoch: 001 | loss: 0.61200 - acc: 0.7227 -- iter: 10240/22500
Training Step: 161  | total loss: [1m[32m0.63165[0m[0m | time: 66.004s
| Adam | epoch: 001 | loss: 0.63165 - acc: 0.7067 -- iter: 10304/22500
Training Step: 162  | total loss: [1m[32m0.63361[0m[0m | time: 66.434s
| Adam | epoch: 001 | loss: 0.63361 - acc: 0.7063 -- iter: 10368/22500
Training Step: 163  | total loss: [1m[32m0.63098[0m[0m | time: 66.858s
| Adam | epoch: 001 | loss: 0.63098 - acc: 0.7060 -- iter: 10432/22500
Training Step: 164  | total loss: [1m[32m0.64214[0m[0m | time: 67.278s
| Adam | epoch: 001 | loss: 0.64214 - acc: 0.6932 -- iter: 10496/22500
Training Step: 165  | total loss: [1m[32m0.64238[0m[0m | time: 67.725s
| Adam | epoch: 001 | loss: 0.64238 - acc: 0.6926 -- iter: 10560/22500
Training Step: 166  | total loss: [1m[32m0.64160[0m[0m | time: 68.170s
| Adam | epoch: 001 | loss: 0.64160 - acc: 0.6937 -- iter: 10624/22500
Training Step: 167  | total loss: [1m[32m0.64417[0m[0m | time: 68.582s
| Adam | epoch: 001 | loss: 0.64417 - acc: 0.6915 -- iter: 10688/22500
Training Step: 168  | total loss: [1m[32m0.64664[0m[0m | time: 68.999s
| Adam | epoch: 001 | loss: 0.64664 - acc: 0.6880 -- iter: 10752/22500
Training Step: 169  | total loss: [1m[32m0.66021[0m[0m | time: 69.415s
| Adam | epoch: 001 | loss: 0.66021 - acc: 0.6707 -- iter: 10816/22500
Training Step: 170  | total loss: [1m[32m0.66083[0m[0m | time: 69.922s
| Adam | epoch: 001 | loss: 0.66083 - acc: 0.6677 -- iter: 10880/22500
Training Step: 171  | total loss: [1m[32m0.65753[0m[0m | time: 70.366s
| Adam | epoch: 001 | loss: 0.65753 - acc: 0.6697 -- iter: 10944/22500
Training Step: 172  | total loss: [1m[32m0.66167[0m[0m | time: 70.788s
| Adam | epoch: 001 | loss: 0.66167 - acc: 0.6621 -- iter: 11008/22500
Training Step: 173  | total loss: [1m[32m0.65592[0m[0m | time: 71.257s
| Adam | epoch: 001 | loss: 0.65592 - acc: 0.6678 -- iter: 11072/22500
Training Step: 174  | total loss: [1m[32m0.65958[0m[0m | time: 71.742s
| Adam | epoch: 001 | loss: 0.65958 - acc: 0.6572 -- iter: 11136/22500
Training Step: 175  | total loss: [1m[32m0.66460[0m[0m | time: 72.278s
| Adam | epoch: 001 | loss: 0.66460 - acc: 0.6446 -- iter: 11200/22500
Training Step: 176  | total loss: [1m[32m0.66897[0m[0m | time: 72.744s
| Adam | epoch: 001 | loss: 0.66897 - acc: 0.6317 -- iter: 11264/22500
Training Step: 177  | total loss: [1m[32m0.67215[0m[0m | time: 73.219s
| Adam | epoch: 001 | loss: 0.67215 - acc: 0.6201 -- iter: 11328/22500
Training Step: 178  | total loss: [1m[32m0.67058[0m[0m | time: 73.774s
| Adam | epoch: 001 | loss: 0.67058 - acc: 0.6206 -- iter: 11392/22500
Training Step: 179  | total loss: [1m[32m0.66862[0m[0m | time: 74.267s
| Adam | epoch: 001 | loss: 0.66862 - acc: 0.6257 -- iter: 11456/22500
Training Step: 180  | total loss: [1m[32m0.66787[0m[0m | time: 74.753s
| Adam | epoch: 001 | loss: 0.66787 - acc: 0.6257 -- iter: 11520/22500
Training Step: 181  | total loss: [1m[32m0.66731[0m[0m | time: 75.240s
| Adam | epoch: 001 | loss: 0.66731 - acc: 0.6225 -- iter: 11584/22500
Training Step: 182  | total loss: [1m[32m0.66738[0m[0m | time: 75.737s
| Adam | epoch: 001 | loss: 0.66738 - acc: 0.6180 -- iter: 11648/22500
Training Step: 183  | total loss: [1m[32m0.67290[0m[0m | time: 76.232s
| Adam | epoch: 001 | loss: 0.67290 - acc: 0.5891 -- iter: 11712/22500
Training Step: 184  | total loss: [1m[32m0.67263[0m[0m | time: 76.716s
| Adam | epoch: 001 | loss: 0.67263 - acc: 0.5801 -- iter: 11776/22500
Training Step: 185  | total loss: [1m[32m0.67407[0m[0m | time: 77.181s
| Adam | epoch: 001 | loss: 0.67407 - acc: 0.5721 -- iter: 11840/22500
Training Step: 186  | total loss: [1m[32m0.67523[0m[0m | time: 77.667s
| Adam | epoch: 001 | loss: 0.67523 - acc: 0.5665 -- iter: 11904/22500
Training Step: 187  | total loss: [1m[32m0.67733[0m[0m | time: 78.159s
| Adam | epoch: 001 | loss: 0.67733 - acc: 0.5598 -- iter: 11968/22500
Training Step: 188  | total loss: [1m[32m0.67892[0m[0m | time: 78.670s
| Adam | epoch: 001 | loss: 0.67892 - acc: 0.5507 -- iter: 12032/22500
Training Step: 189  | total loss: [1m[32m0.68062[0m[0m | time: 79.157s
| Adam | epoch: 001 | loss: 0.68062 - acc: 0.5363 -- iter: 12096/22500
Training Step: 190  | total loss: [1m[32m0.68288[0m[0m | time: 79.532s
| Adam | epoch: 001 | loss: 0.68288 - acc: 0.5155 -- iter: 12160/22500
Training Step: 191  | total loss: [1m[32m0.68401[0m[0m | time: 79.918s
| Adam | epoch: 001 | loss: 0.68401 - acc: 0.5124 -- iter: 12224/22500
Training Step: 192  | total loss: [1m[32m0.68427[0m[0m | time: 80.346s
| Adam | epoch: 001 | loss: 0.68427 - acc: 0.5158 -- iter: 12288/22500
Training Step: 193  | total loss: [1m[32m0.68614[0m[0m | time: 80.832s
| Adam | epoch: 001 | loss: 0.68614 - acc: 0.5127 -- iter: 12352/22500
Training Step: 194  | total loss: [1m[32m0.68636[0m[0m | time: 81.296s
| Adam | epoch: 001 | loss: 0.68636 - acc: 0.5036 -- iter: 12416/22500
Training Step: 195  | total loss: [1m[32m0.68739[0m[0m | time: 81.834s
| Adam | epoch: 001 | loss: 0.68739 - acc: 0.4954 -- iter: 12480/22500
Training Step: 196  | total loss: [1m[32m0.68725[0m[0m | time: 82.420s
| Adam | epoch: 001 | loss: 0.68725 - acc: 0.4990 -- iter: 12544/22500
Training Step: 197  | total loss: [1m[32m0.68906[0m[0m | time: 82.992s
| Adam | epoch: 001 | loss: 0.68906 - acc: 0.4850 -- iter: 12608/22500
Training Step: 198  | total loss: [1m[32m0.68968[0m[0m | time: 83.492s
| Adam | epoch: 001 | loss: 0.68968 - acc: 0.4818 -- iter: 12672/22500
Training Step: 199  | total loss: [1m[32m0.68985[0m[0m | time: 83.980s
| Adam | epoch: 001 | loss: 0.68985 - acc: 0.4930 -- iter: 12736/22500
Training Step: 200  | total loss: [1m[32m0.68905[0m[0m | time: 90.716s
| Adam | epoch: 001 | loss: 0.68905 - acc: 0.5062 | val_loss: 0.69215 - val_acc: 0.4980 -- iter: 12800/22500
--
Training Step: 201  | total loss: [1m[32m0.69031[0m[0m | time: 91.202s
| Adam | epoch: 001 | loss: 0.69031 - acc: 0.5040 -- iter: 12864/22500
Training Step: 202  | total loss: [1m[32m0.69170[0m[0m | time: 91.691s
| Adam | epoch: 001 | loss: 0.69170 - acc: 0.4990 -- iter: 12928/22500
Training Step: 203  | total loss: [1m[32m0.69264[0m[0m | time: 92.192s
| Adam | epoch: 001 | loss: 0.69264 - acc: 0.4944 -- iter: 12992/22500
Training Step: 204  | total loss: [1m[32m0.69275[0m[0m | time: 92.724s
| Adam | epoch: 001 | loss: 0.69275 - acc: 0.4934 -- iter: 13056/22500
Training Step: 205  | total loss: [1m[32m0.69390[0m[0m | time: 93.234s
| Adam | epoch: 001 | loss: 0.69390 - acc: 0.4893 -- iter: 13120/22500
Training Step: 206  | total loss: [1m[32m0.69426[0m[0m | time: 93.759s
| Adam | epoch: 001 | loss: 0.69426 - acc: 0.4857 -- iter: 13184/22500
Training Step: 207  | total loss: [1m[32m0.69358[0m[0m | time: 94.277s
| Adam | epoch: 001 | loss: 0.69358 - acc: 0.4887 -- iter: 13248/22500
Training Step: 208  | total loss: [1m[32m0.69341[0m[0m | time: 94.794s
| Adam | epoch: 001 | loss: 0.69341 - acc: 0.4930 -- iter: 13312/22500
Training Step: 209  | total loss: [1m[32m0.69242[0m[0m | time: 95.372s
| Adam | epoch: 001 | loss: 0.69242 - acc: 0.4968 -- iter: 13376/22500
Training Step: 210  | total loss: [1m[32m0.69069[0m[0m | time: 95.868s
| Adam | epoch: 001 | loss: 0.69069 - acc: 0.5018 -- iter: 13440/22500
Training Step: 211  | total loss: [1m[32m0.69058[0m[0m | time: 96.356s
| Adam | epoch: 001 | loss: 0.69058 - acc: 0.4954 -- iter: 13504/22500
Training Step: 212  | total loss: [1m[32m0.69083[0m[0m | time: 96.837s
| Adam | epoch: 001 | loss: 0.69083 - acc: 0.5036 -- iter: 13568/22500
Training Step: 213  | total loss: [1m[32m0.68920[0m[0m | time: 97.273s
| Adam | epoch: 001 | loss: 0.68920 - acc: 0.5111 -- iter: 13632/22500
Training Step: 214  | total loss: [1m[32m0.68946[0m[0m | time: 97.695s
| Adam | epoch: 001 | loss: 0.68946 - acc: 0.5100 -- iter: 13696/22500
Training Step: 215  | total loss: [1m[32m0.68618[0m[0m | time: 98.116s
| Adam | epoch: 001 | loss: 0.68618 - acc: 0.5168 -- iter: 13760/22500
Training Step: 216  | total loss: [1m[32m0.68614[0m[0m | time: 98.539s
| Adam | epoch: 001 | loss: 0.68614 - acc: 0.5167 -- iter: 13824/22500
Training Step: 217  | total loss: [1m[32m0.68400[0m[0m | time: 98.966s
| Adam | epoch: 001 | loss: 0.68400 - acc: 0.5291 -- iter: 13888/22500
Training Step: 218  | total loss: [1m[32m0.68388[0m[0m | time: 99.392s
| Adam | epoch: 001 | loss: 0.68388 - acc: 0.5262 -- iter: 13952/22500
Training Step: 219  | total loss: [1m[32m0.68527[0m[0m | time: 99.820s
| Adam | epoch: 001 | loss: 0.68527 - acc: 0.5220 -- iter: 14016/22500
Training Step: 220  | total loss: [1m[32m0.68395[0m[0m | time: 100.248s
| Adam | epoch: 001 | loss: 0.68395 - acc: 0.5292 -- iter: 14080/22500
Training Step: 221  | total loss: [1m[32m0.68257[0m[0m | time: 100.675s
| Adam | epoch: 001 | loss: 0.68257 - acc: 0.5262 -- iter: 14144/22500
Training Step: 222  | total loss: [1m[32m0.68033[0m[0m | time: 101.153s
| Adam | epoch: 001 | loss: 0.68033 - acc: 0.5346 -- iter: 14208/22500
Training Step: 223  | total loss: [1m[32m0.67630[0m[0m | time: 101.650s
| Adam | epoch: 001 | loss: 0.67630 - acc: 0.5405 -- iter: 14272/22500
Training Step: 224  | total loss: [1m[32m0.67520[0m[0m | time: 102.123s
| Adam | epoch: 001 | loss: 0.67520 - acc: 0.5427 -- iter: 14336/22500
Training Step: 225  | total loss: [1m[32m0.67305[0m[0m | time: 102.542s
| Adam | epoch: 001 | loss: 0.67305 - acc: 0.5525 -- iter: 14400/22500
Training Step: 226  | total loss: [1m[32m0.67101[0m[0m | time: 102.972s
| Adam | epoch: 001 | loss: 0.67101 - acc: 0.5582 -- iter: 14464/22500
Training Step: 227  | total loss: [1m[32m0.66736[0m[0m | time: 103.457s
| Adam | epoch: 001 | loss: 0.66736 - acc: 0.5649 -- iter: 14528/22500
Training Step: 228  | total loss: [1m[32m0.66796[0m[0m | time: 104.038s
| Adam | epoch: 001 | loss: 0.66796 - acc: 0.5693 -- iter: 14592/22500
Training Step: 229  | total loss: [1m[32m0.66602[0m[0m | time: 104.607s
| Adam | epoch: 001 | loss: 0.66602 - acc: 0.5717 -- iter: 14656/22500
Training Step: 230  | total loss: [1m[32m0.66467[0m[0m | time: 105.222s
| Adam | epoch: 001 | loss: 0.66467 - acc: 0.5693 -- iter: 14720/22500
Training Step: 231  | total loss: [1m[32m0.66657[0m[0m | time: 105.757s
| Adam | epoch: 001 | loss: 0.66657 - acc: 0.5670 -- iter: 14784/22500
Training Step: 232  | total loss: [1m[32m0.66210[0m[0m | time: 106.327s
| Adam | epoch: 001 | loss: 0.66210 - acc: 0.5744 -- iter: 14848/22500
Training Step: 233  | total loss: [1m[32m0.66361[0m[0m | time: 106.860s
| Adam | epoch: 001 | loss: 0.66361 - acc: 0.5716 -- iter: 14912/22500
Training Step: 234  | total loss: [1m[32m0.66449[0m[0m | time: 107.454s
| Adam | epoch: 001 | loss: 0.66449 - acc: 0.5785 -- iter: 14976/22500
Training Step: 235  | total loss: [1m[32m0.66316[0m[0m | time: 108.115s
| Adam | epoch: 001 | loss: 0.66316 - acc: 0.5879 -- iter: 15040/22500
Training Step: 236  | total loss: [1m[32m0.66341[0m[0m | time: 108.494s
| Adam | epoch: 001 | loss: 0.66341 - acc: 0.5931 -- iter: 15104/22500
Training Step: 237  | total loss: [1m[32m0.66366[0m[0m | time: 108.941s
| Adam | epoch: 001 | loss: 0.66366 - acc: 0.5885 -- iter: 15168/22500
Training Step: 238  | total loss: [1m[32m0.66341[0m[0m | time: 109.338s
| Adam | epoch: 001 | loss: 0.66341 - acc: 0.5969 -- iter: 15232/22500
Training Step: 239  | total loss: [1m[32m0.66560[0m[0m | time: 109.780s
| Adam | epoch: 001 | loss: 0.66560 - acc: 0.5997 -- iter: 15296/22500
Training Step: 240  | total loss: [1m[32m0.66656[0m[0m | time: 110.156s
| Adam | epoch: 001 | loss: 0.66656 - acc: 0.6022 -- iter: 15360/22500
Training Step: 241  | total loss: [1m[32m0.66540[0m[0m | time: 110.533s
| Adam | epoch: 001 | loss: 0.66540 - acc: 0.6060 -- iter: 15424/22500
Training Step: 242  | total loss: [1m[32m0.66671[0m[0m | time: 110.976s
| Adam | epoch: 001 | loss: 0.66671 - acc: 0.6064 -- iter: 15488/22500
Training Step: 243  | total loss: [1m[32m0.65592[0m[0m | time: 111.385s
| Adam | epoch: 001 | loss: 0.65592 - acc: 0.6285 -- iter: 15552/22500
Training Step: 244  | total loss: [1m[32m0.65188[0m[0m | time: 111.834s
| Adam | epoch: 001 | loss: 0.65188 - acc: 0.6360 -- iter: 15616/22500
Training Step: 245  | total loss: [1m[32m0.64956[0m[0m | time: 112.175s
| Adam | epoch: 001 | loss: 0.64956 - acc: 0.6412 -- iter: 15680/22500
Training Step: 246  | total loss: [1m[32m0.65163[0m[0m | time: 112.488s
| Adam | epoch: 001 | loss: 0.65163 - acc: 0.6411 -- iter: 15744/22500
Training Step: 247  | total loss: [1m[32m0.65326[0m[0m | time: 112.906s
| Adam | epoch: 001 | loss: 0.65326 - acc: 0.6457 -- iter: 15808/22500
Training Step: 248  | total loss: [1m[32m0.65429[0m[0m | time: 113.342s
| Adam | epoch: 001 | loss: 0.65429 - acc: 0.6468 -- iter: 15872/22500
Training Step: 249  | total loss: [1m[32m0.65460[0m[0m | time: 113.882s
| Adam | epoch: 001 | loss: 0.65460 - acc: 0.6477 -- iter: 15936/22500
Training Step: 250  | total loss: [1m[32m0.65427[0m[0m | time: 114.263s
| Adam | epoch: 001 | loss: 0.65427 - acc: 0.6502 -- iter: 16000/22500
Training Step: 251  | total loss: [1m[32m0.64977[0m[0m | time: 114.724s
| Adam | epoch: 001 | loss: 0.64977 - acc: 0.6539 -- iter: 16064/22500
Training Step: 252  | total loss: [1m[32m0.64553[0m[0m | time: 115.244s
| Adam | epoch: 001 | loss: 0.64553 - acc: 0.6604 -- iter: 16128/22500
Training Step: 253  | total loss: [1m[32m0.64041[0m[0m | time: 115.655s
| Adam | epoch: 001 | loss: 0.64041 - acc: 0.6662 -- iter: 16192/22500
Training Step: 254  | total loss: [1m[32m0.64062[0m[0m | time: 116.026s
| Adam | epoch: 001 | loss: 0.64062 - acc: 0.6683 -- iter: 16256/22500
Training Step: 255  | total loss: [1m[32m0.63974[0m[0m | time: 116.471s
| Adam | epoch: 001 | loss: 0.63974 - acc: 0.6671 -- iter: 16320/22500
Training Step: 256  | total loss: [1m[32m0.64371[0m[0m | time: 116.924s
| Adam | epoch: 001 | loss: 0.64371 - acc: 0.6629 -- iter: 16384/22500
Training Step: 257  | total loss: [1m[32m0.64080[0m[0m | time: 117.355s
| Adam | epoch: 001 | loss: 0.64080 - acc: 0.6669 -- iter: 16448/22500
Training Step: 258  | total loss: [1m[32m0.63818[0m[0m | time: 117.750s
| Adam | epoch: 001 | loss: 0.63818 - acc: 0.6690 -- iter: 16512/22500
Training Step: 259  | total loss: [1m[32m0.63250[0m[0m | time: 118.253s
| Adam | epoch: 001 | loss: 0.63250 - acc: 0.6755 -- iter: 16576/22500
Training Step: 260  | total loss: [1m[32m0.62973[0m[0m | time: 118.736s
| Adam | epoch: 001 | loss: 0.62973 - acc: 0.6799 -- iter: 16640/22500
Training Step: 261  | total loss: [1m[32m0.62382[0m[0m | time: 119.205s
| Adam | epoch: 001 | loss: 0.62382 - acc: 0.6869 -- iter: 16704/22500
Training Step: 262  | total loss: [1m[32m0.62671[0m[0m | time: 119.754s
| Adam | epoch: 001 | loss: 0.62671 - acc: 0.6838 -- iter: 16768/22500
Training Step: 263  | total loss: [1m[32m0.62796[0m[0m | time: 120.301s
| Adam | epoch: 001 | loss: 0.62796 - acc: 0.6811 -- iter: 16832/22500
Training Step: 264  | total loss: [1m[32m0.63103[0m[0m | time: 120.761s
| Adam | epoch: 001 | loss: 0.63103 - acc: 0.6770 -- iter: 16896/22500
Training Step: 265  | total loss: [1m[32m0.63363[0m[0m | time: 121.169s
| Adam | epoch: 001 | loss: 0.63363 - acc: 0.6734 -- iter: 16960/22500
Training Step: 266  | total loss: [1m[32m0.63372[0m[0m | time: 121.578s
| Adam | epoch: 001 | loss: 0.63372 - acc: 0.6732 -- iter: 17024/22500
Training Step: 267  | total loss: [1m[32m0.64399[0m[0m | time: 122.034s
| Adam | epoch: 001 | loss: 0.64399 - acc: 0.6575 -- iter: 17088/22500
Training Step: 268  | total loss: [1m[32m0.64137[0m[0m | time: 122.459s
| Adam | epoch: 001 | loss: 0.64137 - acc: 0.6605 -- iter: 17152/22500
Training Step: 269  | total loss: [1m[32m0.63691[0m[0m | time: 122.942s
| Adam | epoch: 001 | loss: 0.63691 - acc: 0.6663 -- iter: 17216/22500
Training Step: 270  | total loss: [1m[32m0.63420[0m[0m | time: 123.467s
| Adam | epoch: 001 | loss: 0.63420 - acc: 0.6700 -- iter: 17280/22500
Training Step: 271  | total loss: [1m[32m0.63154[0m[0m | time: 123.842s
| Adam | epoch: 001 | loss: 0.63154 - acc: 0.6749 -- iter: 17344/22500
Training Step: 272  | total loss: [1m[32m0.63109[0m[0m | time: 124.264s
| Adam | epoch: 001 | loss: 0.63109 - acc: 0.6746 -- iter: 17408/22500
Training Step: 273  | total loss: [1m[32m0.62925[0m[0m | time: 124.697s
| Adam | epoch: 001 | loss: 0.62925 - acc: 0.6774 -- iter: 17472/22500
Training Step: 274  | total loss: [1m[32m0.63069[0m[0m | time: 125.107s
| Adam | epoch: 001 | loss: 0.63069 - acc: 0.6753 -- iter: 17536/22500
Training Step: 275  | total loss: [1m[32m0.63647[0m[0m | time: 125.771s
| Adam | epoch: 001 | loss: 0.63647 - acc: 0.6671 -- iter: 17600/22500
Training Step: 276  | total loss: [1m[32m0.63521[0m[0m | time: 126.406s
| Adam | epoch: 001 | loss: 0.63521 - acc: 0.6692 -- iter: 17664/22500
Training Step: 277  | total loss: [1m[32m0.63500[0m[0m | time: 126.923s
| Adam | epoch: 001 | loss: 0.63500 - acc: 0.6694 -- iter: 17728/22500
Training Step: 278  | total loss: [1m[32m0.63828[0m[0m | time: 127.390s
| Adam | epoch: 001 | loss: 0.63828 - acc: 0.6650 -- iter: 17792/22500
Training Step: 279  | total loss: [1m[32m0.63226[0m[0m | time: 127.844s
| Adam | epoch: 001 | loss: 0.63226 - acc: 0.6735 -- iter: 17856/22500
Training Step: 280  | total loss: [1m[32m0.63033[0m[0m | time: 128.243s
| Adam | epoch: 001 | loss: 0.63033 - acc: 0.6749 -- iter: 17920/22500
Training Step: 281  | total loss: [1m[32m0.63449[0m[0m | time: 128.712s
| Adam | epoch: 001 | loss: 0.63449 - acc: 0.6683 -- iter: 17984/22500
Training Step: 282  | total loss: [1m[32m0.63928[0m[0m | time: 129.240s
| Adam | epoch: 001 | loss: 0.63928 - acc: 0.6609 -- iter: 18048/22500
Training Step: 283  | total loss: [1m[32m0.64145[0m[0m | time: 129.755s
| Adam | epoch: 001 | loss: 0.64145 - acc: 0.6589 -- iter: 18112/22500
Training Step: 284  | total loss: [1m[32m0.64173[0m[0m | time: 130.210s
| Adam | epoch: 001 | loss: 0.64173 - acc: 0.6602 -- iter: 18176/22500
Training Step: 285  | total loss: [1m[32m0.63955[0m[0m | time: 130.734s
| Adam | epoch: 001 | loss: 0.63955 - acc: 0.6645 -- iter: 18240/22500
Training Step: 286  | total loss: [1m[32m0.63962[0m[0m | time: 131.233s
| Adam | epoch: 001 | loss: 0.63962 - acc: 0.6652 -- iter: 18304/22500
Training Step: 287  | total loss: [1m[32m0.64079[0m[0m | time: 131.670s
| Adam | epoch: 001 | loss: 0.64079 - acc: 0.6627 -- iter: 18368/22500
Training Step: 288  | total loss: [1m[32m0.63494[0m[0m | time: 132.083s
| Adam | epoch: 001 | loss: 0.63494 - acc: 0.6699 -- iter: 18432/22500
Training Step: 289  | total loss: [1m[32m0.63305[0m[0m | time: 132.502s
| Adam | epoch: 001 | loss: 0.63305 - acc: 0.6717 -- iter: 18496/22500
Training Step: 290  | total loss: [1m[32m0.63155[0m[0m | time: 132.993s
| Adam | epoch: 001 | loss: 0.63155 - acc: 0.6748 -- iter: 18560/22500
Training Step: 291  | total loss: [1m[32m0.63012[0m[0m | time: 133.480s
| Adam | epoch: 001 | loss: 0.63012 - acc: 0.6761 -- iter: 18624/22500
Training Step: 292  | total loss: [1m[32m0.63140[0m[0m | time: 133.985s
| Adam | epoch: 001 | loss: 0.63140 - acc: 0.6772 -- iter: 18688/22500
Training Step: 293  | total loss: [1m[32m0.63572[0m[0m | time: 134.462s
| Adam | epoch: 001 | loss: 0.63572 - acc: 0.6720 -- iter: 18752/22500
Training Step: 294  | total loss: [1m[32m0.64242[0m[0m | time: 135.004s
| Adam | epoch: 001 | loss: 0.64242 - acc: 0.6626 -- iter: 18816/22500
Training Step: 295  | total loss: [1m[32m0.65364[0m[0m | time: 135.527s
| Adam | epoch: 001 | loss: 0.65364 - acc: 0.6479 -- iter: 18880/22500
Training Step: 296  | total loss: [1m[32m0.65580[0m[0m | time: 135.965s
| Adam | epoch: 001 | loss: 0.65580 - acc: 0.6441 -- iter: 18944/22500
Training Step: 297  | total loss: [1m[32m0.65450[0m[0m | time: 136.458s
| Adam | epoch: 001 | loss: 0.65450 - acc: 0.6453 -- iter: 19008/22500
Training Step: 298  | total loss: [1m[32m0.64850[0m[0m | time: 136.856s
| Adam | epoch: 001 | loss: 0.64850 - acc: 0.6542 -- iter: 19072/22500
Training Step: 299  | total loss: [1m[32m0.64579[0m[0m | time: 137.296s
| Adam | epoch: 001 | loss: 0.64579 - acc: 0.6591 -- iter: 19136/22500
Training Step: 300  | total loss: [1m[32m0.64041[0m[0m | time: 137.697s
| Adam | epoch: 001 | loss: 0.64041 - acc: 0.6666 -- iter: 19200/22500
Training Step: 301  | total loss: [1m[32m0.63789[0m[0m | time: 138.166s
| Adam | epoch: 001 | loss: 0.63789 - acc: 0.6703 -- iter: 19264/22500
Training Step: 302  | total loss: [1m[32m0.63492[0m[0m | time: 138.563s
| Adam | epoch: 001 | loss: 0.63492 - acc: 0.6735 -- iter: 19328/22500
Training Step: 303  | total loss: [1m[32m0.63570[0m[0m | time: 139.051s
| Adam | epoch: 001 | loss: 0.63570 - acc: 0.6718 -- iter: 19392/22500
Training Step: 304  | total loss: [1m[32m0.63702[0m[0m | time: 139.662s
| Adam | epoch: 001 | loss: 0.63702 - acc: 0.6687 -- iter: 19456/22500
Training Step: 305  | total loss: [1m[32m0.64504[0m[0m | time: 140.221s
| Adam | epoch: 001 | loss: 0.64504 - acc: 0.6534 -- iter: 19520/22500
Training Step: 306  | total loss: [1m[32m0.63704[0m[0m | time: 140.714s
| Adam | epoch: 001 | loss: 0.63704 - acc: 0.6631 -- iter: 19584/22500
Training Step: 307  | total loss: [1m[32m0.63018[0m[0m | time: 141.215s
| Adam | epoch: 001 | loss: 0.63018 - acc: 0.6749 -- iter: 19648/22500
Training Step: 308  | total loss: [1m[32m0.62929[0m[0m | time: 141.731s
| Adam | epoch: 001 | loss: 0.62929 - acc: 0.6746 -- iter: 19712/22500
Training Step: 309  | total loss: [1m[32m0.62806[0m[0m | time: 142.255s
| Adam | epoch: 001 | loss: 0.62806 - acc: 0.6759 -- iter: 19776/22500
Training Step: 310  | total loss: [1m[32m0.62749[0m[0m | time: 142.746s
| Adam | epoch: 001 | loss: 0.62749 - acc: 0.6786 -- iter: 19840/22500
Training Step: 311  | total loss: [1m[32m0.62997[0m[0m | time: 143.301s
| Adam | epoch: 001 | loss: 0.62997 - acc: 0.6717 -- iter: 19904/22500
Training Step: 312  | total loss: [1m[32m0.63891[0m[0m | time: 143.838s
| Adam | epoch: 001 | loss: 0.63891 - acc: 0.6608 -- iter: 19968/22500
Training Step: 313  | total loss: [1m[32m0.63829[0m[0m | time: 144.406s
| Adam | epoch: 001 | loss: 0.63829 - acc: 0.6603 -- iter: 20032/22500
Training Step: 314  | total loss: [1m[32m0.63624[0m[0m | time: 144.802s
| Adam | epoch: 001 | loss: 0.63624 - acc: 0.6615 -- iter: 20096/22500
Training Step: 315  | total loss: [1m[32m0.63386[0m[0m | time: 145.196s
| Adam | epoch: 001 | loss: 0.63386 - acc: 0.6625 -- iter: 20160/22500
Training Step: 316  | total loss: [1m[32m0.63593[0m[0m | time: 145.588s
| Adam | epoch: 001 | loss: 0.63593 - acc: 0.6572 -- iter: 20224/22500
Training Step: 317  | total loss: [1m[32m0.63861[0m[0m | time: 146.104s
| Adam | epoch: 001 | loss: 0.63861 - acc: 0.6524 -- iter: 20288/22500
Training Step: 318  | total loss: [1m[32m0.64187[0m[0m | time: 146.585s
| Adam | epoch: 001 | loss: 0.64187 - acc: 0.6512 -- iter: 20352/22500
Training Step: 319  | total loss: [1m[32m0.63986[0m[0m | time: 147.056s
| Adam | epoch: 001 | loss: 0.63986 - acc: 0.6517 -- iter: 20416/22500
Training Step: 320  | total loss: [1m[32m0.64256[0m[0m | time: 147.647s
| Adam | epoch: 001 | loss: 0.64256 - acc: 0.6491 -- iter: 20480/22500
Training Step: 321  | total loss: [1m[32m0.64394[0m[0m | time: 148.077s
| Adam | epoch: 001 | loss: 0.64394 - acc: 0.6435 -- iter: 20544/22500
Training Step: 322  | total loss: [1m[32m0.64468[0m[0m | time: 148.469s
| Adam | epoch: 001 | loss: 0.64468 - acc: 0.6417 -- iter: 20608/22500
Training Step: 323  | total loss: [1m[32m0.64082[0m[0m | time: 148.958s
| Adam | epoch: 001 | loss: 0.64082 - acc: 0.6478 -- iter: 20672/22500
Training Step: 324  | total loss: [1m[32m0.63681[0m[0m | time: 149.446s
| Adam | epoch: 001 | loss: 0.63681 - acc: 0.6580 -- iter: 20736/22500
Training Step: 325  | total loss: [1m[32m0.63783[0m[0m | time: 149.995s
| Adam | epoch: 001 | loss: 0.63783 - acc: 0.6547 -- iter: 20800/22500
Training Step: 326  | total loss: [1m[32m0.63853[0m[0m | time: 150.558s
| Adam | epoch: 001 | loss: 0.63853 - acc: 0.6533 -- iter: 20864/22500
Training Step: 327  | total loss: [1m[32m0.63836[0m[0m | time: 151.193s
| Adam | epoch: 001 | loss: 0.63836 - acc: 0.6583 -- iter: 20928/22500
Training Step: 328  | total loss: [1m[32m0.64051[0m[0m | time: 151.643s
| Adam | epoch: 001 | loss: 0.64051 - acc: 0.6597 -- iter: 20992/22500
Training Step: 329  | total loss: [1m[32m0.64873[0m[0m | time: 152.103s
| Adam | epoch: 001 | loss: 0.64873 - acc: 0.6468 -- iter: 21056/22500
Training Step: 330  | total loss: [1m[32m0.64782[0m[0m | time: 152.491s
| Adam | epoch: 001 | loss: 0.64782 - acc: 0.6462 -- iter: 21120/22500
Training Step: 331  | total loss: [1m[32m0.64867[0m[0m | time: 152.881s
| Adam | epoch: 001 | loss: 0.64867 - acc: 0.6425 -- iter: 21184/22500
Training Step: 332  | total loss: [1m[32m0.64081[0m[0m | time: 153.299s
| Adam | epoch: 001 | loss: 0.64081 - acc: 0.6517 -- iter: 21248/22500
Training Step: 333  | total loss: [1m[32m0.63607[0m[0m | time: 153.719s
| Adam | epoch: 001 | loss: 0.63607 - acc: 0.6600 -- iter: 21312/22500
Training Step: 334  | total loss: [1m[32m0.63155[0m[0m | time: 154.131s
| Adam | epoch: 001 | loss: 0.63155 - acc: 0.6643 -- iter: 21376/22500
Training Step: 335  | total loss: [1m[32m0.63584[0m[0m | time: 154.599s
| Adam | epoch: 001 | loss: 0.63584 - acc: 0.6557 -- iter: 21440/22500
Training Step: 336  | total loss: [1m[32m0.63470[0m[0m | time: 155.062s
| Adam | epoch: 001 | loss: 0.63470 - acc: 0.6557 -- iter: 21504/22500
Training Step: 337  | total loss: [1m[32m0.63067[0m[0m | time: 155.462s
| Adam | epoch: 001 | loss: 0.63067 - acc: 0.6573 -- iter: 21568/22500
Training Step: 338  | total loss: [1m[32m0.63326[0m[0m | time: 155.886s
| Adam | epoch: 001 | loss: 0.63326 - acc: 0.6572 -- iter: 21632/22500
Training Step: 339  | total loss: [1m[32m0.63140[0m[0m | time: 156.308s
| Adam | epoch: 001 | loss: 0.63140 - acc: 0.6634 -- iter: 21696/22500
Training Step: 340  | total loss: [1m[32m0.63499[0m[0m | time: 156.685s
| Adam | epoch: 001 | loss: 0.63499 - acc: 0.6549 -- iter: 21760/22500
Training Step: 341  | total loss: [1m[32m0.63536[0m[0m | time: 157.079s
| Adam | epoch: 001 | loss: 0.63536 - acc: 0.6534 -- iter: 21824/22500
Training Step: 342  | total loss: [1m[32m0.63686[0m[0m | time: 157.465s
| Adam | epoch: 001 | loss: 0.63686 - acc: 0.6553 -- iter: 21888/22500
Training Step: 343  | total loss: [1m[32m0.64113[0m[0m | time: 157.836s
| Adam | epoch: 001 | loss: 0.64113 - acc: 0.6491 -- iter: 21952/22500
Training Step: 344  | total loss: [1m[32m0.63583[0m[0m | time: 158.200s
| Adam | epoch: 001 | loss: 0.63583 - acc: 0.6592 -- iter: 22016/22500
Training Step: 345  | total loss: [1m[32m0.63926[0m[0m | time: 158.559s
| Adam | epoch: 001 | loss: 0.63926 - acc: 0.6558 -- iter: 22080/22500
Training Step: 346  | total loss: [1m[32m0.63740[0m[0m | time: 158.934s
| Adam | epoch: 001 | loss: 0.63740 - acc: 0.6605 -- iter: 22144/22500
Training Step: 347  | total loss: [1m[32m0.63363[0m[0m | time: 159.308s
| Adam | epoch: 001 | loss: 0.63363 - acc: 0.6648 -- iter: 22208/22500
Training Step: 348  | total loss: [1m[32m0.63340[0m[0m | time: 159.677s
| Adam | epoch: 001 | loss: 0.63340 - acc: 0.6592 -- iter: 22272/22500
Training Step: 349  | total loss: [1m[32m0.63693[0m[0m | time: 160.055s
| Adam | epoch: 001 | loss: 0.63693 - acc: 0.6543 -- iter: 22336/22500
Training Step: 350  | total loss: [1m[32m0.63653[0m[0m | time: 160.441s
| Adam | epoch: 001 | loss: 0.63653 - acc: 0.6560 -- iter: 22400/22500
Training Step: 351  | total loss: [1m[32m0.63650[0m[0m | time: 160.818s
| Adam | epoch: 001 | loss: 0.63650 - acc: 0.6545 -- iter: 22464/22500
Training Step: 352  | total loss: [1m[32m0.64020[0m[0m | time: 164.539s
| Adam | epoch: 001 | loss: 0.64020 - acc: 0.6468 | val_loss: 0.64796 - val_acc: 0.6460 -- iter: 22500/22500
--
Training Step: 353  | total loss: [1m[32m0.64436[0m[0m | time: 0.289s
| Adam | epoch: 002 | loss: 0.64436 - acc: 0.6405 -- iter: 00064/22500
Training Step: 354  | total loss: [1m[32m0.64684[0m[0m | time: 0.646s
| Adam | epoch: 002 | loss: 0.64684 - acc: 0.6376 -- iter: 00128/22500
Training Step: 355  | total loss: [1m[32m0.64298[0m[0m | time: 0.996s
| Adam | epoch: 002 | loss: 0.64298 - acc: 0.6457 -- iter: 00192/22500
Training Step: 356  | total loss: [1m[32m0.64259[0m[0m | time: 1.351s
| Adam | epoch: 002 | loss: 0.64259 - acc: 0.6483 -- iter: 00256/22500
Training Step: 357  | total loss: [1m[32m0.63817[0m[0m | time: 1.710s
| Adam | epoch: 002 | loss: 0.63817 - acc: 0.6553 -- iter: 00320/22500
Training Step: 358  | total loss: [1m[32m0.63576[0m[0m | time: 2.062s
| Adam | epoch: 002 | loss: 0.63576 - acc: 0.6570 -- iter: 00384/22500
Training Step: 359  | total loss: [1m[32m0.63455[0m[0m | time: 2.413s
| Adam | epoch: 002 | loss: 0.63455 - acc: 0.6616 -- iter: 00448/22500
Training Step: 360  | total loss: [1m[32m0.63616[0m[0m | time: 2.799s
| Adam | epoch: 002 | loss: 0.63616 - acc: 0.6611 -- iter: 00512/22500
Training Step: 361  | total loss: [1m[32m0.63508[0m[0m | time: 3.210s
| Adam | epoch: 002 | loss: 0.63508 - acc: 0.6622 -- iter: 00576/22500
Training Step: 362  | total loss: [1m[32m0.63490[0m[0m | time: 3.586s
| Adam | epoch: 002 | loss: 0.63490 - acc: 0.6600 -- iter: 00640/22500
Training Step: 363  | total loss: [1m[32m0.63622[0m[0m | time: 3.958s
| Adam | epoch: 002 | loss: 0.63622 - acc: 0.6627 -- iter: 00704/22500
Training Step: 364  | total loss: [1m[32m0.63736[0m[0m | time: 4.321s
| Adam | epoch: 002 | loss: 0.63736 - acc: 0.6621 -- iter: 00768/22500
Training Step: 365  | total loss: [1m[32m0.63328[0m[0m | time: 4.676s
| Adam | epoch: 002 | loss: 0.63328 - acc: 0.6662 -- iter: 00832/22500
Training Step: 366  | total loss: [1m[32m0.63466[0m[0m | time: 5.037s
| Adam | epoch: 002 | loss: 0.63466 - acc: 0.6636 -- iter: 00896/22500
Training Step: 367  | total loss: [1m[32m0.63383[0m[0m | time: 5.387s
| Adam | epoch: 002 | loss: 0.63383 - acc: 0.6660 -- iter: 00960/22500
Training Step: 368  | total loss: [1m[32m0.63145[0m[0m | time: 5.823s
| Adam | epoch: 002 | loss: 0.63145 - acc: 0.6682 -- iter: 01024/22500
Training Step: 369  | total loss: [1m[32m0.63032[0m[0m | time: 6.188s
| Adam | epoch: 002 | loss: 0.63032 - acc: 0.6701 -- iter: 01088/22500
Training Step: 370  | total loss: [1m[32m0.62460[0m[0m | time: 6.552s
| Adam | epoch: 002 | loss: 0.62460 - acc: 0.6797 -- iter: 01152/22500
Training Step: 371  | total loss: [1m[32m0.62639[0m[0m | time: 6.909s
| Adam | epoch: 002 | loss: 0.62639 - acc: 0.6773 -- iter: 01216/22500
Training Step: 372  | total loss: [1m[32m0.62029[0m[0m | time: 7.281s
| Adam | epoch: 002 | loss: 0.62029 - acc: 0.6830 -- iter: 01280/22500
Training Step: 373  | total loss: [1m[32m0.62972[0m[0m | time: 7.656s
| Adam | epoch: 002 | loss: 0.62972 - acc: 0.6710 -- iter: 01344/22500
Training Step: 374  | total loss: [1m[32m0.63282[0m[0m | time: 8.014s
| Adam | epoch: 002 | loss: 0.63282 - acc: 0.6679 -- iter: 01408/22500
Training Step: 375  | total loss: [1m[32m0.63940[0m[0m | time: 8.402s
| Adam | epoch: 002 | loss: 0.63940 - acc: 0.6605 -- iter: 01472/22500
Training Step: 376  | total loss: [1m[32m0.64631[0m[0m | time: 8.810s
| Adam | epoch: 002 | loss: 0.64631 - acc: 0.6507 -- iter: 01536/22500
Training Step: 377  | total loss: [1m[32m0.65295[0m[0m | time: 9.197s
| Adam | epoch: 002 | loss: 0.65295 - acc: 0.6388 -- iter: 01600/22500
Training Step: 378  | total loss: [1m[32m0.64593[0m[0m | time: 9.554s
| Adam | epoch: 002 | loss: 0.64593 - acc: 0.6468 -- iter: 01664/22500
Training Step: 379  | total loss: [1m[32m0.64387[0m[0m | time: 9.913s
| Adam | epoch: 002 | loss: 0.64387 - acc: 0.6493 -- iter: 01728/22500
Training Step: 380  | total loss: [1m[32m0.64258[0m[0m | time: 10.271s
| Adam | epoch: 002 | loss: 0.64258 - acc: 0.6515 -- iter: 01792/22500
Training Step: 381  | total loss: [1m[32m0.64376[0m[0m | time: 10.627s
| Adam | epoch: 002 | loss: 0.64376 - acc: 0.6504 -- iter: 01856/22500
Training Step: 382  | total loss: [1m[32m0.64438[0m[0m | time: 10.987s
| Adam | epoch: 002 | loss: 0.64438 - acc: 0.6479 -- iter: 01920/22500
Training Step: 383  | total loss: [1m[32m0.64727[0m[0m | time: 11.364s
| Adam | epoch: 002 | loss: 0.64727 - acc: 0.6425 -- iter: 01984/22500
Training Step: 384  | total loss: [1m[32m0.64347[0m[0m | time: 11.742s
| Adam | epoch: 002 | loss: 0.64347 - acc: 0.6486 -- iter: 02048/22500
Training Step: 385  | total loss: [1m[32m0.64050[0m[0m | time: 12.093s
| Adam | epoch: 002 | loss: 0.64050 - acc: 0.6556 -- iter: 02112/22500
Training Step: 386  | total loss: [1m[32m0.63975[0m[0m | time: 12.452s
| Adam | epoch: 002 | loss: 0.63975 - acc: 0.6556 -- iter: 02176/22500
Training Step: 387  | total loss: [1m[32m0.63799[0m[0m | time: 12.811s
| Adam | epoch: 002 | loss: 0.63799 - acc: 0.6557 -- iter: 02240/22500
Training Step: 388  | total loss: [1m[32m0.63315[0m[0m | time: 13.167s
| Adam | epoch: 002 | loss: 0.63315 - acc: 0.6667 -- iter: 02304/22500
Training Step: 389  | total loss: [1m[32m0.63731[0m[0m | time: 13.524s
| Adam | epoch: 002 | loss: 0.63731 - acc: 0.6594 -- iter: 02368/22500
Training Step: 390  | total loss: [1m[32m0.64061[0m[0m | time: 13.880s
| Adam | epoch: 002 | loss: 0.64061 - acc: 0.6528 -- iter: 02432/22500
Training Step: 391  | total loss: [1m[32m0.63145[0m[0m | time: 14.277s
| Adam | epoch: 002 | loss: 0.63145 - acc: 0.6657 -- iter: 02496/22500
Training Step: 392  | total loss: [1m[32m0.63463[0m[0m | time: 14.687s
| Adam | epoch: 002 | loss: 0.63463 - acc: 0.6616 -- iter: 02560/22500
Training Step: 393  | total loss: [1m[32m0.63896[0m[0m | time: 15.082s
| Adam | epoch: 002 | loss: 0.63896 - acc: 0.6548 -- iter: 02624/22500
Training Step: 394  | total loss: [1m[32m0.64064[0m[0m | time: 15.447s
| Adam | epoch: 002 | loss: 0.64064 - acc: 0.6534 -- iter: 02688/22500
Training Step: 395  | total loss: [1m[32m0.63502[0m[0m | time: 15.818s
| Adam | epoch: 002 | loss: 0.63502 - acc: 0.6568 -- iter: 02752/22500
Training Step: 396  | total loss: [1m[32m0.63922[0m[0m | time: 16.192s
| Adam | epoch: 002 | loss: 0.63922 - acc: 0.6489 -- iter: 02816/22500
Training Step: 397  | total loss: [1m[32m0.63730[0m[0m | time: 16.549s
| Adam | epoch: 002 | loss: 0.63730 - acc: 0.6512 -- iter: 02880/22500
Training Step: 398  | total loss: [1m[32m0.63427[0m[0m | time: 16.901s
| Adam | epoch: 002 | loss: 0.63427 - acc: 0.6580 -- iter: 02944/22500
Training Step: 399  | total loss: [1m[32m0.63562[0m[0m | time: 17.255s
| Adam | epoch: 002 | loss: 0.63562 - acc: 0.6594 -- iter: 03008/22500
Training Step: 400  | total loss: [1m[32m0.63629[0m[0m | time: 20.848s
| Adam | epoch: 002 | loss: 0.63629 - acc: 0.6591 | val_loss: 0.63491 - val_acc: 0.6596 -- iter: 03072/22500
--
Training Step: 401  | total loss: [1m[32m0.64383[0m[0m | time: 21.214s
| Adam | epoch: 002 | loss: 0.64383 - acc: 0.6432 -- iter: 03136/22500
Training Step: 402  | total loss: [1m[32m0.63849[0m[0m | time: 21.574s
| Adam | epoch: 002 | loss: 0.63849 - acc: 0.6507 -- iter: 03200/22500
Training Step: 403  | total loss: [1m[32m0.63272[0m[0m | time: 21.931s
| Adam | epoch: 002 | loss: 0.63272 - acc: 0.6638 -- iter: 03264/22500
Training Step: 404  | total loss: [1m[32m0.62643[0m[0m | time: 22.288s
| Adam | epoch: 002 | loss: 0.62643 - acc: 0.6740 -- iter: 03328/22500
Training Step: 405  | total loss: [1m[32m0.62483[0m[0m | time: 22.643s
| Adam | epoch: 002 | loss: 0.62483 - acc: 0.6784 -- iter: 03392/22500
Training Step: 406  | total loss: [1m[32m0.62339[0m[0m | time: 23.017s
| Adam | epoch: 002 | loss: 0.62339 - acc: 0.6809 -- iter: 03456/22500
Training Step: 407  | total loss: [1m[32m0.61706[0m[0m | time: 23.395s
| Adam | epoch: 002 | loss: 0.61706 - acc: 0.6894 -- iter: 03520/22500
Training Step: 408  | total loss: [1m[32m0.62080[0m[0m | time: 23.752s
| Adam | epoch: 002 | loss: 0.62080 - acc: 0.6876 -- iter: 03584/22500
Training Step: 409  | total loss: [1m[32m0.62063[0m[0m | time: 24.108s
| Adam | epoch: 002 | loss: 0.62063 - acc: 0.6876 -- iter: 03648/22500
Training Step: 410  | total loss: [1m[32m0.62490[0m[0m | time: 24.465s
| Adam | epoch: 002 | loss: 0.62490 - acc: 0.6829 -- iter: 03712/22500
Training Step: 411  | total loss: [1m[32m0.62997[0m[0m | time: 24.818s
| Adam | epoch: 002 | loss: 0.62997 - acc: 0.6771 -- iter: 03776/22500
Training Step: 412  | total loss: [1m[32m0.62788[0m[0m | time: 25.173s
| Adam | epoch: 002 | loss: 0.62788 - acc: 0.6766 -- iter: 03840/22500
Training Step: 413  | total loss: [1m[32m0.62612[0m[0m | time: 25.539s
| Adam | epoch: 002 | loss: 0.62612 - acc: 0.6824 -- iter: 03904/22500
Training Step: 414  | total loss: [1m[32m0.62584[0m[0m | time: 25.941s
| Adam | epoch: 002 | loss: 0.62584 - acc: 0.6813 -- iter: 03968/22500
Training Step: 415  | total loss: [1m[32m0.62195[0m[0m | time: 26.327s
| Adam | epoch: 002 | loss: 0.62195 - acc: 0.6882 -- iter: 04032/22500
Training Step: 416  | total loss: [1m[32m0.61948[0m[0m | time: 26.707s
| Adam | epoch: 002 | loss: 0.61948 - acc: 0.6928 -- iter: 04096/22500
Training Step: 417  | total loss: [1m[32m0.61791[0m[0m | time: 27.066s
| Adam | epoch: 002 | loss: 0.61791 - acc: 0.6970 -- iter: 04160/22500
Training Step: 418  | total loss: [1m[32m0.61554[0m[0m | time: 27.419s
| Adam | epoch: 002 | loss: 0.61554 - acc: 0.6945 -- iter: 04224/22500
Training Step: 419  | total loss: [1m[32m0.61881[0m[0m | time: 27.776s
| Adam | epoch: 002 | loss: 0.61881 - acc: 0.6860 -- iter: 04288/22500
Training Step: 420  | total loss: [1m[32m0.61889[0m[0m | time: 28.130s
| Adam | epoch: 002 | loss: 0.61889 - acc: 0.6830 -- iter: 04352/22500
Training Step: 421  | total loss: [1m[32m0.61965[0m[0m | time: 28.485s
| Adam | epoch: 002 | loss: 0.61965 - acc: 0.6850 -- iter: 04416/22500
Training Step: 422  | total loss: [1m[32m0.61550[0m[0m | time: 28.837s
| Adam | epoch: 002 | loss: 0.61550 - acc: 0.6868 -- iter: 04480/22500
Training Step: 423  | total loss: [1m[32m0.61151[0m[0m | time: 29.191s
| Adam | epoch: 002 | loss: 0.61151 - acc: 0.6931 -- iter: 04544/22500
Training Step: 424  | total loss: [1m[32m0.60678[0m[0m | time: 29.545s
| Adam | epoch: 002 | loss: 0.60678 - acc: 0.7004 -- iter: 04608/22500
Training Step: 425  | total loss: [1m[32m0.60087[0m[0m | time: 29.900s
| Adam | epoch: 002 | loss: 0.60087 - acc: 0.7085 -- iter: 04672/22500
Training Step: 426  | total loss: [1m[32m0.60582[0m[0m | time: 30.256s
| Adam | epoch: 002 | loss: 0.60582 - acc: 0.7001 -- iter: 04736/22500
Training Step: 427  | total loss: [1m[32m0.60143[0m[0m | time: 30.614s
| Adam | epoch: 002 | loss: 0.60143 - acc: 0.7145 -- iter: 04800/22500
Training Step: 428  | total loss: [1m[32m0.60713[0m[0m | time: 30.975s
| Adam | epoch: 002 | loss: 0.60713 - acc: 0.7149 -- iter: 04864/22500
Training Step: 429  | total loss: [1m[32m0.60102[0m[0m | time: 31.347s
| Adam | epoch: 002 | loss: 0.60102 - acc: 0.7169 -- iter: 04928/22500
Training Step: 430  | total loss: [1m[32m0.60637[0m[0m | time: 31.747s
| Adam | epoch: 002 | loss: 0.60637 - acc: 0.7124 -- iter: 04992/22500
Training Step: 431  | total loss: [1m[32m0.59406[0m[0m | time: 32.134s
| Adam | epoch: 002 | loss: 0.59406 - acc: 0.7224 -- iter: 05056/22500
Training Step: 432  | total loss: [1m[32m0.59021[0m[0m | time: 32.522s
| Adam | epoch: 002 | loss: 0.59021 - acc: 0.7251 -- iter: 05120/22500
Training Step: 433  | total loss: [1m[32m0.58894[0m[0m | time: 32.888s
| Adam | epoch: 002 | loss: 0.58894 - acc: 0.7214 -- iter: 05184/22500
Training Step: 434  | total loss: [1m[32m0.58053[0m[0m | time: 33.238s
| Adam | epoch: 002 | loss: 0.58053 - acc: 0.7274 -- iter: 05248/22500
Training Step: 435  | total loss: [1m[32m0.56919[0m[0m | time: 33.599s
| Adam | epoch: 002 | loss: 0.56919 - acc: 0.7343 -- iter: 05312/22500
Training Step: 436  | total loss: [1m[32m0.56397[0m[0m | time: 33.954s
| Adam | epoch: 002 | loss: 0.56397 - acc: 0.7374 -- iter: 05376/22500
Training Step: 437  | total loss: [1m[32m0.58446[0m[0m | time: 34.306s
| Adam | epoch: 002 | loss: 0.58446 - acc: 0.7231 -- iter: 05440/22500
Training Step: 438  | total loss: [1m[32m0.60663[0m[0m | time: 34.662s
| Adam | epoch: 002 | loss: 0.60663 - acc: 0.7086 -- iter: 05504/22500
Training Step: 439  | total loss: [1m[32m0.61531[0m[0m | time: 35.016s
| Adam | epoch: 002 | loss: 0.61531 - acc: 0.7049 -- iter: 05568/22500
Training Step: 440  | total loss: [1m[32m0.63436[0m[0m | time: 35.396s
| Adam | epoch: 002 | loss: 0.63436 - acc: 0.6922 -- iter: 05632/22500
Training Step: 441  | total loss: [1m[32m0.64337[0m[0m | time: 35.775s
| Adam | epoch: 002 | loss: 0.64337 - acc: 0.6855 -- iter: 05696/22500
Training Step: 442  | total loss: [1m[32m0.64927[0m[0m | time: 36.172s
| Adam | epoch: 002 | loss: 0.64927 - acc: 0.6810 -- iter: 05760/22500
Training Step: 443  | total loss: [1m[32m0.66698[0m[0m | time: 36.485s
| Adam | epoch: 002 | loss: 0.66698 - acc: 0.6676 -- iter: 05824/22500
Training Step: 444  | total loss: [1m[32m0.66175[0m[0m | time: 36.782s
| Adam | epoch: 002 | loss: 0.66175 - acc: 0.6696 -- iter: 05888/22500
Training Step: 445  | total loss: [1m[32m0.65737[0m[0m | time: 37.068s
| Adam | epoch: 002 | loss: 0.65737 - acc: 0.6714 -- iter: 05952/22500
Training Step: 446  | total loss: [1m[32m0.65871[0m[0m | time: 37.364s
| Adam | epoch: 002 | loss: 0.65871 - acc: 0.6667 -- iter: 06016/22500
Training Step: 447  | total loss: [1m[32m0.65677[0m[0m | time: 37.712s
| Adam | epoch: 002 | loss: 0.65677 - acc: 0.6657 -- iter: 06080/22500
Training Step: 448  | total loss: [1m[32m0.64930[0m[0m | time: 38.134s
| Adam | epoch: 002 | loss: 0.64930 - acc: 0.6710 -- iter: 06144/22500
Training Step: 449  | total loss: [1m[32m0.65375[0m[0m | time: 38.518s
| Adam | epoch: 002 | loss: 0.65375 - acc: 0.6617 -- iter: 06208/22500
Training Step: 450  | total loss: [1m[32m0.65413[0m[0m | time: 38.874s
| Adam | epoch: 002 | loss: 0.65413 - acc: 0.6580 -- iter: 06272/22500
Training Step: 451  | total loss: [1m[32m0.65640[0m[0m | time: 39.223s
| Adam | epoch: 002 | loss: 0.65640 - acc: 0.6532 -- iter: 06336/22500
Training Step: 452  | total loss: [1m[32m0.65541[0m[0m | time: 39.575s
| Adam | epoch: 002 | loss: 0.65541 - acc: 0.6504 -- iter: 06400/22500
Training Step: 453  | total loss: [1m[32m0.65700[0m[0m | time: 39.932s
| Adam | epoch: 002 | loss: 0.65700 - acc: 0.6463 -- iter: 06464/22500
Training Step: 454  | total loss: [1m[32m0.66068[0m[0m | time: 40.284s
| Adam | epoch: 002 | loss: 0.66068 - acc: 0.6394 -- iter: 06528/22500
Training Step: 455  | total loss: [1m[32m0.65839[0m[0m | time: 40.633s
| Adam | epoch: 002 | loss: 0.65839 - acc: 0.6411 -- iter: 06592/22500
Training Step: 456  | total loss: [1m[32m0.66011[0m[0m | time: 40.985s
| Adam | epoch: 002 | loss: 0.66011 - acc: 0.6348 -- iter: 06656/22500
Training Step: 457  | total loss: [1m[32m0.65994[0m[0m | time: 41.339s
| Adam | epoch: 002 | loss: 0.65994 - acc: 0.6323 -- iter: 06720/22500
Training Step: 458  | total loss: [1m[32m0.65529[0m[0m | time: 41.690s
| Adam | epoch: 002 | loss: 0.65529 - acc: 0.6394 -- iter: 06784/22500
Training Step: 459  | total loss: [1m[32m0.65504[0m[0m | time: 42.039s
| Adam | epoch: 002 | loss: 0.65504 - acc: 0.6426 -- iter: 06848/22500
Training Step: 460  | total loss: [1m[32m0.66107[0m[0m | time: 42.396s
| Adam | epoch: 002 | loss: 0.66107 - acc: 0.6299 -- iter: 06912/22500
Training Step: 461  | total loss: [1m[32m0.66088[0m[0m | time: 42.749s
| Adam | epoch: 002 | loss: 0.66088 - acc: 0.6294 -- iter: 06976/22500
Training Step: 462  | total loss: [1m[32m0.65802[0m[0m | time: 43.100s
| Adam | epoch: 002 | loss: 0.65802 - acc: 0.6321 -- iter: 07040/22500
Training Step: 463  | total loss: [1m[32m0.65745[0m[0m | time: 43.498s
| Adam | epoch: 002 | loss: 0.65745 - acc: 0.6298 -- iter: 07104/22500
Training Step: 464  | total loss: [1m[32m0.65586[0m[0m | time: 43.939s
| Adam | epoch: 002 | loss: 0.65586 - acc: 0.6309 -- iter: 07168/22500
Training Step: 465  | total loss: [1m[32m0.65507[0m[0m | time: 44.256s
| Adam | epoch: 002 | loss: 0.65507 - acc: 0.6319 -- iter: 07232/22500
Training Step: 466  | total loss: [1m[32m0.65437[0m[0m | time: 44.550s
| Adam | epoch: 002 | loss: 0.65437 - acc: 0.6296 -- iter: 07296/22500
Training Step: 467  | total loss: [1m[32m0.65042[0m[0m | time: 44.833s
| Adam | epoch: 002 | loss: 0.65042 - acc: 0.6385 -- iter: 07360/22500
Training Step: 468  | total loss: [1m[32m0.65023[0m[0m | time: 45.127s
| Adam | epoch: 002 | loss: 0.65023 - acc: 0.6372 -- iter: 07424/22500
Training Step: 469  | total loss: [1m[32m0.65221[0m[0m | time: 45.453s
| Adam | epoch: 002 | loss: 0.65221 - acc: 0.6360 -- iter: 07488/22500
Training Step: 470  | total loss: [1m[32m0.64777[0m[0m | time: 45.796s
| Adam | epoch: 002 | loss: 0.64777 - acc: 0.6411 -- iter: 07552/22500
Training Step: 471  | total loss: [1m[32m0.64753[0m[0m | time: 46.149s
| Adam | epoch: 002 | loss: 0.64753 - acc: 0.6442 -- iter: 07616/22500
Training Step: 472  | total loss: [1m[32m0.64784[0m[0m | time: 46.501s
| Adam | epoch: 002 | loss: 0.64784 - acc: 0.6376 -- iter: 07680/22500
Training Step: 473  | total loss: [1m[32m0.64798[0m[0m | time: 46.863s
| Adam | epoch: 002 | loss: 0.64798 - acc: 0.6332 -- iter: 07744/22500
Training Step: 474  | total loss: [1m[32m0.64288[0m[0m | time: 47.225s
| Adam | epoch: 002 | loss: 0.64288 - acc: 0.6402 -- iter: 07808/22500
Training Step: 475  | total loss: [1m[32m0.64860[0m[0m | time: 47.598s
| Adam | epoch: 002 | loss: 0.64860 - acc: 0.6309 -- iter: 07872/22500
Training Step: 476  | total loss: [1m[32m0.64851[0m[0m | time: 47.948s
| Adam | epoch: 002 | loss: 0.64851 - acc: 0.6303 -- iter: 07936/22500
Training Step: 477  | total loss: [1m[32m0.65170[0m[0m | time: 48.301s
| Adam | epoch: 002 | loss: 0.65170 - acc: 0.6298 -- iter: 08000/22500
Training Step: 478  | total loss: [1m[32m0.64596[0m[0m | time: 48.655s
| Adam | epoch: 002 | loss: 0.64596 - acc: 0.6371 -- iter: 08064/22500
Training Step: 479  | total loss: [1m[32m0.64963[0m[0m | time: 49.003s
| Adam | epoch: 002 | loss: 0.64963 - acc: 0.6265 -- iter: 08128/22500
Training Step: 480  | total loss: [1m[32m0.64494[0m[0m | time: 49.374s
| Adam | epoch: 002 | loss: 0.64494 - acc: 0.6310 -- iter: 08192/22500
Training Step: 481  | total loss: [1m[32m0.64408[0m[0m | time: 49.773s
| Adam | epoch: 002 | loss: 0.64408 - acc: 0.6304 -- iter: 08256/22500
Training Step: 482  | total loss: [1m[32m0.64262[0m[0m | time: 50.190s
| Adam | epoch: 002 | loss: 0.64262 - acc: 0.6283 -- iter: 08320/22500
Training Step: 483  | total loss: [1m[32m0.64372[0m[0m | time: 50.489s
| Adam | epoch: 002 | loss: 0.64372 - acc: 0.6264 -- iter: 08384/22500
Training Step: 484  | total loss: [1m[32m0.64737[0m[0m | time: 50.776s
| Adam | epoch: 002 | loss: 0.64737 - acc: 0.6216 -- iter: 08448/22500
Training Step: 485  | total loss: [1m[32m0.64838[0m[0m | time: 51.081s
| Adam | epoch: 002 | loss: 0.64838 - acc: 0.6235 -- iter: 08512/22500
Training Step: 486  | total loss: [1m[32m0.64716[0m[0m | time: 51.382s
| Adam | epoch: 002 | loss: 0.64716 - acc: 0.6252 -- iter: 08576/22500
Training Step: 487  | total loss: [1m[32m0.64339[0m[0m | time: 51.723s
| Adam | epoch: 002 | loss: 0.64339 - acc: 0.6299 -- iter: 08640/22500
Training Step: 488  | total loss: [1m[32m0.64234[0m[0m | time: 52.072s
| Adam | epoch: 002 | loss: 0.64234 - acc: 0.6263 -- iter: 08704/22500
Training Step: 489  | total loss: [1m[32m0.64845[0m[0m | time: 52.424s
| Adam | epoch: 002 | loss: 0.64845 - acc: 0.6277 -- iter: 08768/22500
Training Step: 490  | total loss: [1m[32m0.64347[0m[0m | time: 52.773s
| Adam | epoch: 002 | loss: 0.64347 - acc: 0.6290 -- iter: 08832/22500
Training Step: 491  | total loss: [1m[32m0.63874[0m[0m | time: 53.135s
| Adam | epoch: 002 | loss: 0.63874 - acc: 0.6411 -- iter: 08896/22500
Training Step: 492  | total loss: [1m[32m0.63998[0m[0m | time: 53.488s
| Adam | epoch: 002 | loss: 0.63998 - acc: 0.6364 -- iter: 08960/22500
Training Step: 493  | total loss: [1m[32m0.64421[0m[0m | time: 53.839s
| Adam | epoch: 002 | loss: 0.64421 - acc: 0.6352 -- iter: 09024/22500
Training Step: 494  | total loss: [1m[32m0.63845[0m[0m | time: 54.190s
| Adam | epoch: 002 | loss: 0.63845 - acc: 0.6436 -- iter: 09088/22500
Training Step: 495  | total loss: [1m[32m0.64356[0m[0m | time: 54.536s
| Adam | epoch: 002 | loss: 0.64356 - acc: 0.6308 -- iter: 09152/22500
Training Step: 496  | total loss: [1m[32m0.64572[0m[0m | time: 54.897s
| Adam | epoch: 002 | loss: 0.64572 - acc: 0.6349 -- iter: 09216/22500
Training Step: 497  | total loss: [1m[32m0.63954[0m[0m | time: 55.310s
| Adam | epoch: 002 | loss: 0.63954 - acc: 0.6433 -- iter: 09280/22500
Training Step: 498  | total loss: [1m[32m0.63716[0m[0m | time: 55.758s
| Adam | epoch: 002 | loss: 0.63716 - acc: 0.6508 -- iter: 09344/22500
Training Step: 499  | total loss: [1m[32m0.64126[0m[0m | time: 56.145s
| Adam | epoch: 002 | loss: 0.64126 - acc: 0.6436 -- iter: 09408/22500
Training Step: 500  | total loss: [1m[32m0.63914[0m[0m | time: 56.493s
| Adam | epoch: 002 | loss: 0.63914 - acc: 0.6526 -- iter: 09472/22500
Training Step: 501  | total loss: [1m[32m0.64638[0m[0m | time: 56.781s
| Adam | epoch: 002 | loss: 0.64638 - acc: 0.6405 -- iter: 09536/22500
Training Step: 502  | total loss: [1m[32m0.64980[0m[0m | time: 57.063s
| Adam | epoch: 002 | loss: 0.64980 - acc: 0.6327 -- iter: 09600/22500
Training Step: 503  | total loss: [1m[32m0.64307[0m[0m | time: 57.348s
| Adam | epoch: 002 | loss: 0.64307 - acc: 0.6460 -- iter: 09664/22500
Training Step: 504  | total loss: [1m[32m0.63366[0m[0m | time: 57.643s
| Adam | epoch: 002 | loss: 0.63366 - acc: 0.6580 -- iter: 09728/22500
Training Step: 505  | total loss: [1m[32m0.62648[0m[0m | time: 58.005s
| Adam | epoch: 002 | loss: 0.62648 - acc: 0.6687 -- iter: 09792/22500
Training Step: 506  | total loss: [1m[32m0.63490[0m[0m | time: 58.371s
| Adam | epoch: 002 | loss: 0.63490 - acc: 0.6628 -- iter: 09856/22500
Training Step: 507  | total loss: [1m[32m0.62709[0m[0m | time: 58.722s
| Adam | epoch: 002 | loss: 0.62709 - acc: 0.6762 -- iter: 09920/22500
Training Step: 508  | total loss: [1m[32m0.62361[0m[0m | time: 59.173s
| Adam | epoch: 002 | loss: 0.62361 - acc: 0.6773 -- iter: 09984/22500
Training Step: 509  | total loss: [1m[32m0.62050[0m[0m | time: 59.670s
| Adam | epoch: 002 | loss: 0.62050 - acc: 0.6846 -- iter: 10048/22500
Training Step: 510  | total loss: [1m[32m0.61452[0m[0m | time: 60.096s
| Adam | epoch: 002 | loss: 0.61452 - acc: 0.6896 -- iter: 10112/22500
Training Step: 511  | total loss: [1m[32m0.60610[0m[0m | time: 60.479s
| Adam | epoch: 002 | loss: 0.60610 - acc: 0.7019 -- iter: 10176/22500
Training Step: 512  | total loss: [1m[32m0.60443[0m[0m | time: 60.815s
| Adam | epoch: 002 | loss: 0.60443 - acc: 0.7036 -- iter: 10240/22500
Training Step: 513  | total loss: [1m[32m0.60788[0m[0m | time: 61.144s
| Adam | epoch: 002 | loss: 0.60788 - acc: 0.6973 -- iter: 10304/22500
Training Step: 514  | total loss: [1m[32m0.60703[0m[0m | time: 61.501s
| Adam | epoch: 002 | loss: 0.60703 - acc: 0.6978 -- iter: 10368/22500
Training Step: 515  | total loss: [1m[32m0.61086[0m[0m | time: 61.960s
| Adam | epoch: 002 | loss: 0.61086 - acc: 0.6890 -- iter: 10432/22500
Training Step: 516  | total loss: [1m[32m0.60760[0m[0m | time: 62.407s
| Adam | epoch: 002 | loss: 0.60760 - acc: 0.6888 -- iter: 10496/22500
Training Step: 517  | total loss: [1m[32m0.61391[0m[0m | time: 62.807s
| Adam | epoch: 002 | loss: 0.61391 - acc: 0.6856 -- iter: 10560/22500
Training Step: 518  | total loss: [1m[32m0.62017[0m[0m | time: 63.166s
| Adam | epoch: 002 | loss: 0.62017 - acc: 0.6795 -- iter: 10624/22500
Training Step: 519  | total loss: [1m[32m0.62928[0m[0m | time: 63.536s
| Adam | epoch: 002 | loss: 0.62928 - acc: 0.6710 -- iter: 10688/22500
Training Step: 520  | total loss: [1m[32m0.62910[0m[0m | time: 63.898s
| Adam | epoch: 002 | loss: 0.62910 - acc: 0.6757 -- iter: 10752/22500
Training Step: 521  | total loss: [1m[32m0.62463[0m[0m | time: 64.249s
| Adam | epoch: 002 | loss: 0.62463 - acc: 0.6832 -- iter: 10816/22500
Training Step: 522  | total loss: [1m[32m0.61951[0m[0m | time: 64.583s
| Adam | epoch: 002 | loss: 0.61951 - acc: 0.6883 -- iter: 10880/22500
Training Step: 523  | total loss: [1m[32m0.61519[0m[0m | time: 64.906s
| Adam | epoch: 002 | loss: 0.61519 - acc: 0.6929 -- iter: 10944/22500
Training Step: 524  | total loss: [1m[32m0.61419[0m[0m | time: 65.281s
| Adam | epoch: 002 | loss: 0.61419 - acc: 0.6892 -- iter: 11008/22500
Training Step: 525  | total loss: [1m[32m0.61910[0m[0m | time: 65.674s
| Adam | epoch: 002 | loss: 0.61910 - acc: 0.6844 -- iter: 11072/22500
Training Step: 526  | total loss: [1m[32m0.62063[0m[0m | time: 66.121s
| Adam | epoch: 002 | loss: 0.62063 - acc: 0.6831 -- iter: 11136/22500
Training Step: 527  | total loss: [1m[32m0.61642[0m[0m | time: 66.664s
| Adam | epoch: 002 | loss: 0.61642 - acc: 0.6867 -- iter: 11200/22500
Training Step: 528  | total loss: [1m[32m0.61643[0m[0m | time: 67.215s
| Adam | epoch: 002 | loss: 0.61643 - acc: 0.6883 -- iter: 11264/22500
Training Step: 529  | total loss: [1m[32m0.61544[0m[0m | time: 67.567s
| Adam | epoch: 002 | loss: 0.61544 - acc: 0.6929 -- iter: 11328/22500
Training Step: 530  | total loss: [1m[32m0.62010[0m[0m | time: 67.929s
| Adam | epoch: 002 | loss: 0.62010 - acc: 0.6893 -- iter: 11392/22500
Training Step: 531  | total loss: [1m[32m0.63099[0m[0m | time: 68.363s
| Adam | epoch: 002 | loss: 0.63099 - acc: 0.6781 -- iter: 11456/22500
Training Step: 532  | total loss: [1m[32m0.63962[0m[0m | time: 68.760s
| Adam | epoch: 002 | loss: 0.63962 - acc: 0.6666 -- iter: 11520/22500
Training Step: 533  | total loss: [1m[32m0.64303[0m[0m | time: 69.130s
| Adam | epoch: 002 | loss: 0.64303 - acc: 0.6624 -- iter: 11584/22500
Training Step: 534  | total loss: [1m[32m0.63899[0m[0m | time: 69.449s
| Adam | epoch: 002 | loss: 0.63899 - acc: 0.6665 -- iter: 11648/22500
Training Step: 535  | total loss: [1m[32m0.63927[0m[0m | time: 69.838s
| Adam | epoch: 002 | loss: 0.63927 - acc: 0.6655 -- iter: 11712/22500
Training Step: 536  | total loss: [1m[32m0.63548[0m[0m | time: 70.229s
| Adam | epoch: 002 | loss: 0.63548 - acc: 0.6739 -- iter: 11776/22500
Training Step: 537  | total loss: [1m[32m0.63295[0m[0m | time: 70.662s
| Adam | epoch: 002 | loss: 0.63295 - acc: 0.6815 -- iter: 11840/22500
Training Step: 538  | total loss: [1m[32m0.63036[0m[0m | time: 71.087s
| Adam | epoch: 002 | loss: 0.63036 - acc: 0.6884 -- iter: 11904/22500
Training Step: 539  | total loss: [1m[32m0.62813[0m[0m | time: 71.450s
| Adam | epoch: 002 | loss: 0.62813 - acc: 0.6914 -- iter: 11968/22500
Training Step: 540  | total loss: [1m[32m0.62579[0m[0m | time: 71.868s
| Adam | epoch: 002 | loss: 0.62579 - acc: 0.6957 -- iter: 12032/22500
Training Step: 541  | total loss: [1m[32m0.62716[0m[0m | time: 72.300s
| Adam | epoch: 002 | loss: 0.62716 - acc: 0.6918 -- iter: 12096/22500
Training Step: 542  | total loss: [1m[32m0.62499[0m[0m | time: 72.667s
| Adam | epoch: 002 | loss: 0.62499 - acc: 0.6960 -- iter: 12160/22500
Training Step: 543  | total loss: [1m[32m0.62584[0m[0m | time: 73.098s
| Adam | epoch: 002 | loss: 0.62584 - acc: 0.6905 -- iter: 12224/22500
Training Step: 544  | total loss: [1m[32m0.62538[0m[0m | time: 73.453s
| Adam | epoch: 002 | loss: 0.62538 - acc: 0.6949 -- iter: 12288/22500
Training Step: 545  | total loss: [1m[32m0.61936[0m[0m | time: 73.874s
| Adam | epoch: 002 | loss: 0.61936 - acc: 0.7004 -- iter: 12352/22500
Training Step: 546  | total loss: [1m[32m0.61775[0m[0m | time: 74.271s
| Adam | epoch: 002 | loss: 0.61775 - acc: 0.7022 -- iter: 12416/22500
Training Step: 547  | total loss: [1m[32m0.61143[0m[0m | time: 74.643s
| Adam | epoch: 002 | loss: 0.61143 - acc: 0.7101 -- iter: 12480/22500
Training Step: 548  | total loss: [1m[32m0.60568[0m[0m | time: 75.033s
| Adam | epoch: 002 | loss: 0.60568 - acc: 0.7157 -- iter: 12544/22500
Training Step: 549  | total loss: [1m[32m0.60192[0m[0m | time: 75.584s
| Adam | epoch: 002 | loss: 0.60192 - acc: 0.7207 -- iter: 12608/22500
Training Step: 550  | total loss: [1m[32m0.60169[0m[0m | time: 75.943s
| Adam | epoch: 002 | loss: 0.60169 - acc: 0.7174 -- iter: 12672/22500
Training Step: 551  | total loss: [1m[32m0.60533[0m[0m | time: 76.439s
| Adam | epoch: 002 | loss: 0.60533 - acc: 0.7159 -- iter: 12736/22500
Training Step: 552  | total loss: [1m[32m0.60512[0m[0m | time: 76.865s
| Adam | epoch: 002 | loss: 0.60512 - acc: 0.7178 -- iter: 12800/22500
Training Step: 553  | total loss: [1m[32m0.60746[0m[0m | time: 77.343s
| Adam | epoch: 002 | loss: 0.60746 - acc: 0.7132 -- iter: 12864/22500
Training Step: 554  | total loss: [1m[32m0.61299[0m[0m | time: 77.878s
| Adam | epoch: 002 | loss: 0.61299 - acc: 0.7106 -- iter: 12928/22500
Training Step: 555  | total loss: [1m[32m0.60750[0m[0m | time: 78.164s
| Adam | epoch: 002 | loss: 0.60750 - acc: 0.7161 -- iter: 12992/22500
Training Step: 556  | total loss: [1m[32m0.60207[0m[0m | time: 78.472s
| Adam | epoch: 002 | loss: 0.60207 - acc: 0.7195 -- iter: 13056/22500
Training Step: 557  | total loss: [1m[32m0.60440[0m[0m | time: 78.773s
| Adam | epoch: 002 | loss: 0.60440 - acc: 0.7179 -- iter: 13120/22500
Training Step: 558  | total loss: [1m[32m0.61253[0m[0m | time: 79.126s
| Adam | epoch: 002 | loss: 0.61253 - acc: 0.7086 -- iter: 13184/22500
Training Step: 559  | total loss: [1m[32m0.61410[0m[0m | time: 79.510s
| Adam | epoch: 002 | loss: 0.61410 - acc: 0.7049 -- iter: 13248/22500
Training Step: 560  | total loss: [1m[32m0.61253[0m[0m | time: 79.927s
| Adam | epoch: 002 | loss: 0.61253 - acc: 0.7063 -- iter: 13312/22500
Training Step: 561  | total loss: [1m[32m0.62054[0m[0m | time: 80.352s
| Adam | epoch: 002 | loss: 0.62054 - acc: 0.6982 -- iter: 13376/22500
Training Step: 562  | total loss: [1m[32m0.62063[0m[0m | time: 80.723s
| Adam | epoch: 002 | loss: 0.62063 - acc: 0.6971 -- iter: 13440/22500
Training Step: 563  | total loss: [1m[32m0.62414[0m[0m | time: 81.059s
| Adam | epoch: 002 | loss: 0.62414 - acc: 0.6915 -- iter: 13504/22500
Training Step: 564  | total loss: [1m[32m0.62088[0m[0m | time: 81.356s
| Adam | epoch: 002 | loss: 0.62088 - acc: 0.6926 -- iter: 13568/22500
Training Step: 565  | total loss: [1m[32m0.61673[0m[0m | time: 81.654s
| Adam | epoch: 002 | loss: 0.61673 - acc: 0.6952 -- iter: 13632/22500
Training Step: 566  | total loss: [1m[32m0.61503[0m[0m | time: 82.039s
| Adam | epoch: 002 | loss: 0.61503 - acc: 0.6976 -- iter: 13696/22500
Training Step: 567  | total loss: [1m[32m0.61753[0m[0m | time: 82.404s
| Adam | epoch: 002 | loss: 0.61753 - acc: 0.6919 -- iter: 13760/22500
Training Step: 568  | total loss: [1m[32m0.61105[0m[0m | time: 82.788s
| Adam | epoch: 002 | loss: 0.61105 - acc: 0.6993 -- iter: 13824/22500
Training Step: 569  | total loss: [1m[32m0.61546[0m[0m | time: 83.159s
| Adam | epoch: 002 | loss: 0.61546 - acc: 0.6934 -- iter: 13888/22500
Training Step: 570  | total loss: [1m[32m0.61475[0m[0m | time: 83.543s
| Adam | epoch: 002 | loss: 0.61475 - acc: 0.6944 -- iter: 13952/22500
Training Step: 571  | total loss: [1m[32m0.61931[0m[0m | time: 83.915s
| Adam | epoch: 002 | loss: 0.61931 - acc: 0.6890 -- iter: 14016/22500
Training Step: 572  | total loss: [1m[32m0.62330[0m[0m | time: 84.292s
| Adam | epoch: 002 | loss: 0.62330 - acc: 0.6810 -- iter: 14080/22500
Training Step: 573  | total loss: [1m[32m0.62537[0m[0m | time: 84.626s
| Adam | epoch: 002 | loss: 0.62537 - acc: 0.6786 -- iter: 14144/22500
Training Step: 574  | total loss: [1m[32m0.62417[0m[0m | time: 84.944s
| Adam | epoch: 002 | loss: 0.62417 - acc: 0.6763 -- iter: 14208/22500
Training Step: 575  | total loss: [1m[32m0.62085[0m[0m | time: 85.324s
| Adam | epoch: 002 | loss: 0.62085 - acc: 0.6774 -- iter: 14272/22500
Training Step: 576  | total loss: [1m[32m0.61985[0m[0m | time: 85.723s
| Adam | epoch: 002 | loss: 0.61985 - acc: 0.6738 -- iter: 14336/22500
Training Step: 577  | total loss: [1m[32m0.62500[0m[0m | time: 86.153s
| Adam | epoch: 002 | loss: 0.62500 - acc: 0.6658 -- iter: 14400/22500
Training Step: 578  | total loss: [1m[32m0.62292[0m[0m | time: 86.627s
| Adam | epoch: 002 | loss: 0.62292 - acc: 0.6664 -- iter: 14464/22500
Training Step: 579  | total loss: [1m[32m0.62185[0m[0m | time: 87.109s
| Adam | epoch: 002 | loss: 0.62185 - acc: 0.6669 -- iter: 14528/22500
Training Step: 580  | total loss: [1m[32m0.61719[0m[0m | time: 87.499s
| Adam | epoch: 002 | loss: 0.61719 - acc: 0.6737 -- iter: 14592/22500
Training Step: 581  | total loss: [1m[32m0.61802[0m[0m | time: 87.853s
| Adam | epoch: 002 | loss: 0.61802 - acc: 0.6735 -- iter: 14656/22500
Training Step: 582  | total loss: [1m[32m0.62010[0m[0m | time: 88.204s
| Adam | epoch: 002 | loss: 0.62010 - acc: 0.6686 -- iter: 14720/22500
Training Step: 583  | total loss: [1m[32m0.61871[0m[0m | time: 88.557s
| Adam | epoch: 002 | loss: 0.61871 - acc: 0.6721 -- iter: 14784/22500
Training Step: 584  | total loss: [1m[32m0.61822[0m[0m | time: 88.916s
| Adam | epoch: 002 | loss: 0.61822 - acc: 0.6721 -- iter: 14848/22500
Training Step: 585  | total loss: [1m[32m0.61369[0m[0m | time: 89.254s
| Adam | epoch: 002 | loss: 0.61369 - acc: 0.6799 -- iter: 14912/22500
Training Step: 586  | total loss: [1m[32m0.61033[0m[0m | time: 89.619s
| Adam | epoch: 002 | loss: 0.61033 - acc: 0.6806 -- iter: 14976/22500
Training Step: 587  | total loss: [1m[32m0.61140[0m[0m | time: 89.982s
| Adam | epoch: 002 | loss: 0.61140 - acc: 0.6797 -- iter: 15040/22500
Training Step: 588  | total loss: [1m[32m0.61570[0m[0m | time: 90.371s
| Adam | epoch: 002 | loss: 0.61570 - acc: 0.6743 -- iter: 15104/22500
Training Step: 589  | total loss: [1m[32m0.61591[0m[0m | time: 90.837s
| Adam | epoch: 002 | loss: 0.61591 - acc: 0.6693 -- iter: 15168/22500
Training Step: 590  | total loss: [1m[32m0.61919[0m[0m | time: 91.233s
| Adam | epoch: 002 | loss: 0.61919 - acc: 0.6602 -- iter: 15232/22500
Training Step: 591  | total loss: [1m[32m0.61473[0m[0m | time: 91.661s
| Adam | epoch: 002 | loss: 0.61473 - acc: 0.6661 -- iter: 15296/22500
Training Step: 592  | total loss: [1m[32m0.61192[0m[0m | time: 92.162s
| Adam | epoch: 002 | loss: 0.61192 - acc: 0.6682 -- iter: 15360/22500
Training Step: 593  | total loss: [1m[32m0.61382[0m[0m | time: 92.720s
| Adam | epoch: 002 | loss: 0.61382 - acc: 0.6655 -- iter: 15424/22500
Training Step: 594  | total loss: [1m[32m0.61642[0m[0m | time: 93.140s
| Adam | epoch: 002 | loss: 0.61642 - acc: 0.6630 -- iter: 15488/22500
Training Step: 595  | total loss: [1m[32m0.60705[0m[0m | time: 93.546s
| Adam | epoch: 002 | loss: 0.60705 - acc: 0.6732 -- iter: 15552/22500
Training Step: 596  | total loss: [1m[32m0.61696[0m[0m | time: 93.861s
| Adam | epoch: 002 | loss: 0.61696 - acc: 0.6653 -- iter: 15616/22500
Training Step: 597  | total loss: [1m[32m0.61596[0m[0m | time: 94.201s
| Adam | epoch: 002 | loss: 0.61596 - acc: 0.6597 -- iter: 15680/22500
Training Step: 598  | total loss: [1m[32m0.61390[0m[0m | time: 94.561s
| Adam | epoch: 002 | loss: 0.61390 - acc: 0.6578 -- iter: 15744/22500
Training Step: 599  | total loss: [1m[32m0.62386[0m[0m | time: 95.010s
| Adam | epoch: 002 | loss: 0.62386 - acc: 0.6514 -- iter: 15808/22500
Training Step: 600  | total loss: [1m[32m0.62765[0m[0m | time: 99.514s
| Adam | epoch: 002 | loss: 0.62765 - acc: 0.6519 | val_loss: 0.68025 - val_acc: 0.6316 -- iter: 15872/22500
--
Training Step: 601  | total loss: [1m[32m0.63389[0m[0m | time: 99.974s
| Adam | epoch: 002 | loss: 0.63389 - acc: 0.6476 -- iter: 15936/22500
Training Step: 602  | total loss: [1m[32m0.63352[0m[0m | time: 100.381s
| Adam | epoch: 002 | loss: 0.63352 - acc: 0.6422 -- iter: 16000/22500
Training Step: 603  | total loss: [1m[32m0.63618[0m[0m | time: 100.777s
| Adam | epoch: 002 | loss: 0.63618 - acc: 0.6405 -- iter: 16064/22500
Training Step: 604  | total loss: [1m[32m0.63949[0m[0m | time: 101.135s
| Adam | epoch: 002 | loss: 0.63949 - acc: 0.6343 -- iter: 16128/22500
Training Step: 605  | total loss: [1m[32m0.64497[0m[0m | time: 101.482s
| Adam | epoch: 002 | loss: 0.64497 - acc: 0.6333 -- iter: 16192/22500
Training Step: 606  | total loss: [1m[32m0.64753[0m[0m | time: 101.903s
| Adam | epoch: 002 | loss: 0.64753 - acc: 0.6325 -- iter: 16256/22500
Training Step: 607  | total loss: [1m[32m0.64605[0m[0m | time: 102.329s
| Adam | epoch: 002 | loss: 0.64605 - acc: 0.6349 -- iter: 16320/22500
Training Step: 608  | total loss: [1m[32m0.64727[0m[0m | time: 102.724s
| Adam | epoch: 002 | loss: 0.64727 - acc: 0.6401 -- iter: 16384/22500
Training Step: 609  | total loss: [1m[32m0.64064[0m[0m | time: 103.047s
| Adam | epoch: 002 | loss: 0.64064 - acc: 0.6480 -- iter: 16448/22500
Training Step: 610  | total loss: [1m[32m0.63810[0m[0m | time: 103.394s
| Adam | epoch: 002 | loss: 0.63810 - acc: 0.6582 -- iter: 16512/22500
Training Step: 611  | total loss: [1m[32m0.63694[0m[0m | time: 103.730s
| Adam | epoch: 002 | loss: 0.63694 - acc: 0.6580 -- iter: 16576/22500
Training Step: 612  | total loss: [1m[32m0.63798[0m[0m | time: 104.178s
| Adam | epoch: 002 | loss: 0.63798 - acc: 0.6563 -- iter: 16640/22500
Training Step: 613  | total loss: [1m[32m0.63222[0m[0m | time: 104.590s
| Adam | epoch: 002 | loss: 0.63222 - acc: 0.6688 -- iter: 16704/22500
Training Step: 614  | total loss: [1m[32m0.62907[0m[0m | time: 104.943s
| Adam | epoch: 002 | loss: 0.62907 - acc: 0.6722 -- iter: 16768/22500
Training Step: 615  | total loss: [1m[32m0.62540[0m[0m | time: 105.448s
| Adam | epoch: 002 | loss: 0.62540 - acc: 0.6737 -- iter: 16832/22500
Training Step: 616  | total loss: [1m[32m0.62545[0m[0m | time: 105.853s
| Adam | epoch: 002 | loss: 0.62545 - acc: 0.6751 -- iter: 16896/22500
Training Step: 617  | total loss: [1m[32m0.62657[0m[0m | time: 106.187s
| Adam | epoch: 002 | loss: 0.62657 - acc: 0.6670 -- iter: 16960/22500
Training Step: 618  | total loss: [1m[32m0.62584[0m[0m | time: 106.493s
| Adam | epoch: 002 | loss: 0.62584 - acc: 0.6659 -- iter: 17024/22500
Training Step: 619  | total loss: [1m[32m0.62073[0m[0m | time: 106.777s
| Adam | epoch: 002 | loss: 0.62073 - acc: 0.6728 -- iter: 17088/22500
Training Step: 620  | total loss: [1m[32m0.62267[0m[0m | time: 107.113s
| Adam | epoch: 002 | loss: 0.62267 - acc: 0.6695 -- iter: 17152/22500
Training Step: 621  | total loss: [1m[32m0.61761[0m[0m | time: 107.550s
| Adam | epoch: 002 | loss: 0.61761 - acc: 0.6745 -- iter: 17216/22500
Training Step: 622  | total loss: [1m[32m0.62582[0m[0m | time: 107.951s
| Adam | epoch: 002 | loss: 0.62582 - acc: 0.6601 -- iter: 17280/22500
Training Step: 623  | total loss: [1m[32m0.62424[0m[0m | time: 108.352s
| Adam | epoch: 002 | loss: 0.62424 - acc: 0.6644 -- iter: 17344/22500
Training Step: 624  | total loss: [1m[32m0.62963[0m[0m | time: 108.737s
| Adam | epoch: 002 | loss: 0.62963 - acc: 0.6558 -- iter: 17408/22500
Training Step: 625  | total loss: [1m[32m0.62771[0m[0m | time: 109.122s
| Adam | epoch: 002 | loss: 0.62771 - acc: 0.6558 -- iter: 17472/22500
Training Step: 626  | total loss: [1m[32m0.62636[0m[0m | time: 109.501s
| Adam | epoch: 002 | loss: 0.62636 - acc: 0.6590 -- iter: 17536/22500
Training Step: 627  | total loss: [1m[32m0.63058[0m[0m | time: 109.949s
| Adam | epoch: 002 | loss: 0.63058 - acc: 0.6494 -- iter: 17600/22500
Training Step: 628  | total loss: [1m[32m0.62259[0m[0m | time: 110.468s
| Adam | epoch: 002 | loss: 0.62259 - acc: 0.6610 -- iter: 17664/22500
Training Step: 629  | total loss: [1m[32m0.62437[0m[0m | time: 110.918s
| Adam | epoch: 002 | loss: 0.62437 - acc: 0.6590 -- iter: 17728/22500
Training Step: 630  | total loss: [1m[32m0.62421[0m[0m | time: 111.298s
| Adam | epoch: 002 | loss: 0.62421 - acc: 0.6571 -- iter: 17792/22500
Training Step: 631  | total loss: [1m[32m0.62041[0m[0m | time: 111.650s
| Adam | epoch: 002 | loss: 0.62041 - acc: 0.6602 -- iter: 17856/22500
Training Step: 632  | total loss: [1m[32m0.62502[0m[0m | time: 111.997s
| Adam | epoch: 002 | loss: 0.62502 - acc: 0.6535 -- iter: 17920/22500
Training Step: 633  | total loss: [1m[32m0.62399[0m[0m | time: 112.383s
| Adam | epoch: 002 | loss: 0.62399 - acc: 0.6554 -- iter: 17984/22500
Training Step: 634  | total loss: [1m[32m0.62498[0m[0m | time: 112.783s
| Adam | epoch: 002 | loss: 0.62498 - acc: 0.6554 -- iter: 18048/22500
Training Step: 635  | total loss: [1m[32m0.61910[0m[0m | time: 113.221s
| Adam | epoch: 002 | loss: 0.61910 - acc: 0.6618 -- iter: 18112/22500
Training Step: 636  | total loss: [1m[32m0.61943[0m[0m | time: 113.621s
| Adam | epoch: 002 | loss: 0.61943 - acc: 0.6597 -- iter: 18176/22500
Training Step: 637  | total loss: [1m[32m0.61955[0m[0m | time: 113.999s
| Adam | epoch: 002 | loss: 0.61955 - acc: 0.6624 -- iter: 18240/22500
Training Step: 638  | total loss: [1m[32m0.62443[0m[0m | time: 114.353s
| Adam | epoch: 002 | loss: 0.62443 - acc: 0.6509 -- iter: 18304/22500
Training Step: 639  | total loss: [1m[32m0.62030[0m[0m | time: 114.700s
| Adam | epoch: 002 | loss: 0.62030 - acc: 0.6608 -- iter: 18368/22500
Training Step: 640  | total loss: [1m[32m0.61498[0m[0m | time: 115.070s
| Adam | epoch: 002 | loss: 0.61498 - acc: 0.6775 -- iter: 18432/22500
Training Step: 641  | total loss: [1m[32m0.61652[0m[0m | time: 115.422s
| Adam | epoch: 002 | loss: 0.61652 - acc: 0.6801 -- iter: 18496/22500
Training Step: 642  | total loss: [1m[32m0.61607[0m[0m | time: 115.769s
| Adam | epoch: 002 | loss: 0.61607 - acc: 0.6746 -- iter: 18560/22500
Training Step: 643  | total loss: [1m[32m0.61264[0m[0m | time: 116.123s
| Adam | epoch: 002 | loss: 0.61264 - acc: 0.6774 -- iter: 18624/22500
Training Step: 644  | total loss: [1m[32m0.60411[0m[0m | time: 116.470s
| Adam | epoch: 002 | loss: 0.60411 - acc: 0.6847 -- iter: 18688/22500
Training Step: 645  | total loss: [1m[32m0.59768[0m[0m | time: 116.848s
| Adam | epoch: 002 | loss: 0.59768 - acc: 0.6897 -- iter: 18752/22500
Training Step: 646  | total loss: [1m[32m0.59730[0m[0m | time: 117.202s
| Adam | epoch: 002 | loss: 0.59730 - acc: 0.6910 -- iter: 18816/22500
Training Step: 647  | total loss: [1m[32m0.59464[0m[0m | time: 117.556s
| Adam | epoch: 002 | loss: 0.59464 - acc: 0.6953 -- iter: 18880/22500
Training Step: 648  | total loss: [1m[32m0.58222[0m[0m | time: 117.926s
| Adam | epoch: 002 | loss: 0.58222 - acc: 0.7086 -- iter: 18944/22500
Training Step: 649  | total loss: [1m[32m0.58975[0m[0m | time: 118.294s
| Adam | epoch: 002 | loss: 0.58975 - acc: 0.7034 -- iter: 19008/22500
Training Step: 650  | total loss: [1m[32m0.58200[0m[0m | time: 118.658s
| Adam | epoch: 002 | loss: 0.58200 - acc: 0.7112 -- iter: 19072/22500
Training Step: 651  | total loss: [1m[32m0.57483[0m[0m | time: 119.004s
| Adam | epoch: 002 | loss: 0.57483 - acc: 0.7213 -- iter: 19136/22500
Training Step: 652  | total loss: [1m[32m0.58699[0m[0m | time: 119.365s
| Adam | epoch: 002 | loss: 0.58699 - acc: 0.7148 -- iter: 19200/22500
Training Step: 653  | total loss: [1m[32m0.59019[0m[0m | time: 119.725s
| Adam | epoch: 002 | loss: 0.59019 - acc: 0.7121 -- iter: 19264/22500
Training Step: 654  | total loss: [1m[32m0.58633[0m[0m | time: 120.079s
| Adam | epoch: 002 | loss: 0.58633 - acc: 0.7143 -- iter: 19328/22500
Training Step: 655  | total loss: [1m[32m0.57544[0m[0m | time: 120.444s
| Adam | epoch: 002 | loss: 0.57544 - acc: 0.7226 -- iter: 19392/22500
Training Step: 656  | total loss: [1m[32m0.57352[0m[0m | time: 120.895s
| Adam | epoch: 002 | loss: 0.57352 - acc: 0.7253 -- iter: 19456/22500
Training Step: 657  | total loss: [1m[32m0.57706[0m[0m | time: 121.327s
| Adam | epoch: 002 | loss: 0.57706 - acc: 0.7215 -- iter: 19520/22500
Training Step: 658  | total loss: [1m[32m0.56986[0m[0m | time: 121.674s
| Adam | epoch: 002 | loss: 0.56986 - acc: 0.7275 -- iter: 19584/22500
Training Step: 659  | total loss: [1m[32m0.57005[0m[0m | time: 122.111s
| Adam | epoch: 002 | loss: 0.57005 - acc: 0.7266 -- iter: 19648/22500
Training Step: 660  | total loss: [1m[32m0.58007[0m[0m | time: 122.498s
| Adam | epoch: 002 | loss: 0.58007 - acc: 0.7211 -- iter: 19712/22500
Training Step: 661  | total loss: [1m[32m0.57826[0m[0m | time: 122.817s
| Adam | epoch: 002 | loss: 0.57826 - acc: 0.7225 -- iter: 19776/22500
Training Step: 662  | total loss: [1m[32m0.58489[0m[0m | time: 123.107s
| Adam | epoch: 002 | loss: 0.58489 - acc: 0.7143 -- iter: 19840/22500
Training Step: 663  | total loss: [1m[32m0.58572[0m[0m | time: 123.393s
| Adam | epoch: 002 | loss: 0.58572 - acc: 0.7163 -- iter: 19904/22500
Training Step: 664  | total loss: [1m[32m0.58499[0m[0m | time: 123.699s
| Adam | epoch: 002 | loss: 0.58499 - acc: 0.7181 -- iter: 19968/22500
Training Step: 665  | total loss: [1m[32m0.58660[0m[0m | time: 124.008s
| Adam | epoch: 002 | loss: 0.58660 - acc: 0.7166 -- iter: 20032/22500
Training Step: 666  | total loss: [1m[32m0.58638[0m[0m | time: 124.396s
| Adam | epoch: 002 | loss: 0.58638 - acc: 0.7168 -- iter: 20096/22500
Training Step: 667  | total loss: [1m[32m0.58518[0m[0m | time: 124.796s
| Adam | epoch: 002 | loss: 0.58518 - acc: 0.7170 -- iter: 20160/22500
Training Step: 668  | total loss: [1m[32m0.57863[0m[0m | time: 125.204s
| Adam | epoch: 002 | loss: 0.57863 - acc: 0.7203 -- iter: 20224/22500
Training Step: 669  | total loss: [1m[32m0.57914[0m[0m | time: 125.581s
| Adam | epoch: 002 | loss: 0.57914 - acc: 0.7155 -- iter: 20288/22500
Training Step: 670  | total loss: [1m[32m0.57106[0m[0m | time: 125.965s
| Adam | epoch: 002 | loss: 0.57106 - acc: 0.7236 -- iter: 20352/22500
Training Step: 671  | total loss: [1m[32m0.56959[0m[0m | time: 126.346s
| Adam | epoch: 002 | loss: 0.56959 - acc: 0.7278 -- iter: 20416/22500
Training Step: 672  | total loss: [1m[32m0.55966[0m[0m | time: 126.725s
| Adam | epoch: 002 | loss: 0.55966 - acc: 0.7378 -- iter: 20480/22500
Training Step: 673  | total loss: [1m[32m0.55791[0m[0m | time: 127.289s
| Adam | epoch: 002 | loss: 0.55791 - acc: 0.7391 -- iter: 20544/22500
Training Step: 674  | total loss: [1m[32m0.56551[0m[0m | time: 127.680s
| Adam | epoch: 002 | loss: 0.56551 - acc: 0.7292 -- iter: 20608/22500
Training Step: 675  | total loss: [1m[32m0.56386[0m[0m | time: 128.036s
| Adam | epoch: 002 | loss: 0.56386 - acc: 0.7329 -- iter: 20672/22500
Training Step: 676  | total loss: [1m[32m0.55651[0m[0m | time: 128.393s
| Adam | epoch: 002 | loss: 0.55651 - acc: 0.7424 -- iter: 20736/22500
Training Step: 677  | total loss: [1m[32m0.54688[0m[0m | time: 128.789s
| Adam | epoch: 002 | loss: 0.54688 - acc: 0.7525 -- iter: 20800/22500
Training Step: 678  | total loss: [1m[32m0.54909[0m[0m | time: 129.163s
| Adam | epoch: 002 | loss: 0.54909 - acc: 0.7554 -- iter: 20864/22500
Training Step: 679  | total loss: [1m[32m0.53378[0m[0m | time: 129.546s
| Adam | epoch: 002 | loss: 0.53378 - acc: 0.7674 -- iter: 20928/22500
Training Step: 680  | total loss: [1m[32m0.53334[0m[0m | time: 129.924s
| Adam | epoch: 002 | loss: 0.53334 - acc: 0.7687 -- iter: 20992/22500
Training Step: 681  | total loss: [1m[32m0.52923[0m[0m | time: 130.281s
| Adam | epoch: 002 | loss: 0.52923 - acc: 0.7700 -- iter: 21056/22500
Training Step: 682  | total loss: [1m[32m0.52420[0m[0m | time: 130.607s
| Adam | epoch: 002 | loss: 0.52420 - acc: 0.7742 -- iter: 21120/22500
Training Step: 683  | total loss: [1m[32m0.51804[0m[0m | time: 130.931s
| Adam | epoch: 002 | loss: 0.51804 - acc: 0.7812 -- iter: 21184/22500
Training Step: 684  | total loss: [1m[32m0.51933[0m[0m | time: 131.287s
| Adam | epoch: 002 | loss: 0.51933 - acc: 0.7781 -- iter: 21248/22500
Training Step: 685  | total loss: [1m[32m0.50934[0m[0m | time: 131.610s
| Adam | epoch: 002 | loss: 0.50934 - acc: 0.7878 -- iter: 21312/22500
Training Step: 686  | total loss: [1m[32m0.51167[0m[0m | time: 132.025s
| Adam | epoch: 002 | loss: 0.51167 - acc: 0.7856 -- iter: 21376/22500
Training Step: 687  | total loss: [1m[32m0.51165[0m[0m | time: 132.451s
| Adam | epoch: 002 | loss: 0.51165 - acc: 0.7851 -- iter: 21440/22500
Training Step: 688  | total loss: [1m[32m0.51057[0m[0m | time: 132.860s
| Adam | epoch: 002 | loss: 0.51057 - acc: 0.7847 -- iter: 21504/22500
Training Step: 689  | total loss: [1m[32m0.50366[0m[0m | time: 133.280s
| Adam | epoch: 002 | loss: 0.50366 - acc: 0.7906 -- iter: 21568/22500
Training Step: 690  | total loss: [1m[32m0.49555[0m[0m | time: 133.713s
| Adam | epoch: 002 | loss: 0.49555 - acc: 0.7928 -- iter: 21632/22500
Training Step: 691  | total loss: [1m[32m0.49628[0m[0m | time: 134.147s
| Adam | epoch: 002 | loss: 0.49628 - acc: 0.7917 -- iter: 21696/22500
Training Step: 692  | total loss: [1m[32m0.50832[0m[0m | time: 134.559s
| Adam | epoch: 002 | loss: 0.50832 - acc: 0.7844 -- iter: 21760/22500
Training Step: 693  | total loss: [1m[32m0.50695[0m[0m | time: 134.978s
| Adam | epoch: 002 | loss: 0.50695 - acc: 0.7841 -- iter: 21824/22500
Training Step: 694  | total loss: [1m[32m0.50917[0m[0m | time: 135.396s
| Adam | epoch: 002 | loss: 0.50917 - acc: 0.7807 -- iter: 21888/22500
Training Step: 695  | total loss: [1m[32m0.51045[0m[0m | time: 135.804s
| Adam | epoch: 002 | loss: 0.51045 - acc: 0.7760 -- iter: 21952/22500
Training Step: 696  | total loss: [1m[32m0.51296[0m[0m | time: 136.216s
| Adam | epoch: 002 | loss: 0.51296 - acc: 0.7703 -- iter: 22016/22500
Training Step: 697  | total loss: [1m[32m0.50559[0m[0m | time: 136.621s
| Adam | epoch: 002 | loss: 0.50559 - acc: 0.7745 -- iter: 22080/22500
Training Step: 698  | total loss: [1m[32m0.50443[0m[0m | time: 137.039s
| Adam | epoch: 002 | loss: 0.50443 - acc: 0.7705 -- iter: 22144/22500
Training Step: 699  | total loss: [1m[32m0.51549[0m[0m | time: 137.449s
| Adam | epoch: 002 | loss: 0.51549 - acc: 0.7606 -- iter: 22208/22500
Training Step: 700  | total loss: [1m[32m0.52200[0m[0m | time: 137.853s
| Adam | epoch: 002 | loss: 0.52200 - acc: 0.7533 -- iter: 22272/22500
Training Step: 701  | total loss: [1m[32m0.52160[0m[0m | time: 138.281s
| Adam | epoch: 002 | loss: 0.52160 - acc: 0.7561 -- iter: 22336/22500
Training Step: 702  | total loss: [1m[32m0.51955[0m[0m | time: 138.703s
| Adam | epoch: 002 | loss: 0.51955 - acc: 0.7571 -- iter: 22400/22500
Training Step: 703  | total loss: [1m[32m0.51319[0m[0m | time: 139.121s
| Adam | epoch: 002 | loss: 0.51319 - acc: 0.7642 -- iter: 22464/22500
Training Step: 704  | total loss: [1m[32m0.51237[0m[0m | time: 143.025s
| Adam | epoch: 002 | loss: 0.51237 - acc: 0.7674 | val_loss: 0.62151 - val_acc: 0.7040 -- iter: 22500/22500
--
Training Step: 705  | total loss: [1m[32m0.51732[0m[0m | time: 0.326s
| Adam | epoch: 003 | loss: 0.51732 - acc: 0.7626 -- iter: 00064/22500
Training Step: 706  | total loss: [1m[32m0.53631[0m[0m | time: 0.650s
| Adam | epoch: 003 | loss: 0.53631 - acc: 0.7502 -- iter: 00128/22500
Training Step: 707  | total loss: [1m[32m0.55098[0m[0m | time: 1.061s
| Adam | epoch: 003 | loss: 0.55098 - acc: 0.7419 -- iter: 00192/22500
Training Step: 708  | total loss: [1m[32m0.54265[0m[0m | time: 1.473s
| Adam | epoch: 003 | loss: 0.54265 - acc: 0.7489 -- iter: 00256/22500
Training Step: 709  | total loss: [1m[32m0.53427[0m[0m | time: 1.896s
| Adam | epoch: 003 | loss: 0.53427 - acc: 0.7553 -- iter: 00320/22500
Training Step: 710  | total loss: [1m[32m0.52541[0m[0m | time: 2.307s
| Adam | epoch: 003 | loss: 0.52541 - acc: 0.7594 -- iter: 00384/22500
Training Step: 711  | total loss: [1m[32m0.51201[0m[0m | time: 2.712s
| Adam | epoch: 003 | loss: 0.51201 - acc: 0.7710 -- iter: 00448/22500
Training Step: 712  | total loss: [1m[32m0.51327[0m[0m | time: 3.079s
| Adam | epoch: 003 | loss: 0.51327 - acc: 0.7689 -- iter: 00512/22500
Training Step: 713  | total loss: [1m[32m0.50577[0m[0m | time: 3.418s
| Adam | epoch: 003 | loss: 0.50577 - acc: 0.7748 -- iter: 00576/22500
Training Step: 714  | total loss: [1m[32m0.50509[0m[0m | time: 3.760s
| Adam | epoch: 003 | loss: 0.50509 - acc: 0.7770 -- iter: 00640/22500
Training Step: 715  | total loss: [1m[32m0.49420[0m[0m | time: 4.087s
| Adam | epoch: 003 | loss: 0.49420 - acc: 0.7868 -- iter: 00704/22500
Training Step: 716  | total loss: [1m[32m0.48980[0m[0m | time: 4.474s
| Adam | epoch: 003 | loss: 0.48980 - acc: 0.7878 -- iter: 00768/22500
Training Step: 717  | total loss: [1m[32m0.48783[0m[0m | time: 4.889s
| Adam | epoch: 003 | loss: 0.48783 - acc: 0.7903 -- iter: 00832/22500
Training Step: 718  | total loss: [1m[32m0.48936[0m[0m | time: 5.297s
| Adam | epoch: 003 | loss: 0.48936 - acc: 0.7925 -- iter: 00896/22500
Training Step: 719  | total loss: [1m[32m0.48841[0m[0m | time: 5.707s
| Adam | epoch: 003 | loss: 0.48841 - acc: 0.7914 -- iter: 00960/22500
Training Step: 720  | total loss: [1m[32m0.48463[0m[0m | time: 6.098s
| Adam | epoch: 003 | loss: 0.48463 - acc: 0.7935 -- iter: 01024/22500
Training Step: 721  | total loss: [1m[32m0.47755[0m[0m | time: 6.421s
| Adam | epoch: 003 | loss: 0.47755 - acc: 0.7985 -- iter: 01088/22500
Training Step: 722  | total loss: [1m[32m0.46745[0m[0m | time: 6.749s
| Adam | epoch: 003 | loss: 0.46745 - acc: 0.8046 -- iter: 01152/22500
Training Step: 723  | total loss: [1m[32m0.45972[0m[0m | time: 7.075s
| Adam | epoch: 003 | loss: 0.45972 - acc: 0.8085 -- iter: 01216/22500
Training Step: 724  | total loss: [1m[32m0.46866[0m[0m | time: 7.467s
| Adam | epoch: 003 | loss: 0.46866 - acc: 0.8027 -- iter: 01280/22500
Training Step: 725  | total loss: [1m[32m0.47342[0m[0m | time: 7.902s
| Adam | epoch: 003 | loss: 0.47342 - acc: 0.7974 -- iter: 01344/22500
Training Step: 726  | total loss: [1m[32m0.47618[0m[0m | time: 8.332s
| Adam | epoch: 003 | loss: 0.47618 - acc: 0.7958 -- iter: 01408/22500
Training Step: 727  | total loss: [1m[32m0.46843[0m[0m | time: 8.744s
| Adam | epoch: 003 | loss: 0.46843 - acc: 0.7990 -- iter: 01472/22500
Training Step: 728  | total loss: [1m[32m0.46223[0m[0m | time: 9.154s
| Adam | epoch: 003 | loss: 0.46223 - acc: 0.8035 -- iter: 01536/22500
Training Step: 729  | total loss: [1m[32m0.46793[0m[0m | time: 9.566s
| Adam | epoch: 003 | loss: 0.46793 - acc: 0.7966 -- iter: 01600/22500
Training Step: 730  | total loss: [1m[32m0.46055[0m[0m | time: 9.978s
| Adam | epoch: 003 | loss: 0.46055 - acc: 0.8029 -- iter: 01664/22500
Training Step: 731  | total loss: [1m[32m0.46064[0m[0m | time: 10.384s
| Adam | epoch: 003 | loss: 0.46064 - acc: 0.7991 -- iter: 01728/22500
Training Step: 732  | total loss: [1m[32m0.46276[0m[0m | time: 10.766s
| Adam | epoch: 003 | loss: 0.46276 - acc: 0.7989 -- iter: 01792/22500
Training Step: 733  | total loss: [1m[32m0.47786[0m[0m | time: 11.093s
| Adam | epoch: 003 | loss: 0.47786 - acc: 0.7893 -- iter: 01856/22500
Training Step: 734  | total loss: [1m[32m0.47939[0m[0m | time: 11.416s
| Adam | epoch: 003 | loss: 0.47939 - acc: 0.7885 -- iter: 01920/22500
Training Step: 735  | total loss: [1m[32m0.48488[0m[0m | time: 11.740s
| Adam | epoch: 003 | loss: 0.48488 - acc: 0.7909 -- iter: 01984/22500
Training Step: 736  | total loss: [1m[32m0.48714[0m[0m | time: 12.142s
| Adam | epoch: 003 | loss: 0.48714 - acc: 0.7915 -- iter: 02048/22500
Training Step: 737  | total loss: [1m[32m0.49941[0m[0m | time: 12.580s
| Adam | epoch: 003 | loss: 0.49941 - acc: 0.7811 -- iter: 02112/22500
Training Step: 738  | total loss: [1m[32m0.50619[0m[0m | time: 12.999s
| Adam | epoch: 003 | loss: 0.50619 - acc: 0.7780 -- iter: 02176/22500
Training Step: 739  | total loss: [1m[32m0.50649[0m[0m | time: 13.417s
| Adam | epoch: 003 | loss: 0.50649 - acc: 0.7768 -- iter: 02240/22500
Training Step: 740  | total loss: [1m[32m0.50469[0m[0m | time: 13.823s
| Adam | epoch: 003 | loss: 0.50469 - acc: 0.7757 -- iter: 02304/22500
Training Step: 741  | total loss: [1m[32m0.50721[0m[0m | time: 14.237s
| Adam | epoch: 003 | loss: 0.50721 - acc: 0.7809 -- iter: 02368/22500
Training Step: 742  | total loss: [1m[32m0.49658[0m[0m | time: 14.656s
| Adam | epoch: 003 | loss: 0.49658 - acc: 0.7856 -- iter: 02432/22500
Training Step: 743  | total loss: [1m[32m0.50237[0m[0m | time: 15.069s
| Adam | epoch: 003 | loss: 0.50237 - acc: 0.7789 -- iter: 02496/22500
Training Step: 744  | total loss: [1m[32m0.49412[0m[0m | time: 15.438s
| Adam | epoch: 003 | loss: 0.49412 - acc: 0.7807 -- iter: 02560/22500
Training Step: 745  | total loss: [1m[32m0.49319[0m[0m | time: 15.766s
| Adam | epoch: 003 | loss: 0.49319 - acc: 0.7808 -- iter: 02624/22500
Training Step: 746  | total loss: [1m[32m0.48701[0m[0m | time: 16.097s
| Adam | epoch: 003 | loss: 0.48701 - acc: 0.7871 -- iter: 02688/22500
Training Step: 747  | total loss: [1m[32m0.47467[0m[0m | time: 16.445s
| Adam | epoch: 003 | loss: 0.47467 - acc: 0.7943 -- iter: 02752/22500
Training Step: 748  | total loss: [1m[32m0.46964[0m[0m | time: 17.027s
| Adam | epoch: 003 | loss: 0.46964 - acc: 0.7993 -- iter: 02816/22500
Training Step: 749  | total loss: [1m[32m0.47324[0m[0m | time: 17.607s
| Adam | epoch: 003 | loss: 0.47324 - acc: 0.7943 -- iter: 02880/22500
Training Step: 750  | total loss: [1m[32m0.45815[0m[0m | time: 18.218s
| Adam | epoch: 003 | loss: 0.45815 - acc: 0.8055 -- iter: 02944/22500
Training Step: 751  | total loss: [1m[32m0.46779[0m[0m | time: 18.671s
| Adam | epoch: 003 | loss: 0.46779 - acc: 0.8031 -- iter: 03008/22500
Training Step: 752  | total loss: [1m[32m0.45551[0m[0m | time: 19.021s
| Adam | epoch: 003 | loss: 0.45551 - acc: 0.8103 -- iter: 03072/22500
Training Step: 753  | total loss: [1m[32m0.45194[0m[0m | time: 19.374s
| Adam | epoch: 003 | loss: 0.45194 - acc: 0.8168 -- iter: 03136/22500
Training Step: 754  | total loss: [1m[32m0.44081[0m[0m | time: 19.711s
| Adam | epoch: 003 | loss: 0.44081 - acc: 0.8241 -- iter: 03200/22500
Training Step: 755  | total loss: [1m[32m0.44894[0m[0m | time: 20.047s
| Adam | epoch: 003 | loss: 0.44894 - acc: 0.8183 -- iter: 03264/22500
Training Step: 756  | total loss: [1m[32m0.45121[0m[0m | time: 20.388s
| Adam | epoch: 003 | loss: 0.45121 - acc: 0.8177 -- iter: 03328/22500
Training Step: 757  | total loss: [1m[32m0.45744[0m[0m | time: 20.723s
| Adam | epoch: 003 | loss: 0.45744 - acc: 0.8172 -- iter: 03392/22500
Training Step: 758  | total loss: [1m[32m0.45618[0m[0m | time: 21.096s
| Adam | epoch: 003 | loss: 0.45618 - acc: 0.8167 -- iter: 03456/22500
Training Step: 759  | total loss: [1m[32m0.44454[0m[0m | time: 21.551s
| Adam | epoch: 003 | loss: 0.44454 - acc: 0.8210 -- iter: 03520/22500
Training Step: 760  | total loss: [1m[32m0.43925[0m[0m | time: 21.973s
| Adam | epoch: 003 | loss: 0.43925 - acc: 0.8248 -- iter: 03584/22500
Training Step: 761  | total loss: [1m[32m0.44570[0m[0m | time: 22.393s
| Adam | epoch: 003 | loss: 0.44570 - acc: 0.8236 -- iter: 03648/22500
Training Step: 762  | total loss: [1m[32m0.45076[0m[0m | time: 22.806s
| Adam | epoch: 003 | loss: 0.45076 - acc: 0.8194 -- iter: 03712/22500
Training Step: 763  | total loss: [1m[32m0.45141[0m[0m | time: 23.227s
| Adam | epoch: 003 | loss: 0.45141 - acc: 0.8187 -- iter: 03776/22500
Training Step: 764  | total loss: [1m[32m0.45202[0m[0m | time: 23.625s
| Adam | epoch: 003 | loss: 0.45202 - acc: 0.8165 -- iter: 03840/22500
Training Step: 765  | total loss: [1m[32m0.47555[0m[0m | time: 24.037s
| Adam | epoch: 003 | loss: 0.47555 - acc: 0.8020 -- iter: 03904/22500
Training Step: 766  | total loss: [1m[32m0.47944[0m[0m | time: 24.450s
| Adam | epoch: 003 | loss: 0.47944 - acc: 0.8015 -- iter: 03968/22500
Training Step: 767  | total loss: [1m[32m0.47721[0m[0m | time: 24.860s
| Adam | epoch: 003 | loss: 0.47721 - acc: 0.8026 -- iter: 04032/22500
Training Step: 768  | total loss: [1m[32m0.46679[0m[0m | time: 25.275s
| Adam | epoch: 003 | loss: 0.46679 - acc: 0.8083 -- iter: 04096/22500
Training Step: 769  | total loss: [1m[32m0.47706[0m[0m | time: 25.684s
| Adam | epoch: 003 | loss: 0.47706 - acc: 0.7978 -- iter: 04160/22500
Training Step: 770  | total loss: [1m[32m0.46833[0m[0m | time: 26.136s
| Adam | epoch: 003 | loss: 0.46833 - acc: 0.7977 -- iter: 04224/22500
Training Step: 771  | total loss: [1m[32m0.47330[0m[0m | time: 26.573s
| Adam | epoch: 003 | loss: 0.47330 - acc: 0.7929 -- iter: 04288/22500
Training Step: 772  | total loss: [1m[32m0.46195[0m[0m | time: 27.007s
| Adam | epoch: 003 | loss: 0.46195 - acc: 0.8011 -- iter: 04352/22500
Training Step: 773  | total loss: [1m[32m0.45764[0m[0m | time: 27.419s
| Adam | epoch: 003 | loss: 0.45764 - acc: 0.8023 -- iter: 04416/22500
Training Step: 774  | total loss: [1m[32m0.46577[0m[0m | time: 27.793s
| Adam | epoch: 003 | loss: 0.46577 - acc: 0.7986 -- iter: 04480/22500
Training Step: 775  | total loss: [1m[32m0.47521[0m[0m | time: 28.127s
| Adam | epoch: 003 | loss: 0.47521 - acc: 0.7922 -- iter: 04544/22500
Training Step: 776  | total loss: [1m[32m0.48036[0m[0m | time: 28.458s
| Adam | epoch: 003 | loss: 0.48036 - acc: 0.7895 -- iter: 04608/22500
Training Step: 777  | total loss: [1m[32m0.48165[0m[0m | time: 28.782s
| Adam | epoch: 003 | loss: 0.48165 - acc: 0.7887 -- iter: 04672/22500
Training Step: 778  | total loss: [1m[32m0.48512[0m[0m | time: 29.178s
| Adam | epoch: 003 | loss: 0.48512 - acc: 0.7833 -- iter: 04736/22500
Training Step: 779  | total loss: [1m[32m0.48997[0m[0m | time: 29.584s
| Adam | epoch: 003 | loss: 0.48997 - acc: 0.7815 -- iter: 04800/22500
Training Step: 780  | total loss: [1m[32m0.49358[0m[0m | time: 29.987s
| Adam | epoch: 003 | loss: 0.49358 - acc: 0.7721 -- iter: 04864/22500
Training Step: 781  | total loss: [1m[32m0.48836[0m[0m | time: 30.429s
| Adam | epoch: 003 | loss: 0.48836 - acc: 0.7777 -- iter: 04928/22500
Training Step: 782  | total loss: [1m[32m0.47242[0m[0m | time: 30.858s
| Adam | epoch: 003 | loss: 0.47242 - acc: 0.7906 -- iter: 04992/22500
Training Step: 783  | total loss: [1m[32m0.47323[0m[0m | time: 31.269s
| Adam | epoch: 003 | loss: 0.47323 - acc: 0.7943 -- iter: 05056/22500
Training Step: 784  | total loss: [1m[32m0.47196[0m[0m | time: 31.683s
| Adam | epoch: 003 | loss: 0.47196 - acc: 0.7977 -- iter: 05120/22500
Training Step: 785  | total loss: [1m[32m0.46629[0m[0m | time: 32.093s
| Adam | epoch: 003 | loss: 0.46629 - acc: 0.8007 -- iter: 05184/22500
Training Step: 786  | total loss: [1m[32m0.46100[0m[0m | time: 32.514s
| Adam | epoch: 003 | loss: 0.46100 - acc: 0.8066 -- iter: 05248/22500
Training Step: 787  | total loss: [1m[32m0.46115[0m[0m | time: 32.937s
| Adam | epoch: 003 | loss: 0.46115 - acc: 0.8041 -- iter: 05312/22500
Training Step: 788  | total loss: [1m[32m0.47012[0m[0m | time: 33.347s
| Adam | epoch: 003 | loss: 0.47012 - acc: 0.7987 -- iter: 05376/22500
Training Step: 789  | total loss: [1m[32m0.46727[0m[0m | time: 33.757s
| Adam | epoch: 003 | loss: 0.46727 - acc: 0.8016 -- iter: 05440/22500
Training Step: 790  | total loss: [1m[32m0.46527[0m[0m | time: 34.176s
| Adam | epoch: 003 | loss: 0.46527 - acc: 0.8027 -- iter: 05504/22500
Training Step: 791  | total loss: [1m[32m0.46986[0m[0m | time: 34.589s
| Adam | epoch: 003 | loss: 0.46986 - acc: 0.8006 -- iter: 05568/22500
Training Step: 792  | total loss: [1m[32m0.46424[0m[0m | time: 35.004s
| Adam | epoch: 003 | loss: 0.46424 - acc: 0.8049 -- iter: 05632/22500
Training Step: 793  | total loss: [1m[32m0.46325[0m[0m | time: 35.474s
| Adam | epoch: 003 | loss: 0.46325 - acc: 0.8072 -- iter: 05696/22500
Training Step: 794  | total loss: [1m[32m0.47264[0m[0m | time: 35.991s
| Adam | epoch: 003 | loss: 0.47264 - acc: 0.8015 -- iter: 05760/22500
Training Step: 795  | total loss: [1m[32m0.46616[0m[0m | time: 36.449s
| Adam | epoch: 003 | loss: 0.46616 - acc: 0.8057 -- iter: 05824/22500
Training Step: 796  | total loss: [1m[32m0.46867[0m[0m | time: 36.866s
| Adam | epoch: 003 | loss: 0.46867 - acc: 0.8017 -- iter: 05888/22500
Training Step: 797  | total loss: [1m[32m0.47098[0m[0m | time: 37.297s
| Adam | epoch: 003 | loss: 0.47098 - acc: 0.8012 -- iter: 05952/22500
Training Step: 798  | total loss: [1m[32m0.46932[0m[0m | time: 37.710s
| Adam | epoch: 003 | loss: 0.46932 - acc: 0.7977 -- iter: 06016/22500
Training Step: 799  | total loss: [1m[32m0.46674[0m[0m | time: 38.121s
| Adam | epoch: 003 | loss: 0.46674 - acc: 0.7945 -- iter: 06080/22500
Training Step: 800  | total loss: [1m[32m0.46785[0m[0m | time: 42.381s
| Adam | epoch: 003 | loss: 0.46785 - acc: 0.7947 | val_loss: 0.50066 - val_acc: 0.7672 -- iter: 06144/22500
--
Training Step: 801  | total loss: [1m[32m0.46975[0m[0m | time: 42.796s
| Adam | epoch: 003 | loss: 0.46975 - acc: 0.7887 -- iter: 06208/22500
Training Step: 802  | total loss: [1m[32m0.46527[0m[0m | time: 43.187s
| Adam | epoch: 003 | loss: 0.46527 - acc: 0.7942 -- iter: 06272/22500
Training Step: 803  | total loss: [1m[32m0.46778[0m[0m | time: 43.532s
| Adam | epoch: 003 | loss: 0.46778 - acc: 0.7929 -- iter: 06336/22500
Training Step: 804  | total loss: [1m[32m0.45897[0m[0m | time: 43.885s
| Adam | epoch: 003 | loss: 0.45897 - acc: 0.7995 -- iter: 06400/22500
Training Step: 805  | total loss: [1m[32m0.45158[0m[0m | time: 44.236s
| Adam | epoch: 003 | loss: 0.45158 - acc: 0.8071 -- iter: 06464/22500
Training Step: 806  | total loss: [1m[32m0.45647[0m[0m | time: 44.635s
| Adam | epoch: 003 | loss: 0.45647 - acc: 0.7998 -- iter: 06528/22500
Training Step: 807  | total loss: [1m[32m0.46176[0m[0m | time: 45.049s
| Adam | epoch: 003 | loss: 0.46176 - acc: 0.7980 -- iter: 06592/22500
Training Step: 808  | total loss: [1m[32m0.45533[0m[0m | time: 45.479s
| Adam | epoch: 003 | loss: 0.45533 - acc: 0.7994 -- iter: 06656/22500
Training Step: 809  | total loss: [1m[32m0.44202[0m[0m | time: 45.905s
| Adam | epoch: 003 | loss: 0.44202 - acc: 0.8085 -- iter: 06720/22500
Training Step: 810  | total loss: [1m[32m0.44715[0m[0m | time: 46.325s
| Adam | epoch: 003 | loss: 0.44715 - acc: 0.8058 -- iter: 06784/22500
Training Step: 811  | total loss: [1m[32m0.44226[0m[0m | time: 46.737s
| Adam | epoch: 003 | loss: 0.44226 - acc: 0.8112 -- iter: 06848/22500
Training Step: 812  | total loss: [1m[32m0.44521[0m[0m | time: 47.161s
| Adam | epoch: 003 | loss: 0.44521 - acc: 0.8082 -- iter: 06912/22500
Training Step: 813  | total loss: [1m[32m0.45556[0m[0m | time: 47.575s
| Adam | epoch: 003 | loss: 0.45556 - acc: 0.8008 -- iter: 06976/22500
Training Step: 814  | total loss: [1m[32m0.44133[0m[0m | time: 47.985s
| Adam | epoch: 003 | loss: 0.44133 - acc: 0.8098 -- iter: 07040/22500
Training Step: 815  | total loss: [1m[32m0.42425[0m[0m | time: 48.417s
| Adam | epoch: 003 | loss: 0.42425 - acc: 0.8225 -- iter: 07104/22500
Training Step: 816  | total loss: [1m[32m0.40991[0m[0m | time: 48.879s
| Adam | epoch: 003 | loss: 0.40991 - acc: 0.8294 -- iter: 07168/22500
Training Step: 817  | total loss: [1m[32m0.41454[0m[0m | time: 49.319s
| Adam | epoch: 003 | loss: 0.41454 - acc: 0.8292 -- iter: 07232/22500
Training Step: 818  | total loss: [1m[32m0.42299[0m[0m | time: 49.737s
| Adam | epoch: 003 | loss: 0.42299 - acc: 0.8244 -- iter: 07296/22500
Training Step: 819  | total loss: [1m[32m0.41584[0m[0m | time: 50.152s
| Adam | epoch: 003 | loss: 0.41584 - acc: 0.8279 -- iter: 07360/22500
Training Step: 820  | total loss: [1m[32m0.42195[0m[0m | time: 50.560s
| Adam | epoch: 003 | loss: 0.42195 - acc: 0.8264 -- iter: 07424/22500
Training Step: 821  | total loss: [1m[32m0.41930[0m[0m | time: 50.933s
| Adam | epoch: 003 | loss: 0.41930 - acc: 0.8297 -- iter: 07488/22500
Training Step: 822  | total loss: [1m[32m0.41438[0m[0m | time: 51.259s
| Adam | epoch: 003 | loss: 0.41438 - acc: 0.8373 -- iter: 07552/22500
Training Step: 823  | total loss: [1m[32m0.41957[0m[0m | time: 51.590s
| Adam | epoch: 003 | loss: 0.41957 - acc: 0.8286 -- iter: 07616/22500
Training Step: 824  | total loss: [1m[32m0.43308[0m[0m | time: 51.924s
| Adam | epoch: 003 | loss: 0.43308 - acc: 0.8176 -- iter: 07680/22500
Training Step: 825  | total loss: [1m[32m0.44181[0m[0m | time: 52.323s
| Adam | epoch: 003 | loss: 0.44181 - acc: 0.8140 -- iter: 07744/22500
Training Step: 826  | total loss: [1m[32m0.45678[0m[0m | time: 52.735s
| Adam | epoch: 003 | loss: 0.45678 - acc: 0.8091 -- iter: 07808/22500
Training Step: 827  | total loss: [1m[32m0.44352[0m[0m | time: 53.168s
| Adam | epoch: 003 | loss: 0.44352 - acc: 0.8173 -- iter: 07872/22500
Training Step: 828  | total loss: [1m[32m0.44975[0m[0m | time: 53.614s
| Adam | epoch: 003 | loss: 0.44975 - acc: 0.8153 -- iter: 07936/22500
Training Step: 829  | total loss: [1m[32m0.45511[0m[0m | time: 54.026s
| Adam | epoch: 003 | loss: 0.45511 - acc: 0.8119 -- iter: 08000/22500
Training Step: 830  | total loss: [1m[32m0.45313[0m[0m | time: 54.446s
| Adam | epoch: 003 | loss: 0.45313 - acc: 0.8104 -- iter: 08064/22500
Training Step: 831  | total loss: [1m[32m0.45452[0m[0m | time: 54.865s
| Adam | epoch: 003 | loss: 0.45452 - acc: 0.8090 -- iter: 08128/22500
Training Step: 832  | total loss: [1m[32m0.46453[0m[0m | time: 55.277s
| Adam | epoch: 003 | loss: 0.46453 - acc: 0.8062 -- iter: 08192/22500
Training Step: 833  | total loss: [1m[32m0.45483[0m[0m | time: 55.685s
| Adam | epoch: 003 | loss: 0.45483 - acc: 0.8100 -- iter: 08256/22500
Training Step: 834  | total loss: [1m[32m0.45687[0m[0m | time: 56.100s
| Adam | epoch: 003 | loss: 0.45687 - acc: 0.8087 -- iter: 08320/22500
Training Step: 835  | total loss: [1m[32m0.45112[0m[0m | time: 56.516s
| Adam | epoch: 003 | loss: 0.45112 - acc: 0.8106 -- iter: 08384/22500
Training Step: 836  | total loss: [1m[32m0.44677[0m[0m | time: 56.929s
| Adam | epoch: 003 | loss: 0.44677 - acc: 0.8124 -- iter: 08448/22500
Training Step: 837  | total loss: [1m[32m0.42835[0m[0m | time: 57.361s
| Adam | epoch: 003 | loss: 0.42835 - acc: 0.8202 -- iter: 08512/22500
Training Step: 838  | total loss: [1m[32m0.42411[0m[0m | time: 57.776s
| Adam | epoch: 003 | loss: 0.42411 - acc: 0.8225 -- iter: 08576/22500
Training Step: 839  | total loss: [1m[32m0.42778[0m[0m | time: 58.188s
| Adam | epoch: 003 | loss: 0.42778 - acc: 0.8231 -- iter: 08640/22500
Training Step: 840  | total loss: [1m[32m0.43725[0m[0m | time: 58.582s
| Adam | epoch: 003 | loss: 0.43725 - acc: 0.8174 -- iter: 08704/22500
Training Step: 841  | total loss: [1m[32m0.44688[0m[0m | time: 58.911s
| Adam | epoch: 003 | loss: 0.44688 - acc: 0.8091 -- iter: 08768/22500
Training Step: 842  | total loss: [1m[32m0.43364[0m[0m | time: 59.237s
| Adam | epoch: 003 | loss: 0.43364 - acc: 0.8203 -- iter: 08832/22500
Training Step: 843  | total loss: [1m[32m0.44380[0m[0m | time: 59.572s
| Adam | epoch: 003 | loss: 0.44380 - acc: 0.8149 -- iter: 08896/22500
Training Step: 844  | total loss: [1m[32m0.45936[0m[0m | time: 59.954s
| Adam | epoch: 003 | loss: 0.45936 - acc: 0.8037 -- iter: 08960/22500
Training Step: 845  | total loss: [1m[32m0.45814[0m[0m | time: 60.359s
| Adam | epoch: 003 | loss: 0.45814 - acc: 0.8015 -- iter: 09024/22500
Training Step: 846  | total loss: [1m[32m0.46012[0m[0m | time: 60.773s
| Adam | epoch: 003 | loss: 0.46012 - acc: 0.7994 -- iter: 09088/22500
Training Step: 847  | total loss: [1m[32m0.45327[0m[0m | time: 61.190s
| Adam | epoch: 003 | loss: 0.45327 - acc: 0.8007 -- iter: 09152/22500
Training Step: 848  | total loss: [1m[32m0.44706[0m[0m | time: 61.607s
| Adam | epoch: 003 | loss: 0.44706 - acc: 0.8066 -- iter: 09216/22500
Training Step: 849  | total loss: [1m[32m0.43551[0m[0m | time: 62.027s
| Adam | epoch: 003 | loss: 0.43551 - acc: 0.8166 -- iter: 09280/22500
Training Step: 850  | total loss: [1m[32m0.43060[0m[0m | time: 62.486s
| Adam | epoch: 003 | loss: 0.43060 - acc: 0.8177 -- iter: 09344/22500
Training Step: 851  | total loss: [1m[32m0.43038[0m[0m | time: 62.935s
| Adam | epoch: 003 | loss: 0.43038 - acc: 0.8172 -- iter: 09408/22500
Training Step: 852  | total loss: [1m[32m0.42537[0m[0m | time: 63.314s
| Adam | epoch: 003 | loss: 0.42537 - acc: 0.8199 -- iter: 09472/22500
Training Step: 853  | total loss: [1m[32m0.43407[0m[0m | time: 63.641s
| Adam | epoch: 003 | loss: 0.43407 - acc: 0.8160 -- iter: 09536/22500
Training Step: 854  | total loss: [1m[32m0.44609[0m[0m | time: 63.973s
| Adam | epoch: 003 | loss: 0.44609 - acc: 0.8094 -- iter: 09600/22500
Training Step: 855  | total loss: [1m[32m0.44979[0m[0m | time: 64.308s
| Adam | epoch: 003 | loss: 0.44979 - acc: 0.8066 -- iter: 09664/22500
Training Step: 856  | total loss: [1m[32m0.44950[0m[0m | time: 64.700s
| Adam | epoch: 003 | loss: 0.44950 - acc: 0.8072 -- iter: 09728/22500
Training Step: 857  | total loss: [1m[32m0.46186[0m[0m | time: 65.113s
| Adam | epoch: 003 | loss: 0.46186 - acc: 0.7968 -- iter: 09792/22500
Training Step: 858  | total loss: [1m[32m0.45531[0m[0m | time: 65.537s
| Adam | epoch: 003 | loss: 0.45531 - acc: 0.8015 -- iter: 09856/22500
Training Step: 859  | total loss: [1m[32m0.45047[0m[0m | time: 65.952s
| Adam | epoch: 003 | loss: 0.45047 - acc: 0.8057 -- iter: 09920/22500
Training Step: 860  | total loss: [1m[32m0.43706[0m[0m | time: 66.374s
| Adam | epoch: 003 | loss: 0.43706 - acc: 0.8095 -- iter: 09984/22500
Training Step: 861  | total loss: [1m[32m0.43177[0m[0m | time: 66.812s
| Adam | epoch: 003 | loss: 0.43177 - acc: 0.8129 -- iter: 10048/22500
Training Step: 862  | total loss: [1m[32m0.44049[0m[0m | time: 67.254s
| Adam | epoch: 003 | loss: 0.44049 - acc: 0.8098 -- iter: 10112/22500
Training Step: 863  | total loss: [1m[32m0.43403[0m[0m | time: 67.671s
| Adam | epoch: 003 | loss: 0.43403 - acc: 0.8116 -- iter: 10176/22500
Training Step: 864  | total loss: [1m[32m0.43027[0m[0m | time: 68.093s
| Adam | epoch: 003 | loss: 0.43027 - acc: 0.8132 -- iter: 10240/22500
Training Step: 865  | total loss: [1m[32m0.41869[0m[0m | time: 68.506s
| Adam | epoch: 003 | loss: 0.41869 - acc: 0.8163 -- iter: 10304/22500
Training Step: 866  | total loss: [1m[32m0.43076[0m[0m | time: 69.026s
| Adam | epoch: 003 | loss: 0.43076 - acc: 0.8128 -- iter: 10368/22500
Training Step: 867  | total loss: [1m[32m0.43465[0m[0m | time: 69.494s
| Adam | epoch: 003 | loss: 0.43465 - acc: 0.8096 -- iter: 10432/22500
Training Step: 868  | total loss: [1m[32m0.43696[0m[0m | time: 69.835s
| Adam | epoch: 003 | loss: 0.43696 - acc: 0.8068 -- iter: 10496/22500
Training Step: 869  | total loss: [1m[32m0.43226[0m[0m | time: 70.196s
| Adam | epoch: 003 | loss: 0.43226 - acc: 0.8105 -- iter: 10560/22500
Training Step: 870  | total loss: [1m[32m0.43459[0m[0m | time: 70.547s
| Adam | epoch: 003 | loss: 0.43459 - acc: 0.8091 -- iter: 10624/22500
Training Step: 871  | total loss: [1m[32m0.43963[0m[0m | time: 70.887s
| Adam | epoch: 003 | loss: 0.43963 - acc: 0.8095 -- iter: 10688/22500
Training Step: 872  | total loss: [1m[32m0.43368[0m[0m | time: 71.318s
| Adam | epoch: 003 | loss: 0.43368 - acc: 0.8145 -- iter: 10752/22500
Training Step: 873  | total loss: [1m[32m0.44196[0m[0m | time: 71.739s
| Adam | epoch: 003 | loss: 0.44196 - acc: 0.8080 -- iter: 10816/22500
Training Step: 874  | total loss: [1m[32m0.44343[0m[0m | time: 72.192s
| Adam | epoch: 003 | loss: 0.44343 - acc: 0.8100 -- iter: 10880/22500
Training Step: 875  | total loss: [1m[32m0.43278[0m[0m | time: 72.556s
| Adam | epoch: 003 | loss: 0.43278 - acc: 0.8181 -- iter: 10944/22500
Training Step: 876  | total loss: [1m[32m0.43291[0m[0m | time: 72.938s
| Adam | epoch: 003 | loss: 0.43291 - acc: 0.8175 -- iter: 11008/22500
Training Step: 877  | total loss: [1m[32m0.42795[0m[0m | time: 73.348s
| Adam | epoch: 003 | loss: 0.42795 - acc: 0.8186 -- iter: 11072/22500
Training Step: 878  | total loss: [1m[32m0.42605[0m[0m | time: 73.727s
| Adam | epoch: 003 | loss: 0.42605 - acc: 0.8180 -- iter: 11136/22500
Training Step: 879  | total loss: [1m[32m0.41524[0m[0m | time: 74.054s
| Adam | epoch: 003 | loss: 0.41524 - acc: 0.8221 -- iter: 11200/22500
Training Step: 880  | total loss: [1m[32m0.40502[0m[0m | time: 74.386s
| Adam | epoch: 003 | loss: 0.40502 - acc: 0.8274 -- iter: 11264/22500
Training Step: 881  | total loss: [1m[32m0.41492[0m[0m | time: 74.716s
| Adam | epoch: 003 | loss: 0.41492 - acc: 0.8181 -- iter: 11328/22500
Training Step: 882  | total loss: [1m[32m0.40247[0m[0m | time: 75.040s
| Adam | epoch: 003 | loss: 0.40247 - acc: 0.8238 -- iter: 11392/22500
Training Step: 883  | total loss: [1m[32m0.40578[0m[0m | time: 75.423s
| Adam | epoch: 003 | loss: 0.40578 - acc: 0.8242 -- iter: 11456/22500
Training Step: 884  | total loss: [1m[32m0.39531[0m[0m | time: 75.869s
| Adam | epoch: 003 | loss: 0.39531 - acc: 0.8293 -- iter: 11520/22500
Training Step: 885  | total loss: [1m[32m0.40355[0m[0m | time: 76.302s
| Adam | epoch: 003 | loss: 0.40355 - acc: 0.8245 -- iter: 11584/22500
Training Step: 886  | total loss: [1m[32m0.40752[0m[0m | time: 76.720s
| Adam | epoch: 003 | loss: 0.40752 - acc: 0.8280 -- iter: 11648/22500
Training Step: 887  | total loss: [1m[32m0.42265[0m[0m | time: 77.159s
| Adam | epoch: 003 | loss: 0.42265 - acc: 0.8202 -- iter: 11712/22500
Training Step: 888  | total loss: [1m[32m0.44747[0m[0m | time: 77.577s
| Adam | epoch: 003 | loss: 0.44747 - acc: 0.8054 -- iter: 11776/22500
Training Step: 889  | total loss: [1m[32m0.45417[0m[0m | time: 77.991s
| Adam | epoch: 003 | loss: 0.45417 - acc: 0.8014 -- iter: 11840/22500
Training Step: 890  | total loss: [1m[32m0.45107[0m[0m | time: 78.409s
| Adam | epoch: 003 | loss: 0.45107 - acc: 0.8056 -- iter: 11904/22500
Training Step: 891  | total loss: [1m[32m0.46255[0m[0m | time: 78.840s
| Adam | epoch: 003 | loss: 0.46255 - acc: 0.8047 -- iter: 11968/22500
Training Step: 892  | total loss: [1m[32m0.46478[0m[0m | time: 79.315s
| Adam | epoch: 003 | loss: 0.46478 - acc: 0.8024 -- iter: 12032/22500
Training Step: 893  | total loss: [1m[32m0.45915[0m[0m | time: 79.780s
| Adam | epoch: 003 | loss: 0.45915 - acc: 0.8003 -- iter: 12096/22500
Training Step: 894  | total loss: [1m[32m0.46034[0m[0m | time: 80.205s
| Adam | epoch: 003 | loss: 0.46034 - acc: 0.7984 -- iter: 12160/22500
Training Step: 895  | total loss: [1m[32m0.45413[0m[0m | time: 80.621s
| Adam | epoch: 003 | loss: 0.45413 - acc: 0.7998 -- iter: 12224/22500
Training Step: 896  | total loss: [1m[32m0.43904[0m[0m | time: 81.034s
| Adam | epoch: 003 | loss: 0.43904 - acc: 0.8089 -- iter: 12288/22500
Training Step: 897  | total loss: [1m[32m0.46506[0m[0m | time: 81.458s
| Adam | epoch: 003 | loss: 0.46506 - acc: 0.8030 -- iter: 12352/22500
Training Step: 898  | total loss: [1m[32m0.46579[0m[0m | time: 81.876s
| Adam | epoch: 003 | loss: 0.46579 - acc: 0.8055 -- iter: 12416/22500
Training Step: 899  | total loss: [1m[32m0.46668[0m[0m | time: 82.293s
| Adam | epoch: 003 | loss: 0.46668 - acc: 0.8015 -- iter: 12480/22500
Training Step: 900  | total loss: [1m[32m0.46298[0m[0m | time: 82.716s
| Adam | epoch: 003 | loss: 0.46298 - acc: 0.7995 -- iter: 12544/22500
Training Step: 901  | total loss: [1m[32m0.46739[0m[0m | time: 83.136s
| Adam | epoch: 003 | loss: 0.46739 - acc: 0.7977 -- iter: 12608/22500
Training Step: 902  | total loss: [1m[32m0.48154[0m[0m | time: 83.551s
| Adam | epoch: 003 | loss: 0.48154 - acc: 0.7835 -- iter: 12672/22500
Training Step: 903  | total loss: [1m[32m0.47289[0m[0m | time: 83.973s
| Adam | epoch: 003 | loss: 0.47289 - acc: 0.7880 -- iter: 12736/22500
Training Step: 904  | total loss: [1m[32m0.46274[0m[0m | time: 84.384s
| Adam | epoch: 003 | loss: 0.46274 - acc: 0.7967 -- iter: 12800/22500
Training Step: 905  | total loss: [1m[32m0.45339[0m[0m | time: 84.799s
| Adam | epoch: 003 | loss: 0.45339 - acc: 0.7998 -- iter: 12864/22500
Training Step: 906  | total loss: [1m[32m0.45752[0m[0m | time: 85.224s
| Adam | epoch: 003 | loss: 0.45752 - acc: 0.7948 -- iter: 12928/22500
Training Step: 907  | total loss: [1m[32m0.44247[0m[0m | time: 85.653s
| Adam | epoch: 003 | loss: 0.44247 - acc: 0.8029 -- iter: 12992/22500
Training Step: 908  | total loss: [1m[32m0.43430[0m[0m | time: 86.082s
| Adam | epoch: 003 | loss: 0.43430 - acc: 0.8085 -- iter: 13056/22500
Training Step: 909  | total loss: [1m[32m0.43631[0m[0m | time: 86.460s
| Adam | epoch: 003 | loss: 0.43631 - acc: 0.8073 -- iter: 13120/22500
Training Step: 910  | total loss: [1m[32m0.43444[0m[0m | time: 86.792s
| Adam | epoch: 003 | loss: 0.43444 - acc: 0.8094 -- iter: 13184/22500
Training Step: 911  | total loss: [1m[32m0.43321[0m[0m | time: 87.289s
| Adam | epoch: 003 | loss: 0.43321 - acc: 0.8082 -- iter: 13248/22500
Training Step: 912  | total loss: [1m[32m0.42568[0m[0m | time: 87.668s
| Adam | epoch: 003 | loss: 0.42568 - acc: 0.8149 -- iter: 13312/22500
Training Step: 913  | total loss: [1m[32m0.42664[0m[0m | time: 88.006s
| Adam | epoch: 003 | loss: 0.42664 - acc: 0.8131 -- iter: 13376/22500
Training Step: 914  | total loss: [1m[32m0.43560[0m[0m | time: 88.387s
| Adam | epoch: 003 | loss: 0.43560 - acc: 0.8052 -- iter: 13440/22500
Training Step: 915  | total loss: [1m[32m0.43196[0m[0m | time: 88.752s
| Adam | epoch: 003 | loss: 0.43196 - acc: 0.8090 -- iter: 13504/22500
Training Step: 916  | total loss: [1m[32m0.43022[0m[0m | time: 89.085s
| Adam | epoch: 003 | loss: 0.43022 - acc: 0.8078 -- iter: 13568/22500
Training Step: 917  | total loss: [1m[32m0.43170[0m[0m | time: 89.496s
| Adam | epoch: 003 | loss: 0.43170 - acc: 0.8083 -- iter: 13632/22500
Training Step: 918  | total loss: [1m[32m0.42942[0m[0m | time: 89.938s
| Adam | epoch: 003 | loss: 0.42942 - acc: 0.8134 -- iter: 13696/22500
Training Step: 919  | total loss: [1m[32m0.42909[0m[0m | time: 90.374s
| Adam | epoch: 003 | loss: 0.42909 - acc: 0.8102 -- iter: 13760/22500
Training Step: 920  | total loss: [1m[32m0.42016[0m[0m | time: 90.799s
| Adam | epoch: 003 | loss: 0.42016 - acc: 0.8135 -- iter: 13824/22500
Training Step: 921  | total loss: [1m[32m0.40695[0m[0m | time: 91.209s
| Adam | epoch: 003 | loss: 0.40695 - acc: 0.8213 -- iter: 13888/22500
Training Step: 922  | total loss: [1m[32m0.42715[0m[0m | time: 91.617s
| Adam | epoch: 003 | loss: 0.42715 - acc: 0.8110 -- iter: 13952/22500
Training Step: 923  | total loss: [1m[32m0.42995[0m[0m | time: 92.046s
| Adam | epoch: 003 | loss: 0.42995 - acc: 0.8080 -- iter: 14016/22500
Training Step: 924  | total loss: [1m[32m0.42163[0m[0m | time: 92.459s
| Adam | epoch: 003 | loss: 0.42163 - acc: 0.8147 -- iter: 14080/22500
Training Step: 925  | total loss: [1m[32m0.41464[0m[0m | time: 92.869s
| Adam | epoch: 003 | loss: 0.41464 - acc: 0.8192 -- iter: 14144/22500
Training Step: 926  | total loss: [1m[32m0.41559[0m[0m | time: 93.287s
| Adam | epoch: 003 | loss: 0.41559 - acc: 0.8201 -- iter: 14208/22500
Training Step: 927  | total loss: [1m[32m0.41631[0m[0m | time: 93.706s
| Adam | epoch: 003 | loss: 0.41631 - acc: 0.8209 -- iter: 14272/22500
Training Step: 928  | total loss: [1m[32m0.42000[0m[0m | time: 94.112s
| Adam | epoch: 003 | loss: 0.42000 - acc: 0.8200 -- iter: 14336/22500
Training Step: 929  | total loss: [1m[32m0.42749[0m[0m | time: 94.528s
| Adam | epoch: 003 | loss: 0.42749 - acc: 0.8146 -- iter: 14400/22500
Training Step: 930  | total loss: [1m[32m0.42402[0m[0m | time: 94.961s
| Adam | epoch: 003 | loss: 0.42402 - acc: 0.8160 -- iter: 14464/22500
Training Step: 931  | total loss: [1m[32m0.42667[0m[0m | time: 95.404s
| Adam | epoch: 003 | loss: 0.42667 - acc: 0.8094 -- iter: 14528/22500
Training Step: 932  | total loss: [1m[32m0.43031[0m[0m | time: 95.819s
| Adam | epoch: 003 | loss: 0.43031 - acc: 0.8050 -- iter: 14592/22500
Training Step: 933  | total loss: [1m[32m0.44305[0m[0m | time: 96.241s
| Adam | epoch: 003 | loss: 0.44305 - acc: 0.7932 -- iter: 14656/22500
Training Step: 934  | total loss: [1m[32m0.44462[0m[0m | time: 96.663s
| Adam | epoch: 003 | loss: 0.44462 - acc: 0.7920 -- iter: 14720/22500
Training Step: 935  | total loss: [1m[32m0.45191[0m[0m | time: 97.089s
| Adam | epoch: 003 | loss: 0.45191 - acc: 0.7910 -- iter: 14784/22500
Training Step: 936  | total loss: [1m[32m0.43420[0m[0m | time: 97.517s
| Adam | epoch: 003 | loss: 0.43420 - acc: 0.7994 -- iter: 14848/22500
Training Step: 937  | total loss: [1m[32m0.43616[0m[0m | time: 97.930s
| Adam | epoch: 003 | loss: 0.43616 - acc: 0.8022 -- iter: 14912/22500
Training Step: 938  | total loss: [1m[32m0.43234[0m[0m | time: 98.349s
| Adam | epoch: 003 | loss: 0.43234 - acc: 0.8017 -- iter: 14976/22500
Training Step: 939  | total loss: [1m[32m0.43392[0m[0m | time: 98.772s
| Adam | epoch: 003 | loss: 0.43392 - acc: 0.8012 -- iter: 15040/22500
Training Step: 940  | total loss: [1m[32m0.41577[0m[0m | time: 99.184s
| Adam | epoch: 003 | loss: 0.41577 - acc: 0.8148 -- iter: 15104/22500
Training Step: 941  | total loss: [1m[32m0.41621[0m[0m | time: 99.631s
| Adam | epoch: 003 | loss: 0.41621 - acc: 0.8177 -- iter: 15168/22500
Training Step: 942  | total loss: [1m[32m0.40501[0m[0m | time: 100.071s
| Adam | epoch: 003 | loss: 0.40501 - acc: 0.8266 -- iter: 15232/22500
Training Step: 943  | total loss: [1m[32m0.40806[0m[0m | time: 100.483s
| Adam | epoch: 003 | loss: 0.40806 - acc: 0.8205 -- iter: 15296/22500
Training Step: 944  | total loss: [1m[32m0.40526[0m[0m | time: 100.892s
| Adam | epoch: 003 | loss: 0.40526 - acc: 0.8181 -- iter: 15360/22500
Training Step: 945  | total loss: [1m[32m0.39534[0m[0m | time: 101.312s
| Adam | epoch: 003 | loss: 0.39534 - acc: 0.8238 -- iter: 15424/22500
Training Step: 946  | total loss: [1m[32m0.39289[0m[0m | time: 101.722s
| Adam | epoch: 003 | loss: 0.39289 - acc: 0.8211 -- iter: 15488/22500
Training Step: 947  | total loss: [1m[32m0.39042[0m[0m | time: 102.142s
| Adam | epoch: 003 | loss: 0.39042 - acc: 0.8234 -- iter: 15552/22500
Training Step: 948  | total loss: [1m[32m0.39475[0m[0m | time: 102.562s
| Adam | epoch: 003 | loss: 0.39475 - acc: 0.8254 -- iter: 15616/22500
Training Step: 949  | total loss: [1m[32m0.39537[0m[0m | time: 102.974s
| Adam | epoch: 003 | loss: 0.39537 - acc: 0.8210 -- iter: 15680/22500
Training Step: 950  | total loss: [1m[32m0.40001[0m[0m | time: 103.389s
| Adam | epoch: 003 | loss: 0.40001 - acc: 0.8202 -- iter: 15744/22500
Training Step: 951  | total loss: [1m[32m0.39770[0m[0m | time: 103.799s
| Adam | epoch: 003 | loss: 0.39770 - acc: 0.8241 -- iter: 15808/22500
Training Step: 952  | total loss: [1m[32m0.39541[0m[0m | time: 104.233s
| Adam | epoch: 003 | loss: 0.39541 - acc: 0.8292 -- iter: 15872/22500
Training Step: 953  | total loss: [1m[32m0.40174[0m[0m | time: 104.667s
| Adam | epoch: 003 | loss: 0.40174 - acc: 0.8275 -- iter: 15936/22500
Training Step: 954  | total loss: [1m[32m0.40315[0m[0m | time: 105.038s
| Adam | epoch: 003 | loss: 0.40315 - acc: 0.8260 -- iter: 16000/22500
Training Step: 955  | total loss: [1m[32m0.40712[0m[0m | time: 105.373s
| Adam | epoch: 003 | loss: 0.40712 - acc: 0.8262 -- iter: 16064/22500
Training Step: 956  | total loss: [1m[32m0.41804[0m[0m | time: 105.707s
| Adam | epoch: 003 | loss: 0.41804 - acc: 0.8217 -- iter: 16128/22500
Training Step: 957  | total loss: [1m[32m0.42049[0m[0m | time: 106.037s
| Adam | epoch: 003 | loss: 0.42049 - acc: 0.8192 -- iter: 16192/22500
Training Step: 958  | total loss: [1m[32m0.42421[0m[0m | time: 106.442s
| Adam | epoch: 003 | loss: 0.42421 - acc: 0.8232 -- iter: 16256/22500
Training Step: 959  | total loss: [1m[32m0.40728[0m[0m | time: 106.856s
| Adam | epoch: 003 | loss: 0.40728 - acc: 0.8347 -- iter: 16320/22500
Training Step: 960  | total loss: [1m[32m0.40674[0m[0m | time: 107.270s
| Adam | epoch: 003 | loss: 0.40674 - acc: 0.8356 -- iter: 16384/22500
Training Step: 961  | total loss: [1m[32m0.40135[0m[0m | time: 107.687s
| Adam | epoch: 003 | loss: 0.40135 - acc: 0.8364 -- iter: 16448/22500
Training Step: 962  | total loss: [1m[32m0.41412[0m[0m | time: 108.096s
| Adam | epoch: 003 | loss: 0.41412 - acc: 0.8309 -- iter: 16512/22500
Training Step: 963  | total loss: [1m[32m0.42163[0m[0m | time: 108.513s
| Adam | epoch: 003 | loss: 0.42163 - acc: 0.8322 -- iter: 16576/22500
Training Step: 964  | total loss: [1m[32m0.42093[0m[0m | time: 108.968s
| Adam | epoch: 003 | loss: 0.42093 - acc: 0.8286 -- iter: 16640/22500
Training Step: 965  | total loss: [1m[32m0.42147[0m[0m | time: 109.409s
| Adam | epoch: 003 | loss: 0.42147 - acc: 0.8255 -- iter: 16704/22500
Training Step: 966  | total loss: [1m[32m0.41237[0m[0m | time: 109.830s
| Adam | epoch: 003 | loss: 0.41237 - acc: 0.8289 -- iter: 16768/22500
Training Step: 967  | total loss: [1m[32m0.41511[0m[0m | time: 110.251s
| Adam | epoch: 003 | loss: 0.41511 - acc: 0.8288 -- iter: 16832/22500
Training Step: 968  | total loss: [1m[32m0.40743[0m[0m | time: 110.670s
| Adam | epoch: 003 | loss: 0.40743 - acc: 0.8334 -- iter: 16896/22500
Training Step: 969  | total loss: [1m[32m0.42105[0m[0m | time: 111.085s
| Adam | epoch: 003 | loss: 0.42105 - acc: 0.8251 -- iter: 16960/22500
Training Step: 970  | total loss: [1m[32m0.41719[0m[0m | time: 111.504s
| Adam | epoch: 003 | loss: 0.41719 - acc: 0.8301 -- iter: 17024/22500
Training Step: 971  | total loss: [1m[32m0.41161[0m[0m | time: 111.909s
| Adam | epoch: 003 | loss: 0.41161 - acc: 0.8314 -- iter: 17088/22500
Training Step: 972  | total loss: [1m[32m0.40440[0m[0m | time: 112.315s
| Adam | epoch: 003 | loss: 0.40440 - acc: 0.8358 -- iter: 17152/22500
Training Step: 973  | total loss: [1m[32m0.40078[0m[0m | time: 112.704s
| Adam | epoch: 003 | loss: 0.40078 - acc: 0.8413 -- iter: 17216/22500
Training Step: 974  | total loss: [1m[32m0.39843[0m[0m | time: 113.032s
| Adam | epoch: 003 | loss: 0.39843 - acc: 0.8415 -- iter: 17280/22500
Training Step: 975  | total loss: [1m[32m0.40598[0m[0m | time: 113.386s
| Adam | epoch: 003 | loss: 0.40598 - acc: 0.8371 -- iter: 17344/22500
Training Step: 976  | total loss: [1m[32m0.39529[0m[0m | time: 113.744s
| Adam | epoch: 003 | loss: 0.39529 - acc: 0.8440 -- iter: 17408/22500
Training Step: 977  | total loss: [1m[32m0.37850[0m[0m | time: 114.138s
| Adam | epoch: 003 | loss: 0.37850 - acc: 0.8549 -- iter: 17472/22500
Training Step: 978  | total loss: [1m[32m0.38027[0m[0m | time: 114.553s
| Adam | epoch: 003 | loss: 0.38027 - acc: 0.8538 -- iter: 17536/22500
Training Step: 979  | total loss: [1m[32m0.39270[0m[0m | time: 114.970s
| Adam | epoch: 003 | loss: 0.39270 - acc: 0.8481 -- iter: 17600/22500
Training Step: 980  | total loss: [1m[32m0.39429[0m[0m | time: 115.389s
| Adam | epoch: 003 | loss: 0.39429 - acc: 0.8445 -- iter: 17664/22500
Training Step: 981  | total loss: [1m[32m0.38122[0m[0m | time: 115.808s
| Adam | epoch: 003 | loss: 0.38122 - acc: 0.8491 -- iter: 17728/22500
Training Step: 982  | total loss: [1m[32m0.37956[0m[0m | time: 116.226s
| Adam | epoch: 003 | loss: 0.37956 - acc: 0.8486 -- iter: 17792/22500
Training Step: 983  | total loss: [1m[32m0.37455[0m[0m | time: 116.647s
| Adam | epoch: 003 | loss: 0.37455 - acc: 0.8512 -- iter: 17856/22500
Training Step: 984  | total loss: [1m[32m0.39303[0m[0m | time: 117.069s
| Adam | epoch: 003 | loss: 0.39303 - acc: 0.8427 -- iter: 17920/22500
Training Step: 985  | total loss: [1m[32m0.38802[0m[0m | time: 117.486s
| Adam | epoch: 003 | loss: 0.38802 - acc: 0.8412 -- iter: 17984/22500
Training Step: 986  | total loss: [1m[32m0.38792[0m[0m | time: 117.902s
| Adam | epoch: 003 | loss: 0.38792 - acc: 0.8415 -- iter: 18048/22500
Training Step: 987  | total loss: [1m[32m0.38247[0m[0m | time: 118.319s
| Adam | epoch: 003 | loss: 0.38247 - acc: 0.8433 -- iter: 18112/22500
Training Step: 988  | total loss: [1m[32m0.38919[0m[0m | time: 118.745s
| Adam | epoch: 003 | loss: 0.38919 - acc: 0.8449 -- iter: 18176/22500
Training Step: 989  | total loss: [1m[32m0.39042[0m[0m | time: 119.165s
| Adam | epoch: 003 | loss: 0.39042 - acc: 0.8463 -- iter: 18240/22500
Training Step: 990  | total loss: [1m[32m0.38042[0m[0m | time: 119.593s
| Adam | epoch: 003 | loss: 0.38042 - acc: 0.8523 -- iter: 18304/22500
Training Step: 991  | total loss: [1m[32m0.38163[0m[0m | time: 120.006s
| Adam | epoch: 003 | loss: 0.38163 - acc: 0.8515 -- iter: 18368/22500
Training Step: 992  | total loss: [1m[32m0.38243[0m[0m | time: 120.428s
| Adam | epoch: 003 | loss: 0.38243 - acc: 0.8491 -- iter: 18432/22500
Training Step: 993  | total loss: [1m[32m0.38940[0m[0m | time: 120.853s
| Adam | epoch: 003 | loss: 0.38940 - acc: 0.8455 -- iter: 18496/22500
Training Step: 994  | total loss: [1m[32m0.38105[0m[0m | time: 121.274s
| Adam | epoch: 003 | loss: 0.38105 - acc: 0.8484 -- iter: 18560/22500
Training Step: 995  | total loss: [1m[32m0.37825[0m[0m | time: 121.753s
| Adam | epoch: 003 | loss: 0.37825 - acc: 0.8480 -- iter: 18624/22500
Training Step: 996  | total loss: [1m[32m0.38631[0m[0m | time: 122.165s
| Adam | epoch: 003 | loss: 0.38631 - acc: 0.8460 -- iter: 18688/22500
Training Step: 997  | total loss: [1m[32m0.37100[0m[0m | time: 122.573s
| Adam | epoch: 003 | loss: 0.37100 - acc: 0.8536 -- iter: 18752/22500
Training Step: 998  | total loss: [1m[32m0.36462[0m[0m | time: 123.007s
| Adam | epoch: 003 | loss: 0.36462 - acc: 0.8557 -- iter: 18816/22500
Training Step: 999  | total loss: [1m[32m0.36443[0m[0m | time: 123.437s
| Adam | epoch: 003 | loss: 0.36443 - acc: 0.8529 -- iter: 18880/22500
Training Step: 1000  | total loss: [1m[32m0.36769[0m[0m | time: 127.820s
| Adam | epoch: 003 | loss: 0.36769 - acc: 0.8520 | val_loss: 0.45083 - val_acc: 0.7948 -- iter: 18944/22500
--
Training Step: 1001  | total loss: [1m[32m0.36755[0m[0m | time: 128.247s
| Adam | epoch: 003 | loss: 0.36755 - acc: 0.8543 -- iter: 19008/22500
Training Step: 1002  | total loss: [1m[32m0.36445[0m[0m | time: 128.675s
| Adam | epoch: 003 | loss: 0.36445 - acc: 0.8533 -- iter: 19072/22500
Training Step: 1003  | total loss: [1m[32m0.37173[0m[0m | time: 129.037s
| Adam | epoch: 003 | loss: 0.37173 - acc: 0.8508 -- iter: 19136/22500
Training Step: 1004  | total loss: [1m[32m0.36008[0m[0m | time: 129.436s
| Adam | epoch: 003 | loss: 0.36008 - acc: 0.8579 -- iter: 19200/22500
Training Step: 1005  | total loss: [1m[32m0.36238[0m[0m | time: 129.900s
| Adam | epoch: 003 | loss: 0.36238 - acc: 0.8565 -- iter: 19264/22500
Training Step: 1006  | total loss: [1m[32m0.37544[0m[0m | time: 130.440s
| Adam | epoch: 003 | loss: 0.37544 - acc: 0.8505 -- iter: 19328/22500
Training Step: 1007  | total loss: [1m[32m0.37733[0m[0m | time: 130.998s
| Adam | epoch: 003 | loss: 0.37733 - acc: 0.8545 -- iter: 19392/22500
Training Step: 1008  | total loss: [1m[32m0.37935[0m[0m | time: 131.384s
| Adam | epoch: 003 | loss: 0.37935 - acc: 0.8519 -- iter: 19456/22500
Training Step: 1009  | total loss: [1m[32m0.38288[0m[0m | time: 131.742s
| Adam | epoch: 003 | loss: 0.38288 - acc: 0.8432 -- iter: 19520/22500
Training Step: 1010  | total loss: [1m[32m0.39150[0m[0m | time: 132.190s
| Adam | epoch: 003 | loss: 0.39150 - acc: 0.8324 -- iter: 19584/22500
Training Step: 1011  | total loss: [1m[32m0.40352[0m[0m | time: 132.649s
| Adam | epoch: 003 | loss: 0.40352 - acc: 0.8163 -- iter: 19648/22500
Training Step: 1012  | total loss: [1m[32m0.39757[0m[0m | time: 133.040s
| Adam | epoch: 003 | loss: 0.39757 - acc: 0.8284 -- iter: 19712/22500
Training Step: 1013  | total loss: [1m[32m0.39904[0m[0m | time: 133.452s
| Adam | epoch: 003 | loss: 0.39904 - acc: 0.8300 -- iter: 19776/22500
Training Step: 1014  | total loss: [1m[32m0.40172[0m[0m | time: 133.795s
| Adam | epoch: 003 | loss: 0.40172 - acc: 0.8251 -- iter: 19840/22500
Training Step: 1015  | total loss: [1m[32m0.39507[0m[0m | time: 134.225s
| Adam | epoch: 003 | loss: 0.39507 - acc: 0.8254 -- iter: 19904/22500
Training Step: 1016  | total loss: [1m[32m0.39621[0m[0m | time: 134.622s
| Adam | epoch: 003 | loss: 0.39621 - acc: 0.8241 -- iter: 19968/22500
Training Step: 1017  | total loss: [1m[32m0.40960[0m[0m | time: 134.982s
| Adam | epoch: 003 | loss: 0.40960 - acc: 0.8183 -- iter: 20032/22500
Training Step: 1018  | total loss: [1m[32m0.43214[0m[0m | time: 135.344s
| Adam | epoch: 003 | loss: 0.43214 - acc: 0.8083 -- iter: 20096/22500
Training Step: 1019  | total loss: [1m[32m0.43674[0m[0m | time: 135.707s
| Adam | epoch: 003 | loss: 0.43674 - acc: 0.8103 -- iter: 20160/22500
Training Step: 1020  | total loss: [1m[32m0.42059[0m[0m | time: 136.065s
| Adam | epoch: 003 | loss: 0.42059 - acc: 0.8183 -- iter: 20224/22500
Training Step: 1021  | total loss: [1m[32m0.42432[0m[0m | time: 136.458s
| Adam | epoch: 003 | loss: 0.42432 - acc: 0.8177 -- iter: 20288/22500
Training Step: 1022  | total loss: [1m[32m0.42004[0m[0m | time: 136.869s
| Adam | epoch: 003 | loss: 0.42004 - acc: 0.8203 -- iter: 20352/22500
Training Step: 1023  | total loss: [1m[32m0.40274[0m[0m | time: 137.263s
| Adam | epoch: 003 | loss: 0.40274 - acc: 0.8274 -- iter: 20416/22500
Training Step: 1024  | total loss: [1m[32m0.40373[0m[0m | time: 137.636s
| Adam | epoch: 003 | loss: 0.40373 - acc: 0.8243 -- iter: 20480/22500
Training Step: 1025  | total loss: [1m[32m0.40810[0m[0m | time: 138.003s
| Adam | epoch: 003 | loss: 0.40810 - acc: 0.8247 -- iter: 20544/22500
Training Step: 1026  | total loss: [1m[32m0.39904[0m[0m | time: 138.505s
| Adam | epoch: 003 | loss: 0.39904 - acc: 0.8313 -- iter: 20608/22500
Training Step: 1027  | total loss: [1m[32m0.45353[0m[0m | time: 138.842s
| Adam | epoch: 003 | loss: 0.45353 - acc: 0.8029 -- iter: 20672/22500
Training Step: 1028  | total loss: [1m[32m0.43739[0m[0m | time: 139.134s
| Adam | epoch: 003 | loss: 0.43739 - acc: 0.8148 -- iter: 20736/22500
Training Step: 1029  | total loss: [1m[32m0.43268[0m[0m | time: 139.478s
| Adam | epoch: 003 | loss: 0.43268 - acc: 0.8177 -- iter: 20800/22500
Training Step: 1030  | total loss: [1m[32m0.42290[0m[0m | time: 139.780s
| Adam | epoch: 003 | loss: 0.42290 - acc: 0.8249 -- iter: 20864/22500
Training Step: 1031  | total loss: [1m[32m0.42584[0m[0m | time: 140.072s
| Adam | epoch: 003 | loss: 0.42584 - acc: 0.8237 -- iter: 20928/22500
Training Step: 1032  | total loss: [1m[32m0.41726[0m[0m | time: 140.430s
| Adam | epoch: 003 | loss: 0.41726 - acc: 0.8335 -- iter: 20992/22500
Training Step: 1033  | total loss: [1m[32m0.41772[0m[0m | time: 140.811s
| Adam | epoch: 003 | loss: 0.41772 - acc: 0.8314 -- iter: 21056/22500
Training Step: 1034  | total loss: [1m[32m0.40964[0m[0m | time: 141.187s
| Adam | epoch: 003 | loss: 0.40964 - acc: 0.8342 -- iter: 21120/22500
Training Step: 1035  | total loss: [1m[32m0.39989[0m[0m | time: 141.548s
| Adam | epoch: 003 | loss: 0.39989 - acc: 0.8399 -- iter: 21184/22500
Training Step: 1036  | total loss: [1m[32m0.39134[0m[0m | time: 141.909s
| Adam | epoch: 003 | loss: 0.39134 - acc: 0.8418 -- iter: 21248/22500
Training Step: 1037  | total loss: [1m[32m0.38679[0m[0m | time: 142.272s
| Adam | epoch: 003 | loss: 0.38679 - acc: 0.8436 -- iter: 21312/22500
Training Step: 1038  | total loss: [1m[32m0.36948[0m[0m | time: 142.631s
| Adam | epoch: 003 | loss: 0.36948 - acc: 0.8498 -- iter: 21376/22500
Training Step: 1039  | total loss: [1m[32m0.36167[0m[0m | time: 142.996s
| Adam | epoch: 003 | loss: 0.36167 - acc: 0.8555 -- iter: 21440/22500
Training Step: 1040  | total loss: [1m[32m0.36421[0m[0m | time: 143.349s
| Adam | epoch: 003 | loss: 0.36421 - acc: 0.8512 -- iter: 21504/22500
Training Step: 1041  | total loss: [1m[32m0.35079[0m[0m | time: 143.706s
| Adam | epoch: 003 | loss: 0.35079 - acc: 0.8614 -- iter: 21568/22500
Training Step: 1042  | total loss: [1m[32m0.35266[0m[0m | time: 144.069s
| Adam | epoch: 003 | loss: 0.35266 - acc: 0.8596 -- iter: 21632/22500
Training Step: 1043  | total loss: [1m[32m0.34379[0m[0m | time: 144.425s
| Adam | epoch: 003 | loss: 0.34379 - acc: 0.8643 -- iter: 21696/22500
Training Step: 1044  | total loss: [1m[32m0.34802[0m[0m | time: 144.793s
| Adam | epoch: 003 | loss: 0.34802 - acc: 0.8653 -- iter: 21760/22500
Training Step: 1045  | total loss: [1m[32m0.33878[0m[0m | time: 145.164s
| Adam | epoch: 003 | loss: 0.33878 - acc: 0.8694 -- iter: 21824/22500
Training Step: 1046  | total loss: [1m[32m0.34275[0m[0m | time: 145.512s
| Adam | epoch: 003 | loss: 0.34275 - acc: 0.8700 -- iter: 21888/22500
Training Step: 1047  | total loss: [1m[32m0.32981[0m[0m | time: 145.858s
| Adam | epoch: 003 | loss: 0.32981 - acc: 0.8767 -- iter: 21952/22500
Training Step: 1048  | total loss: [1m[32m0.32902[0m[0m | time: 146.209s
| Adam | epoch: 003 | loss: 0.32902 - acc: 0.8766 -- iter: 22016/22500
Training Step: 1049  | total loss: [1m[32m0.32131[0m[0m | time: 146.558s
| Adam | epoch: 003 | loss: 0.32131 - acc: 0.8780 -- iter: 22080/22500
Training Step: 1050  | total loss: [1m[32m0.32192[0m[0m | time: 146.926s
| Adam | epoch: 003 | loss: 0.32192 - acc: 0.8746 -- iter: 22144/22500
Training Step: 1051  | total loss: [1m[32m0.33647[0m[0m | time: 147.279s
| Adam | epoch: 003 | loss: 0.33647 - acc: 0.8683 -- iter: 22208/22500
Training Step: 1052  | total loss: [1m[32m0.34192[0m[0m | time: 147.629s
| Adam | epoch: 003 | loss: 0.34192 - acc: 0.8690 -- iter: 22272/22500
Training Step: 1053  | total loss: [1m[32m0.33225[0m[0m | time: 147.990s
| Adam | epoch: 003 | loss: 0.33225 - acc: 0.8743 -- iter: 22336/22500
Training Step: 1054  | total loss: [1m[32m0.32716[0m[0m | time: 148.341s
| Adam | epoch: 003 | loss: 0.32716 - acc: 0.8744 -- iter: 22400/22500
Training Step: 1055  | total loss: [1m[32m0.34320[0m[0m | time: 148.709s
| Adam | epoch: 003 | loss: 0.34320 - acc: 0.8682 -- iter: 22464/22500
Training Step: 1056  | total loss: [1m[32m0.33858[0m[0m | time: 152.319s
| Adam | epoch: 003 | loss: 0.33858 - acc: 0.8736 | val_loss: 0.49604 - val_acc: 0.7900 -- iter: 22500/22500
--
Training Step: 1057  | total loss: [1m[32m0.34173[0m[0m | time: 0.349s
| Adam | epoch: 004 | loss: 0.34173 - acc: 0.8737 -- iter: 00064/22500
Training Step: 1058  | total loss: [1m[32m0.34812[0m[0m | time: 0.691s
| Adam | epoch: 004 | loss: 0.34812 - acc: 0.8723 -- iter: 00128/22500
Training Step: 1059  | total loss: [1m[32m0.34790[0m[0m | time: 1.097s
| Adam | epoch: 004 | loss: 0.34790 - acc: 0.8711 -- iter: 00192/22500
Training Step: 1060  | total loss: [1m[32m0.34684[0m[0m | time: 1.599s
| Adam | epoch: 004 | loss: 0.34684 - acc: 0.8729 -- iter: 00256/22500
Training Step: 1061  | total loss: [1m[32m0.35155[0m[0m | time: 2.110s
| Adam | epoch: 004 | loss: 0.35155 - acc: 0.8653 -- iter: 00320/22500
Training Step: 1062  | total loss: [1m[32m0.34783[0m[0m | time: 2.550s
| Adam | epoch: 004 | loss: 0.34783 - acc: 0.8678 -- iter: 00384/22500
Training Step: 1063  | total loss: [1m[32m0.33392[0m[0m | time: 2.966s
| Adam | epoch: 004 | loss: 0.33392 - acc: 0.8764 -- iter: 00448/22500
Training Step: 1064  | total loss: [1m[32m0.32499[0m[0m | time: 3.274s
| Adam | epoch: 004 | loss: 0.32499 - acc: 0.8794 -- iter: 00512/22500
Training Step: 1065  | total loss: [1m[32m0.32612[0m[0m | time: 3.799s
| Adam | epoch: 004 | loss: 0.32612 - acc: 0.8821 -- iter: 00576/22500
Training Step: 1066  | total loss: [1m[32m0.33182[0m[0m | time: 4.396s
| Adam | epoch: 004 | loss: 0.33182 - acc: 0.8782 -- iter: 00640/22500
Training Step: 1067  | total loss: [1m[32m0.33758[0m[0m | time: 4.890s
| Adam | epoch: 004 | loss: 0.33758 - acc: 0.8763 -- iter: 00704/22500
Training Step: 1068  | total loss: [1m[32m0.33298[0m[0m | time: 5.299s
| Adam | epoch: 004 | loss: 0.33298 - acc: 0.8762 -- iter: 00768/22500
Training Step: 1069  | total loss: [1m[32m0.33004[0m[0m | time: 5.704s
| Adam | epoch: 004 | loss: 0.33004 - acc: 0.8792 -- iter: 00832/22500
Training Step: 1070  | total loss: [1m[32m0.32533[0m[0m | time: 6.176s
| Adam | epoch: 004 | loss: 0.32533 - acc: 0.8850 -- iter: 00896/22500
Training Step: 1071  | total loss: [1m[32m0.32187[0m[0m | time: 6.526s
| Adam | epoch: 004 | loss: 0.32187 - acc: 0.8872 -- iter: 00960/22500
Training Step: 1072  | total loss: [1m[32m0.31399[0m[0m | time: 6.891s
| Adam | epoch: 004 | loss: 0.31399 - acc: 0.8891 -- iter: 01024/22500
Training Step: 1073  | total loss: [1m[32m0.31684[0m[0m | time: 7.247s
| Adam | epoch: 004 | loss: 0.31684 - acc: 0.8845 -- iter: 01088/22500
Training Step: 1074  | total loss: [1m[32m0.32097[0m[0m | time: 7.596s
| Adam | epoch: 004 | loss: 0.32097 - acc: 0.8836 -- iter: 01152/22500
Training Step: 1075  | total loss: [1m[32m0.31063[0m[0m | time: 7.944s
| Adam | epoch: 004 | loss: 0.31063 - acc: 0.8890 -- iter: 01216/22500
Training Step: 1076  | total loss: [1m[32m0.30234[0m[0m | time: 8.292s
| Adam | epoch: 004 | loss: 0.30234 - acc: 0.8923 -- iter: 01280/22500
Training Step: 1077  | total loss: [1m[32m0.31228[0m[0m | time: 8.642s
| Adam | epoch: 004 | loss: 0.31228 - acc: 0.8859 -- iter: 01344/22500
Training Step: 1078  | total loss: [1m[32m0.31209[0m[0m | time: 9.011s
| Adam | epoch: 004 | loss: 0.31209 - acc: 0.8832 -- iter: 01408/22500
Training Step: 1079  | total loss: [1m[32m0.30197[0m[0m | time: 9.418s
| Adam | epoch: 004 | loss: 0.30197 - acc: 0.8871 -- iter: 01472/22500
Training Step: 1080  | total loss: [1m[32m0.30415[0m[0m | time: 9.770s
| Adam | epoch: 004 | loss: 0.30415 - acc: 0.8827 -- iter: 01536/22500
Training Step: 1081  | total loss: [1m[32m0.30712[0m[0m | time: 10.186s
| Adam | epoch: 004 | loss: 0.30712 - acc: 0.8835 -- iter: 01600/22500
Training Step: 1082  | total loss: [1m[32m0.30874[0m[0m | time: 10.507s
| Adam | epoch: 004 | loss: 0.30874 - acc: 0.8827 -- iter: 01664/22500
Training Step: 1083  | total loss: [1m[32m0.29891[0m[0m | time: 10.854s
| Adam | epoch: 004 | loss: 0.29891 - acc: 0.8882 -- iter: 01728/22500
Training Step: 1084  | total loss: [1m[32m0.28362[0m[0m | time: 11.214s
| Adam | epoch: 004 | loss: 0.28362 - acc: 0.8947 -- iter: 01792/22500
Training Step: 1085  | total loss: [1m[32m0.27684[0m[0m | time: 11.598s
| Adam | epoch: 004 | loss: 0.27684 - acc: 0.8943 -- iter: 01856/22500
Training Step: 1086  | total loss: [1m[32m0.27767[0m[0m | time: 11.948s
| Adam | epoch: 004 | loss: 0.27767 - acc: 0.8939 -- iter: 01920/22500
Training Step: 1087  | total loss: [1m[32m0.31116[0m[0m | time: 12.297s
| Adam | epoch: 004 | loss: 0.31116 - acc: 0.8857 -- iter: 01984/22500
Training Step: 1088  | total loss: [1m[32m0.29848[0m[0m | time: 12.648s
| Adam | epoch: 004 | loss: 0.29848 - acc: 0.8862 -- iter: 02048/22500
Training Step: 1089  | total loss: [1m[32m0.30899[0m[0m | time: 12.989s
| Adam | epoch: 004 | loss: 0.30899 - acc: 0.8836 -- iter: 02112/22500
Training Step: 1090  | total loss: [1m[32m0.30261[0m[0m | time: 13.377s
| Adam | epoch: 004 | loss: 0.30261 - acc: 0.8889 -- iter: 02176/22500
Training Step: 1091  | total loss: [1m[32m0.31290[0m[0m | time: 13.795s
| Adam | epoch: 004 | loss: 0.31290 - acc: 0.8891 -- iter: 02240/22500
Training Step: 1092  | total loss: [1m[32m0.32004[0m[0m | time: 14.139s
| Adam | epoch: 004 | loss: 0.32004 - acc: 0.8861 -- iter: 02304/22500
Training Step: 1093  | total loss: [1m[32m0.32677[0m[0m | time: 14.505s
| Adam | epoch: 004 | loss: 0.32677 - acc: 0.8788 -- iter: 02368/22500
Training Step: 1094  | total loss: [1m[32m0.32917[0m[0m | time: 14.857s
| Adam | epoch: 004 | loss: 0.32917 - acc: 0.8800 -- iter: 02432/22500
Training Step: 1095  | total loss: [1m[32m0.33662[0m[0m | time: 15.202s
| Adam | epoch: 004 | loss: 0.33662 - acc: 0.8779 -- iter: 02496/22500
Training Step: 1096  | total loss: [1m[32m0.33516[0m[0m | time: 15.582s
| Adam | epoch: 004 | loss: 0.33516 - acc: 0.8776 -- iter: 02560/22500
Training Step: 1097  | total loss: [1m[32m0.33428[0m[0m | time: 15.940s
| Adam | epoch: 004 | loss: 0.33428 - acc: 0.8789 -- iter: 02624/22500
Training Step: 1098  | total loss: [1m[32m0.33055[0m[0m | time: 16.289s
| Adam | epoch: 004 | loss: 0.33055 - acc: 0.8832 -- iter: 02688/22500
Training Step: 1099  | total loss: [1m[32m0.33778[0m[0m | time: 16.641s
| Adam | epoch: 004 | loss: 0.33778 - acc: 0.8777 -- iter: 02752/22500
Training Step: 1100  | total loss: [1m[32m0.33987[0m[0m | time: 16.989s
| Adam | epoch: 004 | loss: 0.33987 - acc: 0.8759 -- iter: 02816/22500
Training Step: 1101  | total loss: [1m[32m0.33947[0m[0m | time: 17.355s
| Adam | epoch: 004 | loss: 0.33947 - acc: 0.8758 -- iter: 02880/22500
Training Step: 1102  | total loss: [1m[32m0.33646[0m[0m | time: 17.721s
| Adam | epoch: 004 | loss: 0.33646 - acc: 0.8773 -- iter: 02944/22500
Training Step: 1103  | total loss: [1m[32m0.33707[0m[0m | time: 18.071s
| Adam | epoch: 004 | loss: 0.33707 - acc: 0.8755 -- iter: 03008/22500
Training Step: 1104  | total loss: [1m[32m0.32315[0m[0m | time: 18.456s
| Adam | epoch: 004 | loss: 0.32315 - acc: 0.8786 -- iter: 03072/22500
Training Step: 1105  | total loss: [1m[32m0.31146[0m[0m | time: 18.840s
| Adam | epoch: 004 | loss: 0.31146 - acc: 0.8813 -- iter: 03136/22500
Training Step: 1106  | total loss: [1m[32m0.33142[0m[0m | time: 19.337s
| Adam | epoch: 004 | loss: 0.33142 - acc: 0.8744 -- iter: 03200/22500
Training Step: 1107  | total loss: [1m[32m0.32187[0m[0m | time: 19.715s
| Adam | epoch: 004 | loss: 0.32187 - acc: 0.8761 -- iter: 03264/22500
Training Step: 1108  | total loss: [1m[32m0.31643[0m[0m | time: 20.106s
| Adam | epoch: 004 | loss: 0.31643 - acc: 0.8775 -- iter: 03328/22500
Training Step: 1109  | total loss: [1m[32m0.32009[0m[0m | time: 20.489s
| Adam | epoch: 004 | loss: 0.32009 - acc: 0.8773 -- iter: 03392/22500
Training Step: 1110  | total loss: [1m[32m0.32052[0m[0m | time: 20.839s
| Adam | epoch: 004 | loss: 0.32052 - acc: 0.8770 -- iter: 03456/22500
Training Step: 1111  | total loss: [1m[32m0.31559[0m[0m | time: 21.187s
| Adam | epoch: 004 | loss: 0.31559 - acc: 0.8800 -- iter: 03520/22500
Training Step: 1112  | total loss: [1m[32m0.30646[0m[0m | time: 21.554s
| Adam | epoch: 004 | loss: 0.30646 - acc: 0.8857 -- iter: 03584/22500
Training Step: 1113  | total loss: [1m[32m0.30844[0m[0m | time: 21.926s
| Adam | epoch: 004 | loss: 0.30844 - acc: 0.8893 -- iter: 03648/22500
Training Step: 1114  | total loss: [1m[32m0.30257[0m[0m | time: 22.271s
| Adam | epoch: 004 | loss: 0.30257 - acc: 0.8926 -- iter: 03712/22500
Training Step: 1115  | total loss: [1m[32m0.30540[0m[0m | time: 22.627s
| Adam | epoch: 004 | loss: 0.30540 - acc: 0.8924 -- iter: 03776/22500
Training Step: 1116  | total loss: [1m[32m0.30311[0m[0m | time: 22.978s
| Adam | epoch: 004 | loss: 0.30311 - acc: 0.8906 -- iter: 03840/22500
Training Step: 1117  | total loss: [1m[32m0.30166[0m[0m | time: 23.325s
| Adam | epoch: 004 | loss: 0.30166 - acc: 0.8891 -- iter: 03904/22500
Training Step: 1118  | total loss: [1m[32m0.29958[0m[0m | time: 23.676s
| Adam | epoch: 004 | loss: 0.29958 - acc: 0.8892 -- iter: 03968/22500
Training Step: 1119  | total loss: [1m[32m0.30647[0m[0m | time: 24.063s
| Adam | epoch: 004 | loss: 0.30647 - acc: 0.8847 -- iter: 04032/22500
Training Step: 1120  | total loss: [1m[32m0.29545[0m[0m | time: 24.417s
| Adam | epoch: 004 | loss: 0.29545 - acc: 0.8884 -- iter: 04096/22500
Training Step: 1121  | total loss: [1m[32m0.28645[0m[0m | time: 24.766s
| Adam | epoch: 004 | loss: 0.28645 - acc: 0.8933 -- iter: 04160/22500
Training Step: 1122  | total loss: [1m[32m0.28323[0m[0m | time: 25.117s
| Adam | epoch: 004 | loss: 0.28323 - acc: 0.8930 -- iter: 04224/22500
Training Step: 1123  | total loss: [1m[32m0.30909[0m[0m | time: 25.478s
| Adam | epoch: 004 | loss: 0.30909 - acc: 0.8803 -- iter: 04288/22500
Training Step: 1124  | total loss: [1m[32m0.30044[0m[0m | time: 25.846s
| Adam | epoch: 004 | loss: 0.30044 - acc: 0.8829 -- iter: 04352/22500
Training Step: 1125  | total loss: [1m[32m0.30152[0m[0m | time: 26.202s
| Adam | epoch: 004 | loss: 0.30152 - acc: 0.8837 -- iter: 04416/22500
Training Step: 1126  | total loss: [1m[32m0.29866[0m[0m | time: 26.547s
| Adam | epoch: 004 | loss: 0.29866 - acc: 0.8844 -- iter: 04480/22500
Training Step: 1127  | total loss: [1m[32m0.29522[0m[0m | time: 26.894s
| Adam | epoch: 004 | loss: 0.29522 - acc: 0.8850 -- iter: 04544/22500
Training Step: 1128  | total loss: [1m[32m0.29013[0m[0m | time: 27.274s
| Adam | epoch: 004 | loss: 0.29013 - acc: 0.8887 -- iter: 04608/22500
Training Step: 1129  | total loss: [1m[32m0.29210[0m[0m | time: 27.654s
| Adam | epoch: 004 | loss: 0.29210 - acc: 0.8858 -- iter: 04672/22500
Training Step: 1130  | total loss: [1m[32m0.28997[0m[0m | time: 28.038s
| Adam | epoch: 004 | loss: 0.28997 - acc: 0.8878 -- iter: 04736/22500
Training Step: 1131  | total loss: [1m[32m0.29666[0m[0m | time: 28.383s
| Adam | epoch: 004 | loss: 0.29666 - acc: 0.8865 -- iter: 04800/22500
Training Step: 1132  | total loss: [1m[32m0.30773[0m[0m | time: 28.732s
| Adam | epoch: 004 | loss: 0.30773 - acc: 0.8776 -- iter: 04864/22500
Training Step: 1133  | total loss: [1m[32m0.29733[0m[0m | time: 29.086s
| Adam | epoch: 004 | loss: 0.29733 - acc: 0.8820 -- iter: 04928/22500
Training Step: 1134  | total loss: [1m[32m0.30467[0m[0m | time: 29.476s
| Adam | epoch: 004 | loss: 0.30467 - acc: 0.8766 -- iter: 04992/22500
Training Step: 1135  | total loss: [1m[32m0.31691[0m[0m | time: 29.849s
| Adam | epoch: 004 | loss: 0.31691 - acc: 0.8702 -- iter: 05056/22500
Training Step: 1136  | total loss: [1m[32m0.33335[0m[0m | time: 30.223s
| Adam | epoch: 004 | loss: 0.33335 - acc: 0.8675 -- iter: 05120/22500
Training Step: 1137  | total loss: [1m[32m0.33447[0m[0m | time: 30.571s
| Adam | epoch: 004 | loss: 0.33447 - acc: 0.8667 -- iter: 05184/22500
Training Step: 1138  | total loss: [1m[32m0.32603[0m[0m | time: 30.918s
| Adam | epoch: 004 | loss: 0.32603 - acc: 0.8722 -- iter: 05248/22500
Training Step: 1139  | total loss: [1m[32m0.32532[0m[0m | time: 31.264s
| Adam | epoch: 004 | loss: 0.32532 - acc: 0.8756 -- iter: 05312/22500
Training Step: 1140  | total loss: [1m[32m0.32120[0m[0m | time: 31.615s
| Adam | epoch: 004 | loss: 0.32120 - acc: 0.8756 -- iter: 05376/22500
Training Step: 1141  | total loss: [1m[32m0.32452[0m[0m | time: 31.999s
| Adam | epoch: 004 | loss: 0.32452 - acc: 0.8740 -- iter: 05440/22500
Training Step: 1142  | total loss: [1m[32m0.32752[0m[0m | time: 32.349s
| Adam | epoch: 004 | loss: 0.32752 - acc: 0.8741 -- iter: 05504/22500
Training Step: 1143  | total loss: [1m[32m0.33859[0m[0m | time: 32.699s
| Adam | epoch: 004 | loss: 0.33859 - acc: 0.8663 -- iter: 05568/22500
Training Step: 1144  | total loss: [1m[32m0.33412[0m[0m | time: 33.047s
| Adam | epoch: 004 | loss: 0.33412 - acc: 0.8672 -- iter: 05632/22500
Training Step: 1145  | total loss: [1m[32m0.32452[0m[0m | time: 33.392s
| Adam | epoch: 004 | loss: 0.32452 - acc: 0.8758 -- iter: 05696/22500
Training Step: 1146  | total loss: [1m[32m0.31444[0m[0m | time: 33.757s
| Adam | epoch: 004 | loss: 0.31444 - acc: 0.8788 -- iter: 05760/22500
Training Step: 1147  | total loss: [1m[32m0.32111[0m[0m | time: 34.126s
| Adam | epoch: 004 | loss: 0.32111 - acc: 0.8675 -- iter: 05824/22500
Training Step: 1148  | total loss: [1m[32m0.31018[0m[0m | time: 34.474s
| Adam | epoch: 004 | loss: 0.31018 - acc: 0.8730 -- iter: 05888/22500
Training Step: 1149  | total loss: [1m[32m0.30600[0m[0m | time: 34.827s
| Adam | epoch: 004 | loss: 0.30600 - acc: 0.8763 -- iter: 05952/22500
Training Step: 1150  | total loss: [1m[32m0.30837[0m[0m | time: 35.176s
| Adam | epoch: 004 | loss: 0.30837 - acc: 0.8715 -- iter: 06016/22500
Training Step: 1151  | total loss: [1m[32m0.30678[0m[0m | time: 35.525s
| Adam | epoch: 004 | loss: 0.30678 - acc: 0.8765 -- iter: 06080/22500
Training Step: 1152  | total loss: [1m[32m0.30919[0m[0m | time: 35.872s
| Adam | epoch: 004 | loss: 0.30919 - acc: 0.8779 -- iter: 06144/22500
Training Step: 1153  | total loss: [1m[32m0.30571[0m[0m | time: 36.226s
| Adam | epoch: 004 | loss: 0.30571 - acc: 0.8808 -- iter: 06208/22500
Training Step: 1154  | total loss: [1m[32m0.31013[0m[0m | time: 36.609s
| Adam | epoch: 004 | loss: 0.31013 - acc: 0.8802 -- iter: 06272/22500
Training Step: 1155  | total loss: [1m[32m0.31768[0m[0m | time: 36.971s
| Adam | epoch: 004 | loss: 0.31768 - acc: 0.8750 -- iter: 06336/22500
Training Step: 1156  | total loss: [1m[32m0.32049[0m[0m | time: 37.318s
| Adam | epoch: 004 | loss: 0.32049 - acc: 0.8719 -- iter: 06400/22500
Training Step: 1157  | total loss: [1m[32m0.33498[0m[0m | time: 37.686s
| Adam | epoch: 004 | loss: 0.33498 - acc: 0.8628 -- iter: 06464/22500
Training Step: 1158  | total loss: [1m[32m0.33676[0m[0m | time: 38.054s
| Adam | epoch: 004 | loss: 0.33676 - acc: 0.8656 -- iter: 06528/22500
Training Step: 1159  | total loss: [1m[32m0.33531[0m[0m | time: 38.401s
| Adam | epoch: 004 | loss: 0.33531 - acc: 0.8650 -- iter: 06592/22500
Training Step: 1160  | total loss: [1m[32m0.35037[0m[0m | time: 38.751s
| Adam | epoch: 004 | loss: 0.35037 - acc: 0.8613 -- iter: 06656/22500
Training Step: 1161  | total loss: [1m[32m0.34995[0m[0m | time: 39.108s
| Adam | epoch: 004 | loss: 0.34995 - acc: 0.8595 -- iter: 06720/22500
Training Step: 1162  | total loss: [1m[32m0.33998[0m[0m | time: 39.455s
| Adam | epoch: 004 | loss: 0.33998 - acc: 0.8642 -- iter: 06784/22500
Training Step: 1163  | total loss: [1m[32m0.32940[0m[0m | time: 39.803s
| Adam | epoch: 004 | loss: 0.32940 - acc: 0.8684 -- iter: 06848/22500
Training Step: 1164  | total loss: [1m[32m0.33017[0m[0m | time: 40.152s
| Adam | epoch: 004 | loss: 0.33017 - acc: 0.8691 -- iter: 06912/22500
Training Step: 1165  | total loss: [1m[32m0.32102[0m[0m | time: 40.500s
| Adam | epoch: 004 | loss: 0.32102 - acc: 0.8743 -- iter: 06976/22500
Training Step: 1166  | total loss: [1m[32m0.31745[0m[0m | time: 40.849s
| Adam | epoch: 004 | loss: 0.31745 - acc: 0.8760 -- iter: 07040/22500
Training Step: 1167  | total loss: [1m[32m0.31224[0m[0m | time: 41.198s
| Adam | epoch: 004 | loss: 0.31224 - acc: 0.8790 -- iter: 07104/22500
Training Step: 1168  | total loss: [1m[32m0.30755[0m[0m | time: 41.548s
| Adam | epoch: 004 | loss: 0.30755 - acc: 0.8817 -- iter: 07168/22500
Training Step: 1169  | total loss: [1m[32m0.31904[0m[0m | time: 41.903s
| Adam | epoch: 004 | loss: 0.31904 - acc: 0.8717 -- iter: 07232/22500
Training Step: 1170  | total loss: [1m[32m0.31086[0m[0m | time: 42.253s
| Adam | epoch: 004 | loss: 0.31086 - acc: 0.8798 -- iter: 07296/22500
Training Step: 1171  | total loss: [1m[32m0.31067[0m[0m | time: 42.601s
| Adam | epoch: 004 | loss: 0.31067 - acc: 0.8793 -- iter: 07360/22500
Training Step: 1172  | total loss: [1m[32m0.30578[0m[0m | time: 42.950s
| Adam | epoch: 004 | loss: 0.30578 - acc: 0.8820 -- iter: 07424/22500
Training Step: 1173  | total loss: [1m[32m0.30277[0m[0m | time: 43.301s
| Adam | epoch: 004 | loss: 0.30277 - acc: 0.8876 -- iter: 07488/22500
Training Step: 1174  | total loss: [1m[32m0.30084[0m[0m | time: 43.647s
| Adam | epoch: 004 | loss: 0.30084 - acc: 0.8863 -- iter: 07552/22500
Training Step: 1175  | total loss: [1m[32m0.29467[0m[0m | time: 43.996s
| Adam | epoch: 004 | loss: 0.29467 - acc: 0.8867 -- iter: 07616/22500
Training Step: 1176  | total loss: [1m[32m0.29665[0m[0m | time: 44.346s
| Adam | epoch: 004 | loss: 0.29665 - acc: 0.8856 -- iter: 07680/22500
Training Step: 1177  | total loss: [1m[32m0.31158[0m[0m | time: 44.704s
| Adam | epoch: 004 | loss: 0.31158 - acc: 0.8798 -- iter: 07744/22500
Training Step: 1178  | total loss: [1m[32m0.30874[0m[0m | time: 45.054s
| Adam | epoch: 004 | loss: 0.30874 - acc: 0.8825 -- iter: 07808/22500
Training Step: 1179  | total loss: [1m[32m0.30872[0m[0m | time: 45.401s
| Adam | epoch: 004 | loss: 0.30872 - acc: 0.8802 -- iter: 07872/22500
Training Step: 1180  | total loss: [1m[32m0.30211[0m[0m | time: 45.785s
| Adam | epoch: 004 | loss: 0.30211 - acc: 0.8828 -- iter: 07936/22500
Training Step: 1181  | total loss: [1m[32m0.32106[0m[0m | time: 46.154s
| Adam | epoch: 004 | loss: 0.32106 - acc: 0.8742 -- iter: 08000/22500
Training Step: 1182  | total loss: [1m[32m0.31262[0m[0m | time: 46.538s
| Adam | epoch: 004 | loss: 0.31262 - acc: 0.8774 -- iter: 08064/22500
Training Step: 1183  | total loss: [1m[32m0.31499[0m[0m | time: 46.891s
| Adam | epoch: 004 | loss: 0.31499 - acc: 0.8771 -- iter: 08128/22500
Training Step: 1184  | total loss: [1m[32m0.30704[0m[0m | time: 47.237s
| Adam | epoch: 004 | loss: 0.30704 - acc: 0.8816 -- iter: 08192/22500
Training Step: 1185  | total loss: [1m[32m0.30359[0m[0m | time: 47.620s
| Adam | epoch: 004 | loss: 0.30359 - acc: 0.8810 -- iter: 08256/22500
Training Step: 1186  | total loss: [1m[32m0.30772[0m[0m | time: 47.969s
| Adam | epoch: 004 | loss: 0.30772 - acc: 0.8772 -- iter: 08320/22500
Training Step: 1187  | total loss: [1m[32m0.30884[0m[0m | time: 48.352s
| Adam | epoch: 004 | loss: 0.30884 - acc: 0.8770 -- iter: 08384/22500
Training Step: 1188  | total loss: [1m[32m0.30454[0m[0m | time: 48.736s
| Adam | epoch: 004 | loss: 0.30454 - acc: 0.8815 -- iter: 08448/22500
Training Step: 1189  | total loss: [1m[32m0.30683[0m[0m | time: 49.083s
| Adam | epoch: 004 | loss: 0.30683 - acc: 0.8746 -- iter: 08512/22500
Training Step: 1190  | total loss: [1m[32m0.29964[0m[0m | time: 49.431s
| Adam | epoch: 004 | loss: 0.29964 - acc: 0.8762 -- iter: 08576/22500
Training Step: 1191  | total loss: [1m[32m0.30856[0m[0m | time: 49.838s
| Adam | epoch: 004 | loss: 0.30856 - acc: 0.8714 -- iter: 08640/22500
Training Step: 1192  | total loss: [1m[32m0.31890[0m[0m | time: 50.245s
| Adam | epoch: 004 | loss: 0.31890 - acc: 0.8702 -- iter: 08704/22500
Training Step: 1193  | total loss: [1m[32m0.31109[0m[0m | time: 50.626s
| Adam | epoch: 004 | loss: 0.31109 - acc: 0.8769 -- iter: 08768/22500
Training Step: 1194  | total loss: [1m[32m0.30789[0m[0m | time: 50.977s
| Adam | epoch: 004 | loss: 0.30789 - acc: 0.8767 -- iter: 08832/22500
Training Step: 1195  | total loss: [1m[32m0.30381[0m[0m | time: 51.320s
| Adam | epoch: 004 | loss: 0.30381 - acc: 0.8797 -- iter: 08896/22500
Training Step: 1196  | total loss: [1m[32m0.30513[0m[0m | time: 51.667s
| Adam | epoch: 004 | loss: 0.30513 - acc: 0.8808 -- iter: 08960/22500
Training Step: 1197  | total loss: [1m[32m0.31039[0m[0m | time: 52.022s
| Adam | epoch: 004 | loss: 0.31039 - acc: 0.8786 -- iter: 09024/22500
Training Step: 1198  | total loss: [1m[32m0.30536[0m[0m | time: 52.368s
| Adam | epoch: 004 | loss: 0.30536 - acc: 0.8830 -- iter: 09088/22500
Training Step: 1199  | total loss: [1m[32m0.30598[0m[0m | time: 52.715s
| Adam | epoch: 004 | loss: 0.30598 - acc: 0.8806 -- iter: 09152/22500
Training Step: 1200  | total loss: [1m[32m0.30061[0m[0m | time: 56.336s
| Adam | epoch: 004 | loss: 0.30061 - acc: 0.8800 | val_loss: 0.49266 - val_acc: 0.8056 -- iter: 09216/22500
--
Training Step: 1201  | total loss: [1m[32m0.29583[0m[0m | time: 56.723s
| Adam | epoch: 004 | loss: 0.29583 - acc: 0.8811 -- iter: 09280/22500
Training Step: 1202  | total loss: [1m[32m0.29323[0m[0m | time: 57.108s
| Adam | epoch: 004 | loss: 0.29323 - acc: 0.8836 -- iter: 09344/22500
Training Step: 1203  | total loss: [1m[32m0.29323[0m[0m | time: 57.481s
| Adam | epoch: 004 | loss: 0.29323 - acc: 0.8828 -- iter: 09408/22500
Training Step: 1204  | total loss: [1m[32m0.28752[0m[0m | time: 57.854s
| Adam | epoch: 004 | loss: 0.28752 - acc: 0.8851 -- iter: 09472/22500
Training Step: 1205  | total loss: [1m[32m0.28659[0m[0m | time: 58.202s
| Adam | epoch: 004 | loss: 0.28659 - acc: 0.8857 -- iter: 09536/22500
Training Step: 1206  | total loss: [1m[32m0.28816[0m[0m | time: 58.553s
| Adam | epoch: 004 | loss: 0.28816 - acc: 0.8830 -- iter: 09600/22500
Training Step: 1207  | total loss: [1m[32m0.29118[0m[0m | time: 58.902s
| Adam | epoch: 004 | loss: 0.29118 - acc: 0.8838 -- iter: 09664/22500
Training Step: 1208  | total loss: [1m[32m0.28427[0m[0m | time: 59.249s
| Adam | epoch: 004 | loss: 0.28427 - acc: 0.8876 -- iter: 09728/22500
Training Step: 1209  | total loss: [1m[32m0.30038[0m[0m | time: 59.608s
| Adam | epoch: 004 | loss: 0.30038 - acc: 0.8816 -- iter: 09792/22500
Training Step: 1210  | total loss: [1m[32m0.30261[0m[0m | time: 59.957s
| Adam | epoch: 004 | loss: 0.30261 - acc: 0.8825 -- iter: 09856/22500
Training Step: 1211  | total loss: [1m[32m0.30942[0m[0m | time: 60.305s
| Adam | epoch: 004 | loss: 0.30942 - acc: 0.8802 -- iter: 09920/22500
Training Step: 1212  | total loss: [1m[32m0.30261[0m[0m | time: 60.656s
| Adam | epoch: 004 | loss: 0.30261 - acc: 0.8813 -- iter: 09984/22500
Training Step: 1213  | total loss: [1m[32m0.29613[0m[0m | time: 61.008s
| Adam | epoch: 004 | loss: 0.29613 - acc: 0.8853 -- iter: 10048/22500
Training Step: 1214  | total loss: [1m[32m0.29779[0m[0m | time: 61.375s
| Adam | epoch: 004 | loss: 0.29779 - acc: 0.8843 -- iter: 10112/22500
Training Step: 1215  | total loss: [1m[32m0.31078[0m[0m | time: 61.745s
| Adam | epoch: 004 | loss: 0.31078 - acc: 0.8802 -- iter: 10176/22500
Training Step: 1216  | total loss: [1m[32m0.31210[0m[0m | time: 62.094s
| Adam | epoch: 004 | loss: 0.31210 - acc: 0.8766 -- iter: 10240/22500
Training Step: 1217  | total loss: [1m[32m0.31433[0m[0m | time: 62.448s
| Adam | epoch: 004 | loss: 0.31433 - acc: 0.8796 -- iter: 10304/22500
Training Step: 1218  | total loss: [1m[32m0.31313[0m[0m | time: 62.795s
| Adam | epoch: 004 | loss: 0.31313 - acc: 0.8791 -- iter: 10368/22500
Training Step: 1219  | total loss: [1m[32m0.30658[0m[0m | time: 63.147s
| Adam | epoch: 004 | loss: 0.30658 - acc: 0.8849 -- iter: 10432/22500
Training Step: 1220  | total loss: [1m[32m0.30458[0m[0m | time: 63.495s
| Adam | epoch: 004 | loss: 0.30458 - acc: 0.8839 -- iter: 10496/22500
Training Step: 1221  | total loss: [1m[32m0.30379[0m[0m | time: 63.844s
| Adam | epoch: 004 | loss: 0.30379 - acc: 0.8831 -- iter: 10560/22500
Training Step: 1222  | total loss: [1m[32m0.30363[0m[0m | time: 64.199s
| Adam | epoch: 004 | loss: 0.30363 - acc: 0.8854 -- iter: 10624/22500
Training Step: 1223  | total loss: [1m[32m0.30447[0m[0m | time: 64.551s
| Adam | epoch: 004 | loss: 0.30447 - acc: 0.8843 -- iter: 10688/22500
Training Step: 1224  | total loss: [1m[32m0.31579[0m[0m | time: 64.902s
| Adam | epoch: 004 | loss: 0.31579 - acc: 0.8787 -- iter: 10752/22500
Training Step: 1225  | total loss: [1m[32m0.31637[0m[0m | time: 65.274s
| Adam | epoch: 004 | loss: 0.31637 - acc: 0.8768 -- iter: 10816/22500
Training Step: 1226  | total loss: [1m[32m0.32643[0m[0m | time: 65.645s
| Adam | epoch: 004 | loss: 0.32643 - acc: 0.8704 -- iter: 10880/22500
Training Step: 1227  | total loss: [1m[32m0.31964[0m[0m | time: 65.999s
| Adam | epoch: 004 | loss: 0.31964 - acc: 0.8693 -- iter: 10944/22500
Training Step: 1228  | total loss: [1m[32m0.31393[0m[0m | time: 66.352s
| Adam | epoch: 004 | loss: 0.31393 - acc: 0.8714 -- iter: 11008/22500
Training Step: 1229  | total loss: [1m[32m0.31842[0m[0m | time: 66.737s
| Adam | epoch: 004 | loss: 0.31842 - acc: 0.8686 -- iter: 11072/22500
Training Step: 1230  | total loss: [1m[32m0.32859[0m[0m | time: 67.107s
| Adam | epoch: 004 | loss: 0.32859 - acc: 0.8646 -- iter: 11136/22500
Training Step: 1231  | total loss: [1m[32m0.33273[0m[0m | time: 67.459s
| Adam | epoch: 004 | loss: 0.33273 - acc: 0.8625 -- iter: 11200/22500
Training Step: 1232  | total loss: [1m[32m0.32850[0m[0m | time: 67.809s
| Adam | epoch: 004 | loss: 0.32850 - acc: 0.8606 -- iter: 11264/22500
Training Step: 1233  | total loss: [1m[32m0.32182[0m[0m | time: 68.157s
| Adam | epoch: 004 | loss: 0.32182 - acc: 0.8652 -- iter: 11328/22500
Training Step: 1234  | total loss: [1m[32m0.31425[0m[0m | time: 68.505s
| Adam | epoch: 004 | loss: 0.31425 - acc: 0.8693 -- iter: 11392/22500
Training Step: 1235  | total loss: [1m[32m0.31519[0m[0m | time: 68.855s
| Adam | epoch: 004 | loss: 0.31519 - acc: 0.8667 -- iter: 11456/22500
Training Step: 1236  | total loss: [1m[32m0.31519[0m[0m | time: 69.208s
| Adam | epoch: 004 | loss: 0.31519 - acc: 0.8660 -- iter: 11520/22500
Training Step: 1237  | total loss: [1m[32m0.30970[0m[0m | time: 69.557s
| Adam | epoch: 004 | loss: 0.30970 - acc: 0.8700 -- iter: 11584/22500
Training Step: 1238  | total loss: [1m[32m0.30408[0m[0m | time: 69.902s
| Adam | epoch: 004 | loss: 0.30408 - acc: 0.8721 -- iter: 11648/22500
Training Step: 1239  | total loss: [1m[32m0.30504[0m[0m | time: 70.250s
| Adam | epoch: 004 | loss: 0.30504 - acc: 0.8708 -- iter: 11712/22500
Training Step: 1240  | total loss: [1m[32m0.32153[0m[0m | time: 70.602s
| Adam | epoch: 004 | loss: 0.32153 - acc: 0.8665 -- iter: 11776/22500
Training Step: 1241  | total loss: [1m[32m0.31278[0m[0m | time: 70.952s
| Adam | epoch: 004 | loss: 0.31278 - acc: 0.8705 -- iter: 11840/22500
Training Step: 1242  | total loss: [1m[32m0.31263[0m[0m | time: 71.296s
| Adam | epoch: 004 | loss: 0.31263 - acc: 0.8710 -- iter: 11904/22500
Training Step: 1243  | total loss: [1m[32m0.30729[0m[0m | time: 71.649s
| Adam | epoch: 004 | loss: 0.30729 - acc: 0.8745 -- iter: 11968/22500
Training Step: 1244  | total loss: [1m[32m0.31611[0m[0m | time: 71.997s
| Adam | epoch: 004 | loss: 0.31611 - acc: 0.8667 -- iter: 12032/22500
Training Step: 1245  | total loss: [1m[32m0.31262[0m[0m | time: 72.342s
| Adam | epoch: 004 | loss: 0.31262 - acc: 0.8722 -- iter: 12096/22500
Training Step: 1246  | total loss: [1m[32m0.32730[0m[0m | time: 72.695s
| Adam | epoch: 004 | loss: 0.32730 - acc: 0.8663 -- iter: 12160/22500
Training Step: 1247  | total loss: [1m[32m0.32801[0m[0m | time: 73.043s
| Adam | epoch: 004 | loss: 0.32801 - acc: 0.8656 -- iter: 12224/22500
Training Step: 1248  | total loss: [1m[32m0.33570[0m[0m | time: 73.431s
| Adam | epoch: 004 | loss: 0.33570 - acc: 0.8618 -- iter: 12288/22500
Training Step: 1249  | total loss: [1m[32m0.32637[0m[0m | time: 73.804s
| Adam | epoch: 004 | loss: 0.32637 - acc: 0.8663 -- iter: 12352/22500
Training Step: 1250  | total loss: [1m[32m0.31993[0m[0m | time: 74.147s
| Adam | epoch: 004 | loss: 0.31993 - acc: 0.8703 -- iter: 12416/22500
Training Step: 1251  | total loss: [1m[32m0.33782[0m[0m | time: 74.509s
| Adam | epoch: 004 | loss: 0.33782 - acc: 0.8629 -- iter: 12480/22500
Training Step: 1252  | total loss: [1m[32m0.33669[0m[0m | time: 74.857s
| Adam | epoch: 004 | loss: 0.33669 - acc: 0.8657 -- iter: 12544/22500
Training Step: 1253  | total loss: [1m[32m0.31689[0m[0m | time: 75.208s
| Adam | epoch: 004 | loss: 0.31689 - acc: 0.8760 -- iter: 12608/22500
Training Step: 1254  | total loss: [1m[32m0.31153[0m[0m | time: 75.556s
| Adam | epoch: 004 | loss: 0.31153 - acc: 0.8775 -- iter: 12672/22500
Training Step: 1255  | total loss: [1m[32m0.30947[0m[0m | time: 75.903s
| Adam | epoch: 004 | loss: 0.30947 - acc: 0.8819 -- iter: 12736/22500
Training Step: 1256  | total loss: [1m[32m0.30442[0m[0m | time: 76.254s
| Adam | epoch: 004 | loss: 0.30442 - acc: 0.8828 -- iter: 12800/22500
Training Step: 1257  | total loss: [1m[32m0.30990[0m[0m | time: 76.608s
| Adam | epoch: 004 | loss: 0.30990 - acc: 0.8773 -- iter: 12864/22500
Training Step: 1258  | total loss: [1m[32m0.31502[0m[0m | time: 76.955s
| Adam | epoch: 004 | loss: 0.31502 - acc: 0.8724 -- iter: 12928/22500
Training Step: 1259  | total loss: [1m[32m0.33219[0m[0m | time: 77.318s
| Adam | epoch: 004 | loss: 0.33219 - acc: 0.8680 -- iter: 12992/22500
Training Step: 1260  | total loss: [1m[32m0.33204[0m[0m | time: 77.689s
| Adam | epoch: 004 | loss: 0.33204 - acc: 0.8687 -- iter: 13056/22500
Training Step: 1261  | total loss: [1m[32m0.33547[0m[0m | time: 78.038s
| Adam | epoch: 004 | loss: 0.33547 - acc: 0.8662 -- iter: 13120/22500
Training Step: 1262  | total loss: [1m[32m0.34052[0m[0m | time: 78.389s
| Adam | epoch: 004 | loss: 0.34052 - acc: 0.8655 -- iter: 13184/22500
Training Step: 1263  | total loss: [1m[32m0.33149[0m[0m | time: 78.737s
| Adam | epoch: 004 | loss: 0.33149 - acc: 0.8680 -- iter: 13248/22500
Training Step: 1264  | total loss: [1m[32m0.33001[0m[0m | time: 79.088s
| Adam | epoch: 004 | loss: 0.33001 - acc: 0.8671 -- iter: 13312/22500
Training Step: 1265  | total loss: [1m[32m0.34101[0m[0m | time: 79.441s
| Adam | epoch: 004 | loss: 0.34101 - acc: 0.8632 -- iter: 13376/22500
Training Step: 1266  | total loss: [1m[32m0.33350[0m[0m | time: 79.789s
| Adam | epoch: 004 | loss: 0.33350 - acc: 0.8675 -- iter: 13440/22500
Training Step: 1267  | total loss: [1m[32m0.33418[0m[0m | time: 80.144s
| Adam | epoch: 004 | loss: 0.33418 - acc: 0.8667 -- iter: 13504/22500
Training Step: 1268  | total loss: [1m[32m0.32692[0m[0m | time: 80.492s
| Adam | epoch: 004 | loss: 0.32692 - acc: 0.8707 -- iter: 13568/22500
Training Step: 1269  | total loss: [1m[32m0.33235[0m[0m | time: 80.841s
| Adam | epoch: 004 | loss: 0.33235 - acc: 0.8680 -- iter: 13632/22500
Training Step: 1270  | total loss: [1m[32m0.32343[0m[0m | time: 81.214s
| Adam | epoch: 004 | loss: 0.32343 - acc: 0.8718 -- iter: 13696/22500
Training Step: 1271  | total loss: [1m[32m0.32371[0m[0m | time: 81.582s
| Adam | epoch: 004 | loss: 0.32371 - acc: 0.8737 -- iter: 13760/22500
Training Step: 1272  | total loss: [1m[32m0.32394[0m[0m | time: 81.931s
| Adam | epoch: 004 | loss: 0.32394 - acc: 0.8738 -- iter: 13824/22500
Training Step: 1273  | total loss: [1m[32m0.32180[0m[0m | time: 82.291s
| Adam | epoch: 004 | loss: 0.32180 - acc: 0.8755 -- iter: 13888/22500
Training Step: 1274  | total loss: [1m[32m0.32305[0m[0m | time: 82.643s
| Adam | epoch: 004 | loss: 0.32305 - acc: 0.8739 -- iter: 13952/22500
Training Step: 1275  | total loss: [1m[32m0.32120[0m[0m | time: 82.997s
| Adam | epoch: 004 | loss: 0.32120 - acc: 0.8787 -- iter: 14016/22500
Training Step: 1276  | total loss: [1m[32m0.33420[0m[0m | time: 83.351s
| Adam | epoch: 004 | loss: 0.33420 - acc: 0.8736 -- iter: 14080/22500
Training Step: 1277  | total loss: [1m[32m0.33782[0m[0m | time: 83.699s
| Adam | epoch: 004 | loss: 0.33782 - acc: 0.8722 -- iter: 14144/22500
Training Step: 1278  | total loss: [1m[32m0.33186[0m[0m | time: 84.049s
| Adam | epoch: 004 | loss: 0.33186 - acc: 0.8772 -- iter: 14208/22500
Training Step: 1279  | total loss: [1m[32m0.33232[0m[0m | time: 84.404s
| Adam | epoch: 004 | loss: 0.33232 - acc: 0.8785 -- iter: 14272/22500
Training Step: 1280  | total loss: [1m[32m0.33035[0m[0m | time: 84.749s
| Adam | epoch: 004 | loss: 0.33035 - acc: 0.8766 -- iter: 14336/22500
Training Step: 1281  | total loss: [1m[32m0.32718[0m[0m | time: 85.102s
| Adam | epoch: 004 | loss: 0.32718 - acc: 0.8780 -- iter: 14400/22500
Training Step: 1282  | total loss: [1m[32m0.31768[0m[0m | time: 85.471s
| Adam | epoch: 004 | loss: 0.31768 - acc: 0.8808 -- iter: 14464/22500
Training Step: 1283  | total loss: [1m[32m0.31468[0m[0m | time: 85.842s
| Adam | epoch: 004 | loss: 0.31468 - acc: 0.8818 -- iter: 14528/22500
Training Step: 1284  | total loss: [1m[32m0.30921[0m[0m | time: 86.191s
| Adam | epoch: 004 | loss: 0.30921 - acc: 0.8858 -- iter: 14592/22500
Training Step: 1285  | total loss: [1m[32m0.30888[0m[0m | time: 86.542s
| Adam | epoch: 004 | loss: 0.30888 - acc: 0.8832 -- iter: 14656/22500
Training Step: 1286  | total loss: [1m[32m0.31370[0m[0m | time: 86.889s
| Adam | epoch: 004 | loss: 0.31370 - acc: 0.8824 -- iter: 14720/22500
Training Step: 1287  | total loss: [1m[32m0.30785[0m[0m | time: 87.235s
| Adam | epoch: 004 | loss: 0.30785 - acc: 0.8863 -- iter: 14784/22500
Training Step: 1288  | total loss: [1m[32m0.30248[0m[0m | time: 87.588s
| Adam | epoch: 004 | loss: 0.30248 - acc: 0.8899 -- iter: 14848/22500
Training Step: 1289  | total loss: [1m[32m0.31349[0m[0m | time: 87.941s
| Adam | epoch: 004 | loss: 0.31349 - acc: 0.8774 -- iter: 14912/22500
Training Step: 1290  | total loss: [1m[32m0.30428[0m[0m | time: 88.295s
| Adam | epoch: 004 | loss: 0.30428 - acc: 0.8819 -- iter: 14976/22500
Training Step: 1291  | total loss: [1m[32m0.30367[0m[0m | time: 88.645s
| Adam | epoch: 004 | loss: 0.30367 - acc: 0.8828 -- iter: 15040/22500
Training Step: 1292  | total loss: [1m[32m0.29569[0m[0m | time: 88.994s
| Adam | epoch: 004 | loss: 0.29569 - acc: 0.8851 -- iter: 15104/22500
Training Step: 1293  | total loss: [1m[32m0.29275[0m[0m | time: 89.367s
| Adam | epoch: 004 | loss: 0.29275 - acc: 0.8857 -- iter: 15168/22500
Training Step: 1294  | total loss: [1m[32m0.29636[0m[0m | time: 89.738s
| Adam | epoch: 004 | loss: 0.29636 - acc: 0.8846 -- iter: 15232/22500
Training Step: 1295  | total loss: [1m[32m0.30378[0m[0m | time: 90.091s
| Adam | epoch: 004 | loss: 0.30378 - acc: 0.8821 -- iter: 15296/22500
Training Step: 1296  | total loss: [1m[32m0.29251[0m[0m | time: 90.445s
| Adam | epoch: 004 | loss: 0.29251 - acc: 0.8861 -- iter: 15360/22500
Training Step: 1297  | total loss: [1m[32m0.29221[0m[0m | time: 90.794s
| Adam | epoch: 004 | loss: 0.29221 - acc: 0.8834 -- iter: 15424/22500
Training Step: 1298  | total loss: [1m[32m0.29552[0m[0m | time: 91.138s
| Adam | epoch: 004 | loss: 0.29552 - acc: 0.8857 -- iter: 15488/22500
Training Step: 1299  | total loss: [1m[32m0.28877[0m[0m | time: 91.489s
| Adam | epoch: 004 | loss: 0.28877 - acc: 0.8877 -- iter: 15552/22500
Training Step: 1300  | total loss: [1m[32m0.30846[0m[0m | time: 91.836s
| Adam | epoch: 004 | loss: 0.30846 - acc: 0.8740 -- iter: 15616/22500
Training Step: 1301  | total loss: [1m[32m0.30260[0m[0m | time: 92.184s
| Adam | epoch: 004 | loss: 0.30260 - acc: 0.8787 -- iter: 15680/22500
Training Step: 1302  | total loss: [1m[32m0.31741[0m[0m | time: 92.538s
| Adam | epoch: 004 | loss: 0.31741 - acc: 0.8721 -- iter: 15744/22500
Training Step: 1303  | total loss: [1m[32m0.31932[0m[0m | time: 92.891s
| Adam | epoch: 004 | loss: 0.31932 - acc: 0.8693 -- iter: 15808/22500
Training Step: 1304  | total loss: [1m[32m0.32536[0m[0m | time: 93.252s
| Adam | epoch: 004 | loss: 0.32536 - acc: 0.8652 -- iter: 15872/22500
Training Step: 1305  | total loss: [1m[32m0.32022[0m[0m | time: 93.615s
| Adam | epoch: 004 | loss: 0.32022 - acc: 0.8677 -- iter: 15936/22500
Training Step: 1306  | total loss: [1m[32m0.31544[0m[0m | time: 93.962s
| Adam | epoch: 004 | loss: 0.31544 - acc: 0.8731 -- iter: 16000/22500
Training Step: 1307  | total loss: [1m[32m0.31764[0m[0m | time: 94.311s
| Adam | epoch: 004 | loss: 0.31764 - acc: 0.8764 -- iter: 16064/22500
Training Step: 1308  | total loss: [1m[32m0.31396[0m[0m | time: 94.656s
| Adam | epoch: 004 | loss: 0.31396 - acc: 0.8794 -- iter: 16128/22500
Training Step: 1309  | total loss: [1m[32m0.31409[0m[0m | time: 95.007s
| Adam | epoch: 004 | loss: 0.31409 - acc: 0.8805 -- iter: 16192/22500
Training Step: 1310  | total loss: [1m[32m0.31441[0m[0m | time: 95.359s
| Adam | epoch: 004 | loss: 0.31441 - acc: 0.8769 -- iter: 16256/22500
Training Step: 1311  | total loss: [1m[32m0.31569[0m[0m | time: 95.708s
| Adam | epoch: 004 | loss: 0.31569 - acc: 0.8751 -- iter: 16320/22500
Training Step: 1312  | total loss: [1m[32m0.32841[0m[0m | time: 96.060s
| Adam | epoch: 004 | loss: 0.32841 - acc: 0.8720 -- iter: 16384/22500
Training Step: 1313  | total loss: [1m[32m0.32723[0m[0m | time: 96.414s
| Adam | epoch: 004 | loss: 0.32723 - acc: 0.8738 -- iter: 16448/22500
Training Step: 1314  | total loss: [1m[32m0.32751[0m[0m | time: 96.764s
| Adam | epoch: 004 | loss: 0.32751 - acc: 0.8755 -- iter: 16512/22500
Training Step: 1315  | total loss: [1m[32m0.31937[0m[0m | time: 97.141s
| Adam | epoch: 004 | loss: 0.31937 - acc: 0.8770 -- iter: 16576/22500
Training Step: 1316  | total loss: [1m[32m0.32813[0m[0m | time: 97.510s
| Adam | epoch: 004 | loss: 0.32813 - acc: 0.8706 -- iter: 16640/22500
Training Step: 1317  | total loss: [1m[32m0.32270[0m[0m | time: 97.839s
| Adam | epoch: 004 | loss: 0.32270 - acc: 0.8757 -- iter: 16704/22500
Training Step: 1318  | total loss: [1m[32m0.31580[0m[0m | time: 98.119s
| Adam | epoch: 004 | loss: 0.31580 - acc: 0.8803 -- iter: 16768/22500
Training Step: 1319  | total loss: [1m[32m0.32279[0m[0m | time: 98.406s
| Adam | epoch: 004 | loss: 0.32279 - acc: 0.8751 -- iter: 16832/22500
Training Step: 1320  | total loss: [1m[32m0.32630[0m[0m | time: 98.688s
| Adam | epoch: 004 | loss: 0.32630 - acc: 0.8704 -- iter: 16896/22500
Training Step: 1321  | total loss: [1m[32m0.32998[0m[0m | time: 98.973s
| Adam | epoch: 004 | loss: 0.32998 - acc: 0.8677 -- iter: 16960/22500
Training Step: 1322  | total loss: [1m[32m0.35241[0m[0m | time: 99.326s
| Adam | epoch: 004 | loss: 0.35241 - acc: 0.8591 -- iter: 17024/22500
Training Step: 1323  | total loss: [1m[32m0.36084[0m[0m | time: 99.676s
| Adam | epoch: 004 | loss: 0.36084 - acc: 0.8513 -- iter: 17088/22500
Training Step: 1324  | total loss: [1m[32m0.35833[0m[0m | time: 100.022s
| Adam | epoch: 004 | loss: 0.35833 - acc: 0.8537 -- iter: 17152/22500
Training Step: 1325  | total loss: [1m[32m0.34972[0m[0m | time: 100.370s
| Adam | epoch: 004 | loss: 0.34972 - acc: 0.8589 -- iter: 17216/22500
Training Step: 1326  | total loss: [1m[32m0.34004[0m[0m | time: 100.722s
| Adam | epoch: 004 | loss: 0.34004 - acc: 0.8668 -- iter: 17280/22500
Training Step: 1327  | total loss: [1m[32m0.33737[0m[0m | time: 101.090s
| Adam | epoch: 004 | loss: 0.33737 - acc: 0.8660 -- iter: 17344/22500
Training Step: 1328  | total loss: [1m[32m0.32833[0m[0m | time: 101.459s
| Adam | epoch: 004 | loss: 0.32833 - acc: 0.8669 -- iter: 17408/22500
Training Step: 1329  | total loss: [1m[32m0.32457[0m[0m | time: 101.809s
| Adam | epoch: 004 | loss: 0.32457 - acc: 0.8724 -- iter: 17472/22500
Training Step: 1330  | total loss: [1m[32m0.32331[0m[0m | time: 102.156s
| Adam | epoch: 004 | loss: 0.32331 - acc: 0.8727 -- iter: 17536/22500
Training Step: 1331  | total loss: [1m[32m0.32518[0m[0m | time: 102.509s
| Adam | epoch: 004 | loss: 0.32518 - acc: 0.8729 -- iter: 17600/22500
Training Step: 1332  | total loss: [1m[32m0.31667[0m[0m | time: 102.861s
| Adam | epoch: 004 | loss: 0.31667 - acc: 0.8778 -- iter: 17664/22500
Training Step: 1333  | total loss: [1m[32m0.31808[0m[0m | time: 103.212s
| Adam | epoch: 004 | loss: 0.31808 - acc: 0.8775 -- iter: 17728/22500
Training Step: 1334  | total loss: [1m[32m0.32820[0m[0m | time: 103.559s
| Adam | epoch: 004 | loss: 0.32820 - acc: 0.8726 -- iter: 17792/22500
Training Step: 1335  | total loss: [1m[32m0.33102[0m[0m | time: 103.908s
| Adam | epoch: 004 | loss: 0.33102 - acc: 0.8713 -- iter: 17856/22500
Training Step: 1336  | total loss: [1m[32m0.32468[0m[0m | time: 104.256s
| Adam | epoch: 004 | loss: 0.32468 - acc: 0.8763 -- iter: 17920/22500
Training Step: 1337  | total loss: [1m[32m0.31719[0m[0m | time: 104.610s
| Adam | epoch: 004 | loss: 0.31719 - acc: 0.8809 -- iter: 17984/22500
Training Step: 1338  | total loss: [1m[32m0.32906[0m[0m | time: 104.978s
| Adam | epoch: 004 | loss: 0.32906 - acc: 0.8740 -- iter: 18048/22500
Training Step: 1339  | total loss: [1m[32m0.31788[0m[0m | time: 105.352s
| Adam | epoch: 004 | loss: 0.31788 - acc: 0.8804 -- iter: 18112/22500
Training Step: 1340  | total loss: [1m[32m0.31581[0m[0m | time: 105.707s
| Adam | epoch: 004 | loss: 0.31581 - acc: 0.8845 -- iter: 18176/22500
Training Step: 1341  | total loss: [1m[32m0.31180[0m[0m | time: 106.057s
| Adam | epoch: 004 | loss: 0.31180 - acc: 0.8867 -- iter: 18240/22500
Training Step: 1342  | total loss: [1m[32m0.31935[0m[0m | time: 106.408s
| Adam | epoch: 004 | loss: 0.31935 - acc: 0.8793 -- iter: 18304/22500
Training Step: 1343  | total loss: [1m[32m0.32223[0m[0m | time: 106.755s
| Adam | epoch: 004 | loss: 0.32223 - acc: 0.8773 -- iter: 18368/22500
Training Step: 1344  | total loss: [1m[32m0.32293[0m[0m | time: 107.108s
| Adam | epoch: 004 | loss: 0.32293 - acc: 0.8771 -- iter: 18432/22500
Training Step: 1345  | total loss: [1m[32m0.33798[0m[0m | time: 107.459s
| Adam | epoch: 004 | loss: 0.33798 - acc: 0.8691 -- iter: 18496/22500
Training Step: 1346  | total loss: [1m[32m0.36031[0m[0m | time: 107.811s
| Adam | epoch: 004 | loss: 0.36031 - acc: 0.8634 -- iter: 18560/22500
Training Step: 1347  | total loss: [1m[32m0.36098[0m[0m | time: 108.162s
| Adam | epoch: 004 | loss: 0.36098 - acc: 0.8630 -- iter: 18624/22500
Training Step: 1348  | total loss: [1m[32m0.35801[0m[0m | time: 108.514s
| Adam | epoch: 004 | loss: 0.35801 - acc: 0.8658 -- iter: 18688/22500
Training Step: 1349  | total loss: [1m[32m0.35363[0m[0m | time: 108.880s
| Adam | epoch: 004 | loss: 0.35363 - acc: 0.8667 -- iter: 18752/22500
Training Step: 1350  | total loss: [1m[32m0.35760[0m[0m | time: 109.248s
| Adam | epoch: 004 | loss: 0.35760 - acc: 0.8644 -- iter: 18816/22500
Training Step: 1351  | total loss: [1m[32m0.36027[0m[0m | time: 109.596s
| Adam | epoch: 004 | loss: 0.36027 - acc: 0.8639 -- iter: 18880/22500
Training Step: 1352  | total loss: [1m[32m0.36685[0m[0m | time: 109.944s
| Adam | epoch: 004 | loss: 0.36685 - acc: 0.8619 -- iter: 18944/22500
Training Step: 1353  | total loss: [1m[32m0.36459[0m[0m | time: 110.297s
| Adam | epoch: 004 | loss: 0.36459 - acc: 0.8616 -- iter: 19008/22500
Training Step: 1354  | total loss: [1m[32m0.35369[0m[0m | time: 110.644s
| Adam | epoch: 004 | loss: 0.35369 - acc: 0.8661 -- iter: 19072/22500
Training Step: 1355  | total loss: [1m[32m0.34741[0m[0m | time: 110.994s
| Adam | epoch: 004 | loss: 0.34741 - acc: 0.8670 -- iter: 19136/22500
Training Step: 1356  | total loss: [1m[32m0.35591[0m[0m | time: 111.342s
| Adam | epoch: 004 | loss: 0.35591 - acc: 0.8693 -- iter: 19200/22500
Training Step: 1357  | total loss: [1m[32m0.35316[0m[0m | time: 111.689s
| Adam | epoch: 004 | loss: 0.35316 - acc: 0.8652 -- iter: 19264/22500
Training Step: 1358  | total loss: [1m[32m0.34226[0m[0m | time: 112.035s
| Adam | epoch: 004 | loss: 0.34226 - acc: 0.8709 -- iter: 19328/22500
Training Step: 1359  | total loss: [1m[32m0.33335[0m[0m | time: 112.391s
| Adam | epoch: 004 | loss: 0.33335 - acc: 0.8744 -- iter: 19392/22500
Training Step: 1360  | total loss: [1m[32m0.34170[0m[0m | time: 112.753s
| Adam | epoch: 004 | loss: 0.34170 - acc: 0.8698 -- iter: 19456/22500
Training Step: 1361  | total loss: [1m[32m0.33335[0m[0m | time: 113.124s
| Adam | epoch: 004 | loss: 0.33335 - acc: 0.8734 -- iter: 19520/22500
Training Step: 1362  | total loss: [1m[32m0.32083[0m[0m | time: 113.475s
| Adam | epoch: 004 | loss: 0.32083 - acc: 0.8798 -- iter: 19584/22500
Training Step: 1363  | total loss: [1m[32m0.32171[0m[0m | time: 113.826s
| Adam | epoch: 004 | loss: 0.32171 - acc: 0.8762 -- iter: 19648/22500
Training Step: 1364  | total loss: [1m[32m0.33602[0m[0m | time: 114.174s
| Adam | epoch: 004 | loss: 0.33602 - acc: 0.8699 -- iter: 19712/22500
Training Step: 1365  | total loss: [1m[32m0.35077[0m[0m | time: 114.550s
| Adam | epoch: 004 | loss: 0.35077 - acc: 0.8626 -- iter: 19776/22500
Training Step: 1366  | total loss: [1m[32m0.34399[0m[0m | time: 114.903s
| Adam | epoch: 004 | loss: 0.34399 - acc: 0.8685 -- iter: 19840/22500
Training Step: 1367  | total loss: [1m[32m0.34883[0m[0m | time: 115.280s
| Adam | epoch: 004 | loss: 0.34883 - acc: 0.8707 -- iter: 19904/22500
Training Step: 1368  | total loss: [1m[32m0.35469[0m[0m | time: 115.703s
| Adam | epoch: 004 | loss: 0.35469 - acc: 0.8618 -- iter: 19968/22500
Training Step: 1369  | total loss: [1m[32m0.34701[0m[0m | time: 116.088s
| Adam | epoch: 004 | loss: 0.34701 - acc: 0.8678 -- iter: 20032/22500
Training Step: 1370  | total loss: [1m[32m0.35188[0m[0m | time: 116.465s
| Adam | epoch: 004 | loss: 0.35188 - acc: 0.8669 -- iter: 20096/22500
Training Step: 1371  | total loss: [1m[32m0.35242[0m[0m | time: 116.764s
| Adam | epoch: 004 | loss: 0.35242 - acc: 0.8662 -- iter: 20160/22500
Training Step: 1372  | total loss: [1m[32m0.35181[0m[0m | time: 117.063s
| Adam | epoch: 004 | loss: 0.35181 - acc: 0.8655 -- iter: 20224/22500
Training Step: 1373  | total loss: [1m[32m0.34814[0m[0m | time: 117.366s
| Adam | epoch: 004 | loss: 0.34814 - acc: 0.8727 -- iter: 20288/22500
Training Step: 1374  | total loss: [1m[32m0.34298[0m[0m | time: 117.650s
| Adam | epoch: 004 | loss: 0.34298 - acc: 0.8761 -- iter: 20352/22500
Training Step: 1375  | total loss: [1m[32m0.34727[0m[0m | time: 118.023s
| Adam | epoch: 004 | loss: 0.34727 - acc: 0.8744 -- iter: 20416/22500
Training Step: 1376  | total loss: [1m[32m0.35329[0m[0m | time: 118.373s
| Adam | epoch: 004 | loss: 0.35329 - acc: 0.8729 -- iter: 20480/22500
Training Step: 1377  | total loss: [1m[32m0.35044[0m[0m | time: 118.730s
| Adam | epoch: 004 | loss: 0.35044 - acc: 0.8731 -- iter: 20544/22500
Training Step: 1378  | total loss: [1m[32m0.36666[0m[0m | time: 119.082s
| Adam | epoch: 004 | loss: 0.36666 - acc: 0.8670 -- iter: 20608/22500
Training Step: 1379  | total loss: [1m[32m0.35682[0m[0m | time: 119.435s
| Adam | epoch: 004 | loss: 0.35682 - acc: 0.8710 -- iter: 20672/22500
Training Step: 1380  | total loss: [1m[32m0.45098[0m[0m | time: 119.795s
| Adam | epoch: 004 | loss: 0.45098 - acc: 0.8417 -- iter: 20736/22500
Training Step: 1381  | total loss: [1m[32m0.43126[0m[0m | time: 120.142s
| Adam | epoch: 004 | loss: 0.43126 - acc: 0.8513 -- iter: 20800/22500
Training Step: 1382  | total loss: [1m[32m0.42736[0m[0m | time: 120.493s
| Adam | epoch: 004 | loss: 0.42736 - acc: 0.8536 -- iter: 20864/22500
Training Step: 1383  | total loss: [1m[32m0.40598[0m[0m | time: 120.858s
| Adam | epoch: 004 | loss: 0.40598 - acc: 0.8620 -- iter: 20928/22500
Training Step: 1384  | total loss: [1m[32m0.40642[0m[0m | time: 121.227s
| Adam | epoch: 004 | loss: 0.40642 - acc: 0.8633 -- iter: 20992/22500
Training Step: 1385  | total loss: [1m[32m0.40781[0m[0m | time: 121.583s
| Adam | epoch: 004 | loss: 0.40781 - acc: 0.8629 -- iter: 21056/22500
Training Step: 1386  | total loss: [1m[32m0.38531[0m[0m | time: 121.930s
| Adam | epoch: 004 | loss: 0.38531 - acc: 0.8719 -- iter: 21120/22500
Training Step: 1387  | total loss: [1m[32m0.36866[0m[0m | time: 122.281s
| Adam | epoch: 004 | loss: 0.36866 - acc: 0.8754 -- iter: 21184/22500
Training Step: 1388  | total loss: [1m[32m0.36455[0m[0m | time: 122.633s
| Adam | epoch: 004 | loss: 0.36455 - acc: 0.8800 -- iter: 21248/22500
Training Step: 1389  | total loss: [1m[32m0.35225[0m[0m | time: 122.981s
| Adam | epoch: 004 | loss: 0.35225 - acc: 0.8826 -- iter: 21312/22500
Training Step: 1390  | total loss: [1m[32m0.34441[0m[0m | time: 123.331s
| Adam | epoch: 004 | loss: 0.34441 - acc: 0.8834 -- iter: 21376/22500
Training Step: 1391  | total loss: [1m[32m0.32623[0m[0m | time: 123.681s
| Adam | epoch: 004 | loss: 0.32623 - acc: 0.8904 -- iter: 21440/22500
Training Step: 1392  | total loss: [1m[32m0.33257[0m[0m | time: 124.034s
| Adam | epoch: 004 | loss: 0.33257 - acc: 0.8826 -- iter: 21504/22500
Training Step: 1393  | total loss: [1m[32m0.32901[0m[0m | time: 124.387s
| Adam | epoch: 004 | loss: 0.32901 - acc: 0.8819 -- iter: 21568/22500
Training Step: 1394  | total loss: [1m[32m0.31686[0m[0m | time: 124.754s
| Adam | epoch: 004 | loss: 0.31686 - acc: 0.8874 -- iter: 21632/22500
Training Step: 1395  | total loss: [1m[32m0.31517[0m[0m | time: 125.126s
| Adam | epoch: 004 | loss: 0.31517 - acc: 0.8862 -- iter: 21696/22500
Training Step: 1396  | total loss: [1m[32m0.30114[0m[0m | time: 125.475s
| Adam | epoch: 004 | loss: 0.30114 - acc: 0.8913 -- iter: 21760/22500
Training Step: 1397  | total loss: [1m[32m0.29896[0m[0m | time: 125.826s
| Adam | epoch: 004 | loss: 0.29896 - acc: 0.8928 -- iter: 21824/22500
Training Step: 1398  | total loss: [1m[32m0.29026[0m[0m | time: 126.207s
| Adam | epoch: 004 | loss: 0.29026 - acc: 0.8926 -- iter: 21888/22500
Training Step: 1399  | total loss: [1m[32m0.30428[0m[0m | time: 126.559s
| Adam | epoch: 004 | loss: 0.30428 - acc: 0.8877 -- iter: 21952/22500
Training Step: 1400  | total loss: [1m[32m0.30133[0m[0m | time: 130.133s
| Adam | epoch: 004 | loss: 0.30133 - acc: 0.8896 | val_loss: 0.47006 - val_acc: 0.8036 -- iter: 22016/22500
--
Training Step: 1401  | total loss: [1m[32m0.29923[0m[0m | time: 130.466s
| Adam | epoch: 004 | loss: 0.29923 - acc: 0.8897 -- iter: 22080/22500
Training Step: 1402  | total loss: [1m[32m0.30496[0m[0m | time: 130.811s
| Adam | epoch: 004 | loss: 0.30496 - acc: 0.8882 -- iter: 22144/22500
Training Step: 1403  | total loss: [1m[32m0.29925[0m[0m | time: 131.159s
| Adam | epoch: 004 | loss: 0.29925 - acc: 0.8931 -- iter: 22208/22500
Training Step: 1404  | total loss: [1m[32m0.30532[0m[0m | time: 131.509s
| Adam | epoch: 004 | loss: 0.30532 - acc: 0.8882 -- iter: 22272/22500
Training Step: 1405  | total loss: [1m[32m0.29792[0m[0m | time: 131.862s
| Adam | epoch: 004 | loss: 0.29792 - acc: 0.8931 -- iter: 22336/22500
Training Step: 1406  | total loss: [1m[32m0.29066[0m[0m | time: 132.229s
| Adam | epoch: 004 | loss: 0.29066 - acc: 0.8976 -- iter: 22400/22500
Training Step: 1407  | total loss: [1m[32m0.28340[0m[0m | time: 132.601s
| Adam | epoch: 004 | loss: 0.28340 - acc: 0.9016 -- iter: 22464/22500
Training Step: 1408  | total loss: [1m[32m0.27835[0m[0m | time: 136.123s
| Adam | epoch: 004 | loss: 0.27835 - acc: 0.9036 | val_loss: 0.46401 - val_acc: 0.8052 -- iter: 22500/22500
--
Training Step: 1409  | total loss: [1m[32m0.27272[0m[0m | time: 0.353s
| Adam | epoch: 005 | loss: 0.27272 - acc: 0.9070 -- iter: 00064/22500
Training Step: 1410  | total loss: [1m[32m0.28103[0m[0m | time: 0.739s
| Adam | epoch: 005 | loss: 0.28103 - acc: 0.9038 -- iter: 00128/22500
Training Step: 1411  | total loss: [1m[32m0.27772[0m[0m | time: 1.053s
| Adam | epoch: 005 | loss: 0.27772 - acc: 0.9040 -- iter: 00192/22500
Training Step: 1412  | total loss: [1m[32m0.26573[0m[0m | time: 1.360s
| Adam | epoch: 005 | loss: 0.26573 - acc: 0.9081 -- iter: 00256/22500
Training Step: 1413  | total loss: [1m[32m0.25191[0m[0m | time: 1.778s
| Adam | epoch: 005 | loss: 0.25191 - acc: 0.9173 -- iter: 00320/22500
Training Step: 1414  | total loss: [1m[32m0.25734[0m[0m | time: 2.266s
| Adam | epoch: 005 | loss: 0.25734 - acc: 0.9177 -- iter: 00384/22500
Training Step: 1415  | total loss: [1m[32m0.25286[0m[0m | time: 2.699s
| Adam | epoch: 005 | loss: 0.25286 - acc: 0.9181 -- iter: 00448/22500
Training Step: 1416  | total loss: [1m[32m0.24198[0m[0m | time: 3.093s
| Adam | epoch: 005 | loss: 0.24198 - acc: 0.9216 -- iter: 00512/22500
Training Step: 1417  | total loss: [1m[32m0.25480[0m[0m | time: 3.448s
| Adam | epoch: 005 | loss: 0.25480 - acc: 0.9185 -- iter: 00576/22500
Training Step: 1418  | total loss: [1m[32m0.25014[0m[0m | time: 3.849s
| Adam | epoch: 005 | loss: 0.25014 - acc: 0.9157 -- iter: 00640/22500
Training Step: 1419  | total loss: [1m[32m0.24402[0m[0m | time: 4.219s
| Adam | epoch: 005 | loss: 0.24402 - acc: 0.9164 -- iter: 00704/22500
Training Step: 1420  | total loss: [1m[32m0.24019[0m[0m | time: 4.576s
| Adam | epoch: 005 | loss: 0.24019 - acc: 0.9185 -- iter: 00768/22500
Training Step: 1421  | total loss: [1m[32m0.24499[0m[0m | time: 4.928s
| Adam | epoch: 005 | loss: 0.24499 - acc: 0.9188 -- iter: 00832/22500
Training Step: 1422  | total loss: [1m[32m0.23717[0m[0m | time: 5.284s
| Adam | epoch: 005 | loss: 0.23717 - acc: 0.9207 -- iter: 00896/22500
Training Step: 1423  | total loss: [1m[32m0.23782[0m[0m | time: 5.634s
| Adam | epoch: 005 | loss: 0.23782 - acc: 0.9177 -- iter: 00960/22500
Training Step: 1424  | total loss: [1m[32m0.22781[0m[0m | time: 5.982s
| Adam | epoch: 005 | loss: 0.22781 - acc: 0.9228 -- iter: 01024/22500
Training Step: 1425  | total loss: [1m[32m0.23015[0m[0m | time: 6.336s
| Adam | epoch: 005 | loss: 0.23015 - acc: 0.9227 -- iter: 01088/22500
Training Step: 1426  | total loss: [1m[32m0.23189[0m[0m | time: 6.679s
| Adam | epoch: 005 | loss: 0.23189 - acc: 0.9242 -- iter: 01152/22500
Training Step: 1427  | total loss: [1m[32m0.23291[0m[0m | time: 7.027s
| Adam | epoch: 005 | loss: 0.23291 - acc: 0.9239 -- iter: 01216/22500
Training Step: 1428  | total loss: [1m[32m0.23079[0m[0m | time: 7.378s
| Adam | epoch: 005 | loss: 0.23079 - acc: 0.9237 -- iter: 01280/22500
Training Step: 1429  | total loss: [1m[32m0.23556[0m[0m | time: 7.742s
| Adam | epoch: 005 | loss: 0.23556 - acc: 0.9204 -- iter: 01344/22500
Training Step: 1430  | total loss: [1m[32m0.23438[0m[0m | time: 8.110s
| Adam | epoch: 005 | loss: 0.23438 - acc: 0.9206 -- iter: 01408/22500
Training Step: 1431  | total loss: [1m[32m0.22341[0m[0m | time: 8.465s
| Adam | epoch: 005 | loss: 0.22341 - acc: 0.9238 -- iter: 01472/22500
Training Step: 1432  | total loss: [1m[32m0.21456[0m[0m | time: 8.814s
| Adam | epoch: 005 | loss: 0.21456 - acc: 0.9268 -- iter: 01536/22500
Training Step: 1433  | total loss: [1m[32m0.20988[0m[0m | time: 9.162s
| Adam | epoch: 005 | loss: 0.20988 - acc: 0.9263 -- iter: 01600/22500
Training Step: 1434  | total loss: [1m[32m0.21096[0m[0m | time: 9.516s
| Adam | epoch: 005 | loss: 0.21096 - acc: 0.9274 -- iter: 01664/22500
Training Step: 1435  | total loss: [1m[32m0.21612[0m[0m | time: 9.864s
| Adam | epoch: 005 | loss: 0.21612 - acc: 0.9253 -- iter: 01728/22500
Training Step: 1436  | total loss: [1m[32m0.22955[0m[0m | time: 10.215s
| Adam | epoch: 005 | loss: 0.22955 - acc: 0.9202 -- iter: 01792/22500
Training Step: 1437  | total loss: [1m[32m0.21932[0m[0m | time: 10.563s
| Adam | epoch: 005 | loss: 0.21932 - acc: 0.9235 -- iter: 01856/22500
Training Step: 1438  | total loss: [1m[32m0.22124[0m[0m | time: 10.913s
| Adam | epoch: 005 | loss: 0.22124 - acc: 0.9249 -- iter: 01920/22500
Training Step: 1439  | total loss: [1m[32m0.22393[0m[0m | time: 11.267s
| Adam | epoch: 005 | loss: 0.22393 - acc: 0.9231 -- iter: 01984/22500
Training Step: 1440  | total loss: [1m[32m0.23129[0m[0m | time: 11.632s
| Adam | epoch: 005 | loss: 0.23129 - acc: 0.9183 -- iter: 02048/22500
Training Step: 1441  | total loss: [1m[32m0.23064[0m[0m | time: 12.005s
| Adam | epoch: 005 | loss: 0.23064 - acc: 0.9233 -- iter: 02112/22500
Training Step: 1442  | total loss: [1m[32m0.23672[0m[0m | time: 12.357s
| Adam | epoch: 005 | loss: 0.23672 - acc: 0.9200 -- iter: 02176/22500
Training Step: 1443  | total loss: [1m[32m0.24216[0m[0m | time: 12.703s
| Adam | epoch: 005 | loss: 0.24216 - acc: 0.9171 -- iter: 02240/22500
Training Step: 1444  | total loss: [1m[32m0.24460[0m[0m | time: 13.055s
| Adam | epoch: 005 | loss: 0.24460 - acc: 0.9160 -- iter: 02304/22500
Training Step: 1445  | total loss: [1m[32m0.25139[0m[0m | time: 13.414s
| Adam | epoch: 005 | loss: 0.25139 - acc: 0.9150 -- iter: 02368/22500
Training Step: 1446  | total loss: [1m[32m0.25382[0m[0m | time: 13.766s
| Adam | epoch: 005 | loss: 0.25382 - acc: 0.9142 -- iter: 02432/22500
Training Step: 1447  | total loss: [1m[32m0.26838[0m[0m | time: 14.112s
| Adam | epoch: 005 | loss: 0.26838 - acc: 0.9071 -- iter: 02496/22500
Training Step: 1448  | total loss: [1m[32m0.26360[0m[0m | time: 14.461s
| Adam | epoch: 005 | loss: 0.26360 - acc: 0.9102 -- iter: 02560/22500
Training Step: 1449  | total loss: [1m[32m0.27009[0m[0m | time: 14.814s
| Adam | epoch: 005 | loss: 0.27009 - acc: 0.9066 -- iter: 02624/22500
Training Step: 1450  | total loss: [1m[32m0.28123[0m[0m | time: 15.163s
| Adam | epoch: 005 | loss: 0.28123 - acc: 0.9035 -- iter: 02688/22500
Training Step: 1451  | total loss: [1m[32m0.27028[0m[0m | time: 15.510s
| Adam | epoch: 005 | loss: 0.27028 - acc: 0.9100 -- iter: 02752/22500
Training Step: 1452  | total loss: [1m[32m0.28954[0m[0m | time: 15.860s
| Adam | epoch: 005 | loss: 0.28954 - acc: 0.8956 -- iter: 02816/22500
Training Step: 1453  | total loss: [1m[32m0.29320[0m[0m | time: 16.206s
| Adam | epoch: 005 | loss: 0.29320 - acc: 0.8919 -- iter: 02880/22500
Training Step: 1454  | total loss: [1m[32m0.29440[0m[0m | time: 16.557s
| Adam | epoch: 005 | loss: 0.29440 - acc: 0.8934 -- iter: 02944/22500
Training Step: 1455  | total loss: [1m[32m0.29536[0m[0m | time: 16.908s
| Adam | epoch: 005 | loss: 0.29536 - acc: 0.8947 -- iter: 03008/22500
Training Step: 1456  | total loss: [1m[32m0.28860[0m[0m | time: 17.261s
| Adam | epoch: 005 | loss: 0.28860 - acc: 0.8974 -- iter: 03072/22500
Training Step: 1457  | total loss: [1m[32m0.29543[0m[0m | time: 17.610s
| Adam | epoch: 005 | loss: 0.29543 - acc: 0.8920 -- iter: 03136/22500
Training Step: 1458  | total loss: [1m[32m0.33700[0m[0m | time: 17.959s
| Adam | epoch: 005 | loss: 0.33700 - acc: 0.8763 -- iter: 03200/22500
Training Step: 1459  | total loss: [1m[32m0.34809[0m[0m | time: 18.317s
| Adam | epoch: 005 | loss: 0.34809 - acc: 0.8730 -- iter: 03264/22500
Training Step: 1460  | total loss: [1m[32m0.35841[0m[0m | time: 18.667s
| Adam | epoch: 005 | loss: 0.35841 - acc: 0.8670 -- iter: 03328/22500
Training Step: 1461  | total loss: [1m[32m0.34397[0m[0m | time: 19.016s
| Adam | epoch: 005 | loss: 0.34397 - acc: 0.8756 -- iter: 03392/22500
Training Step: 1462  | total loss: [1m[32m0.34105[0m[0m | time: 19.368s
| Adam | epoch: 005 | loss: 0.34105 - acc: 0.8771 -- iter: 03456/22500
Training Step: 1463  | total loss: [1m[32m0.32494[0m[0m | time: 19.761s
| Adam | epoch: 005 | loss: 0.32494 - acc: 0.8831 -- iter: 03520/22500
Training Step: 1464  | total loss: [1m[32m0.31634[0m[0m | time: 20.130s
| Adam | epoch: 005 | loss: 0.31634 - acc: 0.8854 -- iter: 03584/22500
Training Step: 1465  | total loss: [1m[32m0.30621[0m[0m | time: 20.485s
| Adam | epoch: 005 | loss: 0.30621 - acc: 0.8906 -- iter: 03648/22500
Training Step: 1466  | total loss: [1m[32m0.30792[0m[0m | time: 20.838s
| Adam | epoch: 005 | loss: 0.30792 - acc: 0.8860 -- iter: 03712/22500
Training Step: 1467  | total loss: [1m[32m0.31211[0m[0m | time: 21.187s
| Adam | epoch: 005 | loss: 0.31211 - acc: 0.8849 -- iter: 03776/22500
Training Step: 1468  | total loss: [1m[32m0.30831[0m[0m | time: 21.538s
| Adam | epoch: 005 | loss: 0.30831 - acc: 0.8870 -- iter: 03840/22500
Training Step: 1469  | total loss: [1m[32m0.30341[0m[0m | time: 21.883s
| Adam | epoch: 005 | loss: 0.30341 - acc: 0.8842 -- iter: 03904/22500
Training Step: 1470  | total loss: [1m[32m0.29466[0m[0m | time: 22.232s
| Adam | epoch: 005 | loss: 0.29466 - acc: 0.8864 -- iter: 03968/22500
Training Step: 1471  | total loss: [1m[32m0.29515[0m[0m | time: 22.576s
| Adam | epoch: 005 | loss: 0.29515 - acc: 0.8884 -- iter: 04032/22500
Training Step: 1472  | total loss: [1m[32m0.28938[0m[0m | time: 22.925s
| Adam | epoch: 005 | loss: 0.28938 - acc: 0.8918 -- iter: 04096/22500
Training Step: 1473  | total loss: [1m[32m0.27955[0m[0m | time: 23.284s
| Adam | epoch: 005 | loss: 0.27955 - acc: 0.8948 -- iter: 04160/22500
Training Step: 1474  | total loss: [1m[32m0.27165[0m[0m | time: 23.651s
| Adam | epoch: 005 | loss: 0.27165 - acc: 0.8975 -- iter: 04224/22500
Training Step: 1475  | total loss: [1m[32m0.26690[0m[0m | time: 24.020s
| Adam | epoch: 005 | loss: 0.26690 - acc: 0.9015 -- iter: 04288/22500
Training Step: 1476  | total loss: [1m[32m0.25972[0m[0m | time: 24.373s
| Adam | epoch: 005 | loss: 0.25972 - acc: 0.9051 -- iter: 04352/22500
Training Step: 1477  | total loss: [1m[32m0.26540[0m[0m | time: 24.724s
| Adam | epoch: 005 | loss: 0.26540 - acc: 0.9036 -- iter: 04416/22500
Training Step: 1478  | total loss: [1m[32m0.26114[0m[0m | time: 25.074s
| Adam | epoch: 005 | loss: 0.26114 - acc: 0.9039 -- iter: 04480/22500
Training Step: 1479  | total loss: [1m[32m0.25828[0m[0m | time: 25.425s
| Adam | epoch: 005 | loss: 0.25828 - acc: 0.9057 -- iter: 04544/22500
Training Step: 1480  | total loss: [1m[32m0.24994[0m[0m | time: 25.777s
| Adam | epoch: 005 | loss: 0.24994 - acc: 0.9104 -- iter: 04608/22500
Training Step: 1481  | total loss: [1m[32m0.25774[0m[0m | time: 26.127s
| Adam | epoch: 005 | loss: 0.25774 - acc: 0.9069 -- iter: 04672/22500
Training Step: 1482  | total loss: [1m[32m0.25605[0m[0m | time: 26.482s
| Adam | epoch: 005 | loss: 0.25605 - acc: 0.9084 -- iter: 04736/22500
Training Step: 1483  | total loss: [1m[32m0.24772[0m[0m | time: 26.833s
| Adam | epoch: 005 | loss: 0.24772 - acc: 0.9113 -- iter: 04800/22500
Training Step: 1484  | total loss: [1m[32m0.25172[0m[0m | time: 27.181s
| Adam | epoch: 005 | loss: 0.25172 - acc: 0.9061 -- iter: 04864/22500
Training Step: 1485  | total loss: [1m[32m0.24191[0m[0m | time: 27.550s
| Adam | epoch: 005 | loss: 0.24191 - acc: 0.9124 -- iter: 04928/22500
Training Step: 1486  | total loss: [1m[32m0.23011[0m[0m | time: 27.917s
| Adam | epoch: 005 | loss: 0.23011 - acc: 0.9180 -- iter: 04992/22500
Training Step: 1487  | total loss: [1m[32m0.23071[0m[0m | time: 28.272s
| Adam | epoch: 005 | loss: 0.23071 - acc: 0.9184 -- iter: 05056/22500
Training Step: 1488  | total loss: [1m[32m0.23304[0m[0m | time: 28.622s
| Adam | epoch: 005 | loss: 0.23304 - acc: 0.9172 -- iter: 05120/22500
Training Step: 1489  | total loss: [1m[32m0.22691[0m[0m | time: 28.975s
| Adam | epoch: 005 | loss: 0.22691 - acc: 0.9192 -- iter: 05184/22500
Training Step: 1490  | total loss: [1m[32m0.23623[0m[0m | time: 29.324s
| Adam | epoch: 005 | loss: 0.23623 - acc: 0.9132 -- iter: 05248/22500
Training Step: 1491  | total loss: [1m[32m0.23008[0m[0m | time: 29.674s
| Adam | epoch: 005 | loss: 0.23008 - acc: 0.9188 -- iter: 05312/22500
Training Step: 1492  | total loss: [1m[32m0.23277[0m[0m | time: 30.025s
| Adam | epoch: 005 | loss: 0.23277 - acc: 0.9207 -- iter: 05376/22500
Training Step: 1493  | total loss: [1m[32m0.23833[0m[0m | time: 30.384s
| Adam | epoch: 005 | loss: 0.23833 - acc: 0.9161 -- iter: 05440/22500
Training Step: 1494  | total loss: [1m[32m0.23782[0m[0m | time: 30.740s
| Adam | epoch: 005 | loss: 0.23782 - acc: 0.9135 -- iter: 05504/22500
Training Step: 1495  | total loss: [1m[32m0.24363[0m[0m | time: 31.088s
| Adam | epoch: 005 | loss: 0.24363 - acc: 0.9128 -- iter: 05568/22500
Training Step: 1496  | total loss: [1m[32m0.23417[0m[0m | time: 31.442s
| Adam | epoch: 005 | loss: 0.23417 - acc: 0.9168 -- iter: 05632/22500
Training Step: 1497  | total loss: [1m[32m0.23810[0m[0m | time: 31.791s
| Adam | epoch: 005 | loss: 0.23810 - acc: 0.9158 -- iter: 05696/22500
Training Step: 1498  | total loss: [1m[32m0.25802[0m[0m | time: 32.140s
| Adam | epoch: 005 | loss: 0.25802 - acc: 0.9070 -- iter: 05760/22500
Training Step: 1499  | total loss: [1m[32m0.25361[0m[0m | time: 32.495s
| Adam | epoch: 005 | loss: 0.25361 - acc: 0.9054 -- iter: 05824/22500
Training Step: 1500  | total loss: [1m[32m0.25273[0m[0m | time: 32.846s
| Adam | epoch: 005 | loss: 0.25273 - acc: 0.9055 -- iter: 05888/22500
Training Step: 1501  | total loss: [1m[32m0.25274[0m[0m | time: 33.204s
| Adam | epoch: 005 | loss: 0.25274 - acc: 0.9055 -- iter: 05952/22500
Training Step: 1502  | total loss: [1m[32m0.25447[0m[0m | time: 33.552s
| Adam | epoch: 005 | loss: 0.25447 - acc: 0.9041 -- iter: 06016/22500
Training Step: 1503  | total loss: [1m[32m0.25761[0m[0m | time: 33.904s
| Adam | epoch: 005 | loss: 0.25761 - acc: 0.9027 -- iter: 06080/22500
Training Step: 1504  | total loss: [1m[32m0.26601[0m[0m | time: 34.254s
| Adam | epoch: 005 | loss: 0.26601 - acc: 0.8999 -- iter: 06144/22500
Training Step: 1505  | total loss: [1m[32m0.26819[0m[0m | time: 34.605s
| Adam | epoch: 005 | loss: 0.26819 - acc: 0.8990 -- iter: 06208/22500
Training Step: 1506  | total loss: [1m[32m0.26990[0m[0m | time: 34.957s
| Adam | epoch: 005 | loss: 0.26990 - acc: 0.8919 -- iter: 06272/22500
Training Step: 1507  | total loss: [1m[32m0.26905[0m[0m | time: 35.312s
| Adam | epoch: 005 | loss: 0.26905 - acc: 0.8934 -- iter: 06336/22500
Training Step: 1508  | total loss: [1m[32m0.27386[0m[0m | time: 35.681s
| Adam | epoch: 005 | loss: 0.27386 - acc: 0.8915 -- iter: 06400/22500
Training Step: 1509  | total loss: [1m[32m0.28868[0m[0m | time: 36.056s
| Adam | epoch: 005 | loss: 0.28868 - acc: 0.8899 -- iter: 06464/22500
Training Step: 1510  | total loss: [1m[32m0.27694[0m[0m | time: 36.408s
| Adam | epoch: 005 | loss: 0.27694 - acc: 0.8931 -- iter: 06528/22500
Training Step: 1511  | total loss: [1m[32m0.28954[0m[0m | time: 36.761s
| Adam | epoch: 005 | loss: 0.28954 - acc: 0.8834 -- iter: 06592/22500
Training Step: 1512  | total loss: [1m[32m0.29660[0m[0m | time: 37.105s
| Adam | epoch: 005 | loss: 0.29660 - acc: 0.8795 -- iter: 06656/22500
Training Step: 1513  | total loss: [1m[32m0.29141[0m[0m | time: 37.451s
| Adam | epoch: 005 | loss: 0.29141 - acc: 0.8853 -- iter: 06720/22500
Training Step: 1514  | total loss: [1m[32m0.28006[0m[0m | time: 37.804s
| Adam | epoch: 005 | loss: 0.28006 - acc: 0.8905 -- iter: 06784/22500
Training Step: 1515  | total loss: [1m[32m0.26341[0m[0m | time: 38.152s
| Adam | epoch: 005 | loss: 0.26341 - acc: 0.8952 -- iter: 06848/22500
Training Step: 1516  | total loss: [1m[32m0.25661[0m[0m | time: 38.533s
| Adam | epoch: 005 | loss: 0.25661 - acc: 0.8994 -- iter: 06912/22500
Training Step: 1517  | total loss: [1m[32m0.25240[0m[0m | time: 38.883s
| Adam | epoch: 005 | loss: 0.25240 - acc: 0.9017 -- iter: 06976/22500
Training Step: 1518  | total loss: [1m[32m0.25680[0m[0m | time: 39.245s
| Adam | epoch: 005 | loss: 0.25680 - acc: 0.8974 -- iter: 07040/22500
Training Step: 1519  | total loss: [1m[32m0.26113[0m[0m | time: 39.606s
| Adam | epoch: 005 | loss: 0.26113 - acc: 0.8968 -- iter: 07104/22500
Training Step: 1520  | total loss: [1m[32m0.26597[0m[0m | time: 39.974s
| Adam | epoch: 005 | loss: 0.26597 - acc: 0.8961 -- iter: 07168/22500
Training Step: 1521  | total loss: [1m[32m0.26399[0m[0m | time: 40.331s
| Adam | epoch: 005 | loss: 0.26399 - acc: 0.8972 -- iter: 07232/22500
Training Step: 1522  | total loss: [1m[32m0.26657[0m[0m | time: 40.684s
| Adam | epoch: 005 | loss: 0.26657 - acc: 0.8981 -- iter: 07296/22500
Training Step: 1523  | total loss: [1m[32m0.27601[0m[0m | time: 41.034s
| Adam | epoch: 005 | loss: 0.27601 - acc: 0.8926 -- iter: 07360/22500
Training Step: 1524  | total loss: [1m[32m0.27558[0m[0m | time: 41.387s
| Adam | epoch: 005 | loss: 0.27558 - acc: 0.8924 -- iter: 07424/22500
Training Step: 1525  | total loss: [1m[32m0.28255[0m[0m | time: 41.740s
| Adam | epoch: 005 | loss: 0.28255 - acc: 0.8891 -- iter: 07488/22500
Training Step: 1526  | total loss: [1m[32m0.29064[0m[0m | time: 42.067s
| Adam | epoch: 005 | loss: 0.29064 - acc: 0.8893 -- iter: 07552/22500
Training Step: 1527  | total loss: [1m[32m0.30181[0m[0m | time: 42.354s
| Adam | epoch: 005 | loss: 0.30181 - acc: 0.8863 -- iter: 07616/22500
Training Step: 1528  | total loss: [1m[32m0.29087[0m[0m | time: 42.637s
| Adam | epoch: 005 | loss: 0.29087 - acc: 0.8898 -- iter: 07680/22500
Training Step: 1529  | total loss: [1m[32m0.29917[0m[0m | time: 42.963s
| Adam | epoch: 005 | loss: 0.29917 - acc: 0.8852 -- iter: 07744/22500
Training Step: 1530  | total loss: [1m[32m0.29807[0m[0m | time: 43.266s
| Adam | epoch: 005 | loss: 0.29807 - acc: 0.8842 -- iter: 07808/22500
Training Step: 1531  | total loss: [1m[32m0.30134[0m[0m | time: 43.653s
| Adam | epoch: 005 | loss: 0.30134 - acc: 0.8817 -- iter: 07872/22500
Training Step: 1532  | total loss: [1m[32m0.29567[0m[0m | time: 44.001s
| Adam | epoch: 005 | loss: 0.29567 - acc: 0.8873 -- iter: 07936/22500
Training Step: 1533  | total loss: [1m[32m0.30636[0m[0m | time: 44.353s
| Adam | epoch: 005 | loss: 0.30636 - acc: 0.8861 -- iter: 08000/22500
Training Step: 1534  | total loss: [1m[32m0.30281[0m[0m | time: 44.707s
| Adam | epoch: 005 | loss: 0.30281 - acc: 0.8850 -- iter: 08064/22500
Training Step: 1535  | total loss: [1m[32m0.30980[0m[0m | time: 45.051s
| Adam | epoch: 005 | loss: 0.30980 - acc: 0.8824 -- iter: 08128/22500
Training Step: 1536  | total loss: [1m[32m0.29494[0m[0m | time: 45.397s
| Adam | epoch: 005 | loss: 0.29494 - acc: 0.8910 -- iter: 08192/22500
Training Step: 1537  | total loss: [1m[32m0.27940[0m[0m | time: 45.743s
| Adam | epoch: 005 | loss: 0.27940 - acc: 0.8988 -- iter: 08256/22500
Training Step: 1538  | total loss: [1m[32m0.27046[0m[0m | time: 46.090s
| Adam | epoch: 005 | loss: 0.27046 - acc: 0.9042 -- iter: 08320/22500
Training Step: 1539  | total loss: [1m[32m0.26677[0m[0m | time: 46.442s
| Adam | epoch: 005 | loss: 0.26677 - acc: 0.9044 -- iter: 08384/22500
Training Step: 1540  | total loss: [1m[32m0.28166[0m[0m | time: 46.790s
| Adam | epoch: 005 | loss: 0.28166 - acc: 0.8984 -- iter: 08448/22500
Training Step: 1541  | total loss: [1m[32m0.26771[0m[0m | time: 47.138s
| Adam | epoch: 005 | loss: 0.26771 - acc: 0.9054 -- iter: 08512/22500
Training Step: 1542  | total loss: [1m[32m0.26003[0m[0m | time: 47.507s
| Adam | epoch: 005 | loss: 0.26003 - acc: 0.9071 -- iter: 08576/22500
Training Step: 1543  | total loss: [1m[32m0.25164[0m[0m | time: 47.878s
| Adam | epoch: 005 | loss: 0.25164 - acc: 0.9101 -- iter: 08640/22500
Training Step: 1544  | total loss: [1m[32m0.25048[0m[0m | time: 48.226s
| Adam | epoch: 005 | loss: 0.25048 - acc: 0.9097 -- iter: 08704/22500
Training Step: 1545  | total loss: [1m[32m0.25067[0m[0m | time: 48.590s
| Adam | epoch: 005 | loss: 0.25067 - acc: 0.9078 -- iter: 08768/22500
Training Step: 1546  | total loss: [1m[32m0.25588[0m[0m | time: 48.945s
| Adam | epoch: 005 | loss: 0.25588 - acc: 0.9045 -- iter: 08832/22500
Training Step: 1547  | total loss: [1m[32m0.24588[0m[0m | time: 49.297s
| Adam | epoch: 005 | loss: 0.24588 - acc: 0.9125 -- iter: 08896/22500
Training Step: 1548  | total loss: [1m[32m0.24586[0m[0m | time: 49.647s
| Adam | epoch: 005 | loss: 0.24586 - acc: 0.9103 -- iter: 08960/22500
Training Step: 1549  | total loss: [1m[32m0.26422[0m[0m | time: 49.994s
| Adam | epoch: 005 | loss: 0.26422 - acc: 0.9037 -- iter: 09024/22500
Training Step: 1550  | total loss: [1m[32m0.25307[0m[0m | time: 50.346s
| Adam | epoch: 005 | loss: 0.25307 - acc: 0.9102 -- iter: 09088/22500
Training Step: 1551  | total loss: [1m[32m0.26260[0m[0m | time: 50.695s
| Adam | epoch: 005 | loss: 0.26260 - acc: 0.9067 -- iter: 09152/22500
Training Step: 1552  | total loss: [1m[32m0.27310[0m[0m | time: 51.042s
| Adam | epoch: 005 | loss: 0.27310 - acc: 0.9004 -- iter: 09216/22500
Training Step: 1553  | total loss: [1m[32m0.28465[0m[0m | time: 51.409s
| Adam | epoch: 005 | loss: 0.28465 - acc: 0.8947 -- iter: 09280/22500
Training Step: 1554  | total loss: [1m[32m0.28424[0m[0m | time: 51.778s
| Adam | epoch: 005 | loss: 0.28424 - acc: 0.8959 -- iter: 09344/22500
Training Step: 1555  | total loss: [1m[32m0.28212[0m[0m | time: 52.125s
| Adam | epoch: 005 | loss: 0.28212 - acc: 0.8985 -- iter: 09408/22500
Training Step: 1556  | total loss: [1m[32m0.27298[0m[0m | time: 52.480s
| Adam | epoch: 005 | loss: 0.27298 - acc: 0.9024 -- iter: 09472/22500
Training Step: 1557  | total loss: [1m[32m0.28677[0m[0m | time: 52.827s
| Adam | epoch: 005 | loss: 0.28677 - acc: 0.9012 -- iter: 09536/22500
Training Step: 1558  | total loss: [1m[32m0.27627[0m[0m | time: 53.182s
| Adam | epoch: 005 | loss: 0.27627 - acc: 0.9033 -- iter: 09600/22500
Training Step: 1559  | total loss: [1m[32m0.27717[0m[0m | time: 53.534s
| Adam | epoch: 005 | loss: 0.27717 - acc: 0.9004 -- iter: 09664/22500
Training Step: 1560  | total loss: [1m[32m0.26553[0m[0m | time: 53.897s
| Adam | epoch: 005 | loss: 0.26553 - acc: 0.9026 -- iter: 09728/22500
Training Step: 1561  | total loss: [1m[32m0.27358[0m[0m | time: 54.256s
| Adam | epoch: 005 | loss: 0.27358 - acc: 0.9029 -- iter: 09792/22500
Training Step: 1562  | total loss: [1m[32m0.26072[0m[0m | time: 54.607s
| Adam | epoch: 005 | loss: 0.26072 - acc: 0.9080 -- iter: 09856/22500
Training Step: 1563  | total loss: [1m[32m0.25439[0m[0m | time: 54.958s
| Adam | epoch: 005 | loss: 0.25439 - acc: 0.9109 -- iter: 09920/22500
Training Step: 1564  | total loss: [1m[32m0.25503[0m[0m | time: 55.323s
| Adam | epoch: 005 | loss: 0.25503 - acc: 0.9089 -- iter: 09984/22500
Training Step: 1565  | total loss: [1m[32m0.26125[0m[0m | time: 55.692s
| Adam | epoch: 005 | loss: 0.26125 - acc: 0.9055 -- iter: 10048/22500
Training Step: 1566  | total loss: [1m[32m0.25779[0m[0m | time: 56.043s
| Adam | epoch: 005 | loss: 0.25779 - acc: 0.9071 -- iter: 10112/22500
Training Step: 1567  | total loss: [1m[32m0.25033[0m[0m | time: 56.395s
| Adam | epoch: 005 | loss: 0.25033 - acc: 0.9102 -- iter: 10176/22500
Training Step: 1568  | total loss: [1m[32m0.24589[0m[0m | time: 56.745s
| Adam | epoch: 005 | loss: 0.24589 - acc: 0.9098 -- iter: 10240/22500
Training Step: 1569  | total loss: [1m[32m0.23596[0m[0m | time: 57.107s
| Adam | epoch: 005 | loss: 0.23596 - acc: 0.9157 -- iter: 10304/22500
Training Step: 1570  | total loss: [1m[32m0.23852[0m[0m | time: 57.471s
| Adam | epoch: 005 | loss: 0.23852 - acc: 0.9163 -- iter: 10368/22500
Training Step: 1571  | total loss: [1m[32m0.23657[0m[0m | time: 57.825s
| Adam | epoch: 005 | loss: 0.23657 - acc: 0.9184 -- iter: 10432/22500
Training Step: 1572  | total loss: [1m[32m0.23076[0m[0m | time: 58.177s
| Adam | epoch: 005 | loss: 0.23076 - acc: 0.9203 -- iter: 10496/22500
Training Step: 1573  | total loss: [1m[32m0.22984[0m[0m | time: 58.533s
| Adam | epoch: 005 | loss: 0.22984 - acc: 0.9189 -- iter: 10560/22500
Training Step: 1574  | total loss: [1m[32m0.22336[0m[0m | time: 58.880s
| Adam | epoch: 005 | loss: 0.22336 - acc: 0.9223 -- iter: 10624/22500
Training Step: 1575  | total loss: [1m[32m0.21661[0m[0m | time: 59.226s
| Adam | epoch: 005 | loss: 0.21661 - acc: 0.9223 -- iter: 10688/22500
Training Step: 1576  | total loss: [1m[32m0.21949[0m[0m | time: 59.575s
| Adam | epoch: 005 | loss: 0.21949 - acc: 0.9207 -- iter: 10752/22500
Training Step: 1577  | total loss: [1m[32m0.21991[0m[0m | time: 59.922s
| Adam | epoch: 005 | loss: 0.21991 - acc: 0.9224 -- iter: 10816/22500
Training Step: 1578  | total loss: [1m[32m0.21017[0m[0m | time: 60.275s
| Adam | epoch: 005 | loss: 0.21017 - acc: 0.9270 -- iter: 10880/22500
Training Step: 1579  | total loss: [1m[32m0.20086[0m[0m | time: 60.626s
| Adam | epoch: 005 | loss: 0.20086 - acc: 0.9327 -- iter: 10944/22500
Training Step: 1580  | total loss: [1m[32m0.21749[0m[0m | time: 60.974s
| Adam | epoch: 005 | loss: 0.21749 - acc: 0.9238 -- iter: 11008/22500
Training Step: 1581  | total loss: [1m[32m0.20788[0m[0m | time: 61.325s
| Adam | epoch: 005 | loss: 0.20788 - acc: 0.9236 -- iter: 11072/22500
Training Step: 1582  | total loss: [1m[32m0.20591[0m[0m | time: 61.679s
| Adam | epoch: 005 | loss: 0.20591 - acc: 0.9219 -- iter: 11136/22500
Training Step: 1583  | total loss: [1m[32m0.20891[0m[0m | time: 62.026s
| Adam | epoch: 005 | loss: 0.20891 - acc: 0.9219 -- iter: 11200/22500
Training Step: 1584  | total loss: [1m[32m0.20232[0m[0m | time: 62.379s
| Adam | epoch: 005 | loss: 0.20232 - acc: 0.9235 -- iter: 11264/22500
Training Step: 1585  | total loss: [1m[32m0.22573[0m[0m | time: 62.728s
| Adam | epoch: 005 | loss: 0.22573 - acc: 0.9171 -- iter: 11328/22500
Training Step: 1586  | total loss: [1m[32m0.23154[0m[0m | time: 63.075s
| Adam | epoch: 005 | loss: 0.23154 - acc: 0.9160 -- iter: 11392/22500
Training Step: 1587  | total loss: [1m[32m0.24055[0m[0m | time: 63.445s
| Adam | epoch: 005 | loss: 0.24055 - acc: 0.9103 -- iter: 11456/22500
Training Step: 1588  | total loss: [1m[32m0.24136[0m[0m | time: 63.818s
| Adam | epoch: 005 | loss: 0.24136 - acc: 0.9083 -- iter: 11520/22500
Training Step: 1589  | total loss: [1m[32m0.24121[0m[0m | time: 64.173s
| Adam | epoch: 005 | loss: 0.24121 - acc: 0.9113 -- iter: 11584/22500
Training Step: 1590  | total loss: [1m[32m0.24967[0m[0m | time: 64.524s
| Adam | epoch: 005 | loss: 0.24967 - acc: 0.9092 -- iter: 11648/22500
Training Step: 1591  | total loss: [1m[32m0.25341[0m[0m | time: 64.873s
| Adam | epoch: 005 | loss: 0.25341 - acc: 0.9073 -- iter: 11712/22500
Training Step: 1592  | total loss: [1m[32m0.24567[0m[0m | time: 65.222s
| Adam | epoch: 005 | loss: 0.24567 - acc: 0.9088 -- iter: 11776/22500
Training Step: 1593  | total loss: [1m[32m0.23102[0m[0m | time: 65.571s
| Adam | epoch: 005 | loss: 0.23102 - acc: 0.9164 -- iter: 11840/22500
Training Step: 1594  | total loss: [1m[32m0.26808[0m[0m | time: 65.923s
| Adam | epoch: 005 | loss: 0.26808 - acc: 0.9044 -- iter: 11904/22500
Training Step: 1595  | total loss: [1m[32m0.26660[0m[0m | time: 66.277s
| Adam | epoch: 005 | loss: 0.26660 - acc: 0.9046 -- iter: 11968/22500
Training Step: 1596  | total loss: [1m[32m0.26607[0m[0m | time: 66.628s
| Adam | epoch: 005 | loss: 0.26607 - acc: 0.9032 -- iter: 12032/22500
Training Step: 1597  | total loss: [1m[32m0.26162[0m[0m | time: 66.978s
| Adam | epoch: 005 | loss: 0.26162 - acc: 0.9019 -- iter: 12096/22500
Training Step: 1598  | total loss: [1m[32m0.25017[0m[0m | time: 67.345s
| Adam | epoch: 005 | loss: 0.25017 - acc: 0.9055 -- iter: 12160/22500
Training Step: 1599  | total loss: [1m[32m0.24895[0m[0m | time: 67.715s
| Adam | epoch: 005 | loss: 0.24895 - acc: 0.9040 -- iter: 12224/22500
Training Step: 1600  | total loss: [1m[32m0.24050[0m[0m | time: 71.257s
| Adam | epoch: 005 | loss: 0.24050 - acc: 0.9058 | val_loss: 0.46782 - val_acc: 0.8124 -- iter: 12288/22500
--
Training Step: 1601  | total loss: [1m[32m0.24206[0m[0m | time: 71.619s
| Adam | epoch: 005 | loss: 0.24206 - acc: 0.9074 -- iter: 12352/22500
Training Step: 1602  | total loss: [1m[32m0.23836[0m[0m | time: 71.979s
| Adam | epoch: 005 | loss: 0.23836 - acc: 0.9088 -- iter: 12416/22500
Training Step: 1603  | total loss: [1m[32m0.25976[0m[0m | time: 72.330s
| Adam | epoch: 005 | loss: 0.25976 - acc: 0.8992 -- iter: 12480/22500
Training Step: 1604  | total loss: [1m[32m0.26001[0m[0m | time: 72.683s
| Adam | epoch: 005 | loss: 0.26001 - acc: 0.9030 -- iter: 12544/22500
Training Step: 1605  | total loss: [1m[32m0.25838[0m[0m | time: 73.033s
| Adam | epoch: 005 | loss: 0.25838 - acc: 0.9018 -- iter: 12608/22500
Training Step: 1606  | total loss: [1m[32m0.25721[0m[0m | time: 73.396s
| Adam | epoch: 005 | loss: 0.25721 - acc: 0.9022 -- iter: 12672/22500
Training Step: 1607  | total loss: [1m[32m0.25085[0m[0m | time: 73.746s
| Adam | epoch: 005 | loss: 0.25085 - acc: 0.9073 -- iter: 12736/22500
Training Step: 1608  | total loss: [1m[32m0.25776[0m[0m | time: 74.092s
| Adam | epoch: 005 | loss: 0.25776 - acc: 0.9010 -- iter: 12800/22500
Training Step: 1609  | total loss: [1m[32m0.25512[0m[0m | time: 74.440s
| Adam | epoch: 005 | loss: 0.25512 - acc: 0.9031 -- iter: 12864/22500
Training Step: 1610  | total loss: [1m[32m0.25654[0m[0m | time: 74.812s
| Adam | epoch: 005 | loss: 0.25654 - acc: 0.9018 -- iter: 12928/22500
Training Step: 1611  | total loss: [1m[32m0.24804[0m[0m | time: 75.183s
| Adam | epoch: 005 | loss: 0.24804 - acc: 0.9070 -- iter: 12992/22500
Training Step: 1612  | total loss: [1m[32m0.25323[0m[0m | time: 75.537s
| Adam | epoch: 005 | loss: 0.25323 - acc: 0.9053 -- iter: 13056/22500
Training Step: 1613  | total loss: [1m[32m0.26537[0m[0m | time: 75.888s
| Adam | epoch: 005 | loss: 0.26537 - acc: 0.9007 -- iter: 13120/22500
Training Step: 1614  | total loss: [1m[32m0.27034[0m[0m | time: 76.239s
| Adam | epoch: 005 | loss: 0.27034 - acc: 0.8950 -- iter: 13184/22500
Training Step: 1615  | total loss: [1m[32m0.27572[0m[0m | time: 76.593s
| Adam | epoch: 005 | loss: 0.27572 - acc: 0.8946 -- iter: 13248/22500
Training Step: 1616  | total loss: [1m[32m0.27527[0m[0m | time: 76.946s
| Adam | epoch: 005 | loss: 0.27527 - acc: 0.8973 -- iter: 13312/22500
Training Step: 1617  | total loss: [1m[32m0.30984[0m[0m | time: 77.298s
| Adam | epoch: 005 | loss: 0.30984 - acc: 0.8857 -- iter: 13376/22500
Training Step: 1618  | total loss: [1m[32m0.31105[0m[0m | time: 77.654s
| Adam | epoch: 005 | loss: 0.31105 - acc: 0.8831 -- iter: 13440/22500
Training Step: 1619  | total loss: [1m[32m0.30837[0m[0m | time: 78.009s
| Adam | epoch: 005 | loss: 0.30837 - acc: 0.8854 -- iter: 13504/22500
Training Step: 1620  | total loss: [1m[32m0.31346[0m[0m | time: 78.368s
| Adam | epoch: 005 | loss: 0.31346 - acc: 0.8812 -- iter: 13568/22500
Training Step: 1621  | total loss: [1m[32m0.30409[0m[0m | time: 78.733s
| Adam | epoch: 005 | loss: 0.30409 - acc: 0.8853 -- iter: 13632/22500
Training Step: 1622  | total loss: [1m[32m0.30363[0m[0m | time: 79.103s
| Adam | epoch: 005 | loss: 0.30363 - acc: 0.8874 -- iter: 13696/22500
Training Step: 1623  | total loss: [1m[32m0.29250[0m[0m | time: 79.476s
| Adam | epoch: 005 | loss: 0.29250 - acc: 0.8940 -- iter: 13760/22500
Training Step: 1624  | total loss: [1m[32m0.29391[0m[0m | time: 79.835s
| Adam | epoch: 005 | loss: 0.29391 - acc: 0.8952 -- iter: 13824/22500
Training Step: 1625  | total loss: [1m[32m0.28540[0m[0m | time: 80.192s
| Adam | epoch: 005 | loss: 0.28540 - acc: 0.8979 -- iter: 13888/22500
Training Step: 1626  | total loss: [1m[32m0.28631[0m[0m | time: 80.546s
| Adam | epoch: 005 | loss: 0.28631 - acc: 0.8909 -- iter: 13952/22500
Training Step: 1627  | total loss: [1m[32m0.30313[0m[0m | time: 80.895s
| Adam | epoch: 005 | loss: 0.30313 - acc: 0.8830 -- iter: 14016/22500
Training Step: 1628  | total loss: [1m[32m0.29443[0m[0m | time: 81.280s
| Adam | epoch: 005 | loss: 0.29443 - acc: 0.8885 -- iter: 14080/22500
Training Step: 1629  | total loss: [1m[32m0.28953[0m[0m | time: 81.624s
| Adam | epoch: 005 | loss: 0.28953 - acc: 0.8903 -- iter: 14144/22500
Training Step: 1630  | total loss: [1m[32m0.28819[0m[0m | time: 81.973s
| Adam | epoch: 005 | loss: 0.28819 - acc: 0.8887 -- iter: 14208/22500
Training Step: 1631  | total loss: [1m[32m0.28002[0m[0m | time: 82.325s
| Adam | epoch: 005 | loss: 0.28002 - acc: 0.8936 -- iter: 14272/22500
Training Step: 1632  | total loss: [1m[32m0.28451[0m[0m | time: 82.687s
| Adam | epoch: 005 | loss: 0.28451 - acc: 0.8949 -- iter: 14336/22500
Training Step: 1633  | total loss: [1m[32m0.28510[0m[0m | time: 83.057s
| Adam | epoch: 005 | loss: 0.28510 - acc: 0.8913 -- iter: 14400/22500
Training Step: 1634  | total loss: [1m[32m0.28171[0m[0m | time: 83.446s
| Adam | epoch: 005 | loss: 0.28171 - acc: 0.8959 -- iter: 14464/22500
Training Step: 1635  | total loss: [1m[32m0.27320[0m[0m | time: 83.796s
| Adam | epoch: 005 | loss: 0.27320 - acc: 0.8985 -- iter: 14528/22500
Training Step: 1636  | total loss: [1m[32m0.26362[0m[0m | time: 84.142s
| Adam | epoch: 005 | loss: 0.26362 - acc: 0.9040 -- iter: 14592/22500
Training Step: 1637  | total loss: [1m[32m0.26142[0m[0m | time: 84.640s
| Adam | epoch: 005 | loss: 0.26142 - acc: 0.9073 -- iter: 14656/22500
Training Step: 1638  | total loss: [1m[32m0.26122[0m[0m | time: 85.092s
| Adam | epoch: 005 | loss: 0.26122 - acc: 0.9026 -- iter: 14720/22500
Training Step: 1639  | total loss: [1m[32m0.25955[0m[0m | time: 85.610s
| Adam | epoch: 005 | loss: 0.25955 - acc: 0.9014 -- iter: 14784/22500
Training Step: 1640  | total loss: [1m[32m0.26800[0m[0m | time: 86.074s
| Adam | epoch: 005 | loss: 0.26800 - acc: 0.8987 -- iter: 14848/22500
Training Step: 1641  | total loss: [1m[32m0.26906[0m[0m | time: 86.573s
| Adam | epoch: 005 | loss: 0.26906 - acc: 0.8932 -- iter: 14912/22500
Training Step: 1642  | total loss: [1m[32m0.28428[0m[0m | time: 86.988s
| Adam | epoch: 005 | loss: 0.28428 - acc: 0.8867 -- iter: 14976/22500
Training Step: 1643  | total loss: [1m[32m0.28111[0m[0m | time: 87.432s
| Adam | epoch: 005 | loss: 0.28111 - acc: 0.8855 -- iter: 15040/22500
Training Step: 1644  | total loss: [1m[32m0.27933[0m[0m | time: 87.757s
| Adam | epoch: 005 | loss: 0.27933 - acc: 0.8892 -- iter: 15104/22500
Training Step: 1645  | total loss: [1m[32m0.29425[0m[0m | time: 88.106s
| Adam | epoch: 005 | loss: 0.29425 - acc: 0.8799 -- iter: 15168/22500
Training Step: 1646  | total loss: [1m[32m0.30499[0m[0m | time: 88.558s
| Adam | epoch: 005 | loss: 0.30499 - acc: 0.8795 -- iter: 15232/22500
Training Step: 1647  | total loss: [1m[32m0.30788[0m[0m | time: 88.952s
| Adam | epoch: 005 | loss: 0.30788 - acc: 0.8790 -- iter: 15296/22500
Training Step: 1648  | total loss: [1m[32m0.29846[0m[0m | time: 89.344s
| Adam | epoch: 005 | loss: 0.29846 - acc: 0.8833 -- iter: 15360/22500
Training Step: 1649  | total loss: [1m[32m0.29193[0m[0m | time: 89.722s
| Adam | epoch: 005 | loss: 0.29193 - acc: 0.8840 -- iter: 15424/22500
Training Step: 1650  | total loss: [1m[32m0.27874[0m[0m | time: 90.109s
| Adam | epoch: 005 | loss: 0.27874 - acc: 0.8909 -- iter: 15488/22500
Training Step: 1651  | total loss: [1m[32m0.27301[0m[0m | time: 90.491s
| Adam | epoch: 005 | loss: 0.27301 - acc: 0.8972 -- iter: 15552/22500
Training Step: 1652  | total loss: [1m[32m0.27415[0m[0m | time: 90.840s
| Adam | epoch: 005 | loss: 0.27415 - acc: 0.8996 -- iter: 15616/22500
Training Step: 1653  | total loss: [1m[32m0.26584[0m[0m | time: 91.188s
| Adam | epoch: 005 | loss: 0.26584 - acc: 0.9019 -- iter: 15680/22500
Training Step: 1654  | total loss: [1m[32m0.25477[0m[0m | time: 91.573s
| Adam | epoch: 005 | loss: 0.25477 - acc: 0.9085 -- iter: 15744/22500
Training Step: 1655  | total loss: [1m[32m0.25385[0m[0m | time: 92.004s
| Adam | epoch: 005 | loss: 0.25385 - acc: 0.9068 -- iter: 15808/22500
Training Step: 1656  | total loss: [1m[32m0.24788[0m[0m | time: 92.484s
| Adam | epoch: 005 | loss: 0.24788 - acc: 0.9098 -- iter: 15872/22500
Training Step: 1657  | total loss: [1m[32m0.25926[0m[0m | time: 92.836s
| Adam | epoch: 005 | loss: 0.25926 - acc: 0.9063 -- iter: 15936/22500
Training Step: 1658  | total loss: [1m[32m0.27118[0m[0m | time: 93.184s
| Adam | epoch: 005 | loss: 0.27118 - acc: 0.8970 -- iter: 16000/22500
Training Step: 1659  | total loss: [1m[32m0.26109[0m[0m | time: 93.566s
| Adam | epoch: 005 | loss: 0.26109 - acc: 0.9026 -- iter: 16064/22500
Training Step: 1660  | total loss: [1m[32m0.24716[0m[0m | time: 93.985s
| Adam | epoch: 005 | loss: 0.24716 - acc: 0.9076 -- iter: 16128/22500
Training Step: 1661  | total loss: [1m[32m0.24361[0m[0m | time: 94.465s
| Adam | epoch: 005 | loss: 0.24361 - acc: 0.9106 -- iter: 16192/22500
Training Step: 1662  | total loss: [1m[32m0.24364[0m[0m | time: 94.900s
| Adam | epoch: 005 | loss: 0.24364 - acc: 0.9102 -- iter: 16256/22500
Training Step: 1663  | total loss: [1m[32m0.24259[0m[0m | time: 95.367s
| Adam | epoch: 005 | loss: 0.24259 - acc: 0.9129 -- iter: 16320/22500
Training Step: 1664  | total loss: [1m[32m0.24892[0m[0m | time: 95.827s
| Adam | epoch: 005 | loss: 0.24892 - acc: 0.9091 -- iter: 16384/22500
Training Step: 1665  | total loss: [1m[32m0.25682[0m[0m | time: 96.258s
| Adam | epoch: 005 | loss: 0.25682 - acc: 0.9057 -- iter: 16448/22500
Training Step: 1666  | total loss: [1m[32m0.26768[0m[0m | time: 96.700s
| Adam | epoch: 005 | loss: 0.26768 - acc: 0.9011 -- iter: 16512/22500
Training Step: 1667  | total loss: [1m[32m0.27824[0m[0m | time: 97.110s
| Adam | epoch: 005 | loss: 0.27824 - acc: 0.8969 -- iter: 16576/22500
Training Step: 1668  | total loss: [1m[32m0.27107[0m[0m | time: 97.459s
| Adam | epoch: 005 | loss: 0.27107 - acc: 0.8994 -- iter: 16640/22500
Training Step: 1669  | total loss: [1m[32m0.26087[0m[0m | time: 97.807s
| Adam | epoch: 005 | loss: 0.26087 - acc: 0.9016 -- iter: 16704/22500
Training Step: 1670  | total loss: [1m[32m0.26062[0m[0m | time: 98.157s
| Adam | epoch: 005 | loss: 0.26062 - acc: 0.9005 -- iter: 16768/22500
Training Step: 1671  | total loss: [1m[32m0.26237[0m[0m | time: 98.535s
| Adam | epoch: 005 | loss: 0.26237 - acc: 0.8996 -- iter: 16832/22500
Training Step: 1672  | total loss: [1m[32m0.26636[0m[0m | time: 98.919s
| Adam | epoch: 005 | loss: 0.26636 - acc: 0.8908 -- iter: 16896/22500
Training Step: 1673  | total loss: [1m[32m0.26623[0m[0m | time: 99.320s
| Adam | epoch: 005 | loss: 0.26623 - acc: 0.8877 -- iter: 16960/22500
Training Step: 1674  | total loss: [1m[32m0.26997[0m[0m | time: 99.723s
| Adam | epoch: 005 | loss: 0.26997 - acc: 0.8849 -- iter: 17024/22500
Training Step: 1675  | total loss: [1m[32m0.28090[0m[0m | time: 100.070s
| Adam | epoch: 005 | loss: 0.28090 - acc: 0.8808 -- iter: 17088/22500
Training Step: 1676  | total loss: [1m[32m0.27847[0m[0m | time: 100.432s
| Adam | epoch: 005 | loss: 0.27847 - acc: 0.8880 -- iter: 17152/22500
Training Step: 1677  | total loss: [1m[32m0.26606[0m[0m | time: 100.806s
| Adam | epoch: 005 | loss: 0.26606 - acc: 0.8961 -- iter: 17216/22500
Training Step: 1678  | total loss: [1m[32m0.26794[0m[0m | time: 101.178s
| Adam | epoch: 005 | loss: 0.26794 - acc: 0.8955 -- iter: 17280/22500
Training Step: 1679  | total loss: [1m[32m0.25817[0m[0m | time: 101.534s
| Adam | epoch: 005 | loss: 0.25817 - acc: 0.9013 -- iter: 17344/22500
Training Step: 1680  | total loss: [1m[32m0.25532[0m[0m | time: 101.887s
| Adam | epoch: 005 | loss: 0.25532 - acc: 0.9033 -- iter: 17408/22500
Training Step: 1681  | total loss: [1m[32m0.24661[0m[0m | time: 102.240s
| Adam | epoch: 005 | loss: 0.24661 - acc: 0.9036 -- iter: 17472/22500
Training Step: 1682  | total loss: [1m[32m0.25329[0m[0m | time: 102.758s
| Adam | epoch: 005 | loss: 0.25329 - acc: 0.9023 -- iter: 17536/22500
Training Step: 1683  | total loss: [1m[32m0.25550[0m[0m | time: 103.148s
| Adam | epoch: 005 | loss: 0.25550 - acc: 0.8980 -- iter: 17600/22500
Training Step: 1684  | total loss: [1m[32m0.26816[0m[0m | time: 103.624s
| Adam | epoch: 005 | loss: 0.26816 - acc: 0.8910 -- iter: 17664/22500
Training Step: 1685  | total loss: [1m[32m0.27031[0m[0m | time: 104.012s
| Adam | epoch: 005 | loss: 0.27031 - acc: 0.8879 -- iter: 17728/22500
Training Step: 1686  | total loss: [1m[32m0.26682[0m[0m | time: 104.406s
| Adam | epoch: 005 | loss: 0.26682 - acc: 0.8913 -- iter: 17792/22500
Training Step: 1687  | total loss: [1m[32m0.25922[0m[0m | time: 104.808s
| Adam | epoch: 005 | loss: 0.25922 - acc: 0.8959 -- iter: 17856/22500
Training Step: 1688  | total loss: [1m[32m0.25755[0m[0m | time: 105.196s
| Adam | epoch: 005 | loss: 0.25755 - acc: 0.8969 -- iter: 17920/22500
Training Step: 1689  | total loss: [1m[32m0.26393[0m[0m | time: 105.582s
| Adam | epoch: 005 | loss: 0.26393 - acc: 0.8916 -- iter: 17984/22500
Training Step: 1690  | total loss: [1m[32m0.25886[0m[0m | time: 105.986s
| Adam | epoch: 005 | loss: 0.25886 - acc: 0.8931 -- iter: 18048/22500
Training Step: 1691  | total loss: [1m[32m0.26533[0m[0m | time: 106.345s
| Adam | epoch: 005 | loss: 0.26533 - acc: 0.8913 -- iter: 18112/22500
Training Step: 1692  | total loss: [1m[32m0.25385[0m[0m | time: 106.691s
| Adam | epoch: 005 | loss: 0.25385 - acc: 0.8975 -- iter: 18176/22500
Training Step: 1693  | total loss: [1m[32m0.24692[0m[0m | time: 107.040s
| Adam | epoch: 005 | loss: 0.24692 - acc: 0.8999 -- iter: 18240/22500
Training Step: 1694  | total loss: [1m[32m0.25228[0m[0m | time: 107.392s
| Adam | epoch: 005 | loss: 0.25228 - acc: 0.8974 -- iter: 18304/22500
Training Step: 1695  | total loss: [1m[32m0.24482[0m[0m | time: 107.740s
| Adam | epoch: 005 | loss: 0.24482 - acc: 0.9030 -- iter: 18368/22500
Training Step: 1696  | total loss: [1m[32m0.25017[0m[0m | time: 108.121s
| Adam | epoch: 005 | loss: 0.25017 - acc: 0.9049 -- iter: 18432/22500
Training Step: 1697  | total loss: [1m[32m0.25219[0m[0m | time: 108.472s
| Adam | epoch: 005 | loss: 0.25219 - acc: 0.9003 -- iter: 18496/22500
Training Step: 1698  | total loss: [1m[32m0.24721[0m[0m | time: 108.860s
| Adam | epoch: 005 | loss: 0.24721 - acc: 0.9025 -- iter: 18560/22500
Training Step: 1699  | total loss: [1m[32m0.24299[0m[0m | time: 109.211s
| Adam | epoch: 005 | loss: 0.24299 - acc: 0.9044 -- iter: 18624/22500
Training Step: 1700  | total loss: [1m[32m0.24276[0m[0m | time: 109.579s
| Adam | epoch: 005 | loss: 0.24276 - acc: 0.9062 -- iter: 18688/22500
Training Step: 1701  | total loss: [1m[32m0.23884[0m[0m | time: 109.980s
| Adam | epoch: 005 | loss: 0.23884 - acc: 0.9077 -- iter: 18752/22500
Training Step: 1702  | total loss: [1m[32m0.23819[0m[0m | time: 110.330s
| Adam | epoch: 005 | loss: 0.23819 - acc: 0.9107 -- iter: 18816/22500
Training Step: 1703  | total loss: [1m[32m0.23672[0m[0m | time: 110.721s
| Adam | epoch: 005 | loss: 0.23672 - acc: 0.9134 -- iter: 18880/22500
Training Step: 1704  | total loss: [1m[32m0.23861[0m[0m | time: 111.072s
| Adam | epoch: 005 | loss: 0.23861 - acc: 0.9080 -- iter: 18944/22500
Training Step: 1705  | total loss: [1m[32m0.23897[0m[0m | time: 111.423s
| Adam | epoch: 005 | loss: 0.23897 - acc: 0.9109 -- iter: 19008/22500
Training Step: 1706  | total loss: [1m[32m0.24674[0m[0m | time: 111.785s
| Adam | epoch: 005 | loss: 0.24674 - acc: 0.9073 -- iter: 19072/22500
Training Step: 1707  | total loss: [1m[32m0.24936[0m[0m | time: 112.130s
| Adam | epoch: 005 | loss: 0.24936 - acc: 0.9072 -- iter: 19136/22500
Training Step: 1708  | total loss: [1m[32m0.26094[0m[0m | time: 112.481s
| Adam | epoch: 005 | loss: 0.26094 - acc: 0.9009 -- iter: 19200/22500
Training Step: 1709  | total loss: [1m[32m0.25816[0m[0m | time: 112.833s
| Adam | epoch: 005 | loss: 0.25816 - acc: 0.9045 -- iter: 19264/22500
Training Step: 1710  | total loss: [1m[32m0.25737[0m[0m | time: 113.217s
| Adam | epoch: 005 | loss: 0.25737 - acc: 0.9032 -- iter: 19328/22500
Training Step: 1711  | total loss: [1m[32m0.25747[0m[0m | time: 113.593s
| Adam | epoch: 005 | loss: 0.25747 - acc: 0.9019 -- iter: 19392/22500
Training Step: 1712  | total loss: [1m[32m0.24735[0m[0m | time: 114.024s
| Adam | epoch: 005 | loss: 0.24735 - acc: 0.9055 -- iter: 19456/22500
Training Step: 1713  | total loss: [1m[32m0.23938[0m[0m | time: 114.349s
| Adam | epoch: 005 | loss: 0.23938 - acc: 0.9087 -- iter: 19520/22500
Training Step: 1714  | total loss: [1m[32m0.25755[0m[0m | time: 114.700s
| Adam | epoch: 005 | loss: 0.25755 - acc: 0.9022 -- iter: 19584/22500
Training Step: 1715  | total loss: [1m[32m0.26120[0m[0m | time: 115.070s
| Adam | epoch: 005 | loss: 0.26120 - acc: 0.8995 -- iter: 19648/22500
Training Step: 1716  | total loss: [1m[32m0.25071[0m[0m | time: 115.428s
| Adam | epoch: 005 | loss: 0.25071 - acc: 0.9017 -- iter: 19712/22500
Training Step: 1717  | total loss: [1m[32m0.25417[0m[0m | time: 115.777s
| Adam | epoch: 005 | loss: 0.25417 - acc: 0.9006 -- iter: 19776/22500
Training Step: 1718  | total loss: [1m[32m0.24008[0m[0m | time: 116.129s
| Adam | epoch: 005 | loss: 0.24008 - acc: 0.9074 -- iter: 19840/22500
Training Step: 1719  | total loss: [1m[32m0.24163[0m[0m | time: 116.525s
| Adam | epoch: 005 | loss: 0.24163 - acc: 0.9073 -- iter: 19904/22500
Training Step: 1720  | total loss: [1m[32m0.23684[0m[0m | time: 116.875s
| Adam | epoch: 005 | loss: 0.23684 - acc: 0.9087 -- iter: 19968/22500
Training Step: 1721  | total loss: [1m[32m0.23339[0m[0m | time: 117.236s
| Adam | epoch: 005 | loss: 0.23339 - acc: 0.9132 -- iter: 20032/22500
Training Step: 1722  | total loss: [1m[32m0.23051[0m[0m | time: 117.600s
| Adam | epoch: 005 | loss: 0.23051 - acc: 0.9125 -- iter: 20096/22500
Training Step: 1723  | total loss: [1m[32m0.24861[0m[0m | time: 118.013s
| Adam | epoch: 005 | loss: 0.24861 - acc: 0.9072 -- iter: 20160/22500
Training Step: 1724  | total loss: [1m[32m0.26164[0m[0m | time: 118.387s
| Adam | epoch: 005 | loss: 0.26164 - acc: 0.8977 -- iter: 20224/22500
Training Step: 1725  | total loss: [1m[32m0.25901[0m[0m | time: 118.752s
| Adam | epoch: 005 | loss: 0.25901 - acc: 0.9017 -- iter: 20288/22500
Training Step: 1726  | total loss: [1m[32m0.26048[0m[0m | time: 119.125s
| Adam | epoch: 005 | loss: 0.26048 - acc: 0.9021 -- iter: 20352/22500
Training Step: 1727  | total loss: [1m[32m0.26106[0m[0m | time: 119.552s
| Adam | epoch: 005 | loss: 0.26106 - acc: 0.9010 -- iter: 20416/22500
Training Step: 1728  | total loss: [1m[32m0.26413[0m[0m | time: 119.936s
| Adam | epoch: 005 | loss: 0.26413 - acc: 0.9015 -- iter: 20480/22500
Training Step: 1729  | total loss: [1m[32m0.26637[0m[0m | time: 120.341s
| Adam | epoch: 005 | loss: 0.26637 - acc: 0.8973 -- iter: 20544/22500
Training Step: 1730  | total loss: [1m[32m0.26172[0m[0m | time: 120.726s
| Adam | epoch: 005 | loss: 0.26172 - acc: 0.8998 -- iter: 20608/22500
Training Step: 1731  | total loss: [1m[32m0.25980[0m[0m | time: 121.109s
| Adam | epoch: 005 | loss: 0.25980 - acc: 0.9020 -- iter: 20672/22500
Training Step: 1732  | total loss: [1m[32m0.25112[0m[0m | time: 121.473s
| Adam | epoch: 005 | loss: 0.25112 - acc: 0.9055 -- iter: 20736/22500
Training Step: 1733  | total loss: [1m[32m0.26388[0m[0m | time: 121.829s
| Adam | epoch: 005 | loss: 0.26388 - acc: 0.8962 -- iter: 20800/22500
Training Step: 1734  | total loss: [1m[32m0.25980[0m[0m | time: 122.194s
| Adam | epoch: 005 | loss: 0.25980 - acc: 0.9004 -- iter: 20864/22500
Training Step: 1735  | total loss: [1m[32m0.24162[0m[0m | time: 122.563s
| Adam | epoch: 005 | loss: 0.24162 - acc: 0.9103 -- iter: 20928/22500
Training Step: 1736  | total loss: [1m[32m0.24010[0m[0m | time: 122.920s
| Adam | epoch: 005 | loss: 0.24010 - acc: 0.9099 -- iter: 20992/22500
Training Step: 1737  | total loss: [1m[32m0.23029[0m[0m | time: 123.268s
| Adam | epoch: 005 | loss: 0.23029 - acc: 0.9142 -- iter: 21056/22500
Training Step: 1738  | total loss: [1m[32m0.21979[0m[0m | time: 123.638s
| Adam | epoch: 005 | loss: 0.21979 - acc: 0.9181 -- iter: 21120/22500
Training Step: 1739  | total loss: [1m[32m0.21315[0m[0m | time: 123.990s
| Adam | epoch: 005 | loss: 0.21315 - acc: 0.9216 -- iter: 21184/22500
Training Step: 1740  | total loss: [1m[32m0.20796[0m[0m | time: 124.336s
| Adam | epoch: 005 | loss: 0.20796 - acc: 0.9232 -- iter: 21248/22500
Training Step: 1741  | total loss: [1m[32m0.20282[0m[0m | time: 124.720s
| Adam | epoch: 005 | loss: 0.20282 - acc: 0.9262 -- iter: 21312/22500
Training Step: 1742  | total loss: [1m[32m0.20559[0m[0m | time: 125.068s
| Adam | epoch: 005 | loss: 0.20559 - acc: 0.9289 -- iter: 21376/22500
Training Step: 1743  | total loss: [1m[32m0.18932[0m[0m | time: 125.414s
| Adam | epoch: 005 | loss: 0.18932 - acc: 0.9360 -- iter: 21440/22500
Training Step: 1744  | total loss: [1m[32m0.18015[0m[0m | time: 125.763s
| Adam | epoch: 005 | loss: 0.18015 - acc: 0.9393 -- iter: 21504/22500
Training Step: 1745  | total loss: [1m[32m0.18670[0m[0m | time: 126.133s
| Adam | epoch: 005 | loss: 0.18670 - acc: 0.9375 -- iter: 21568/22500
Training Step: 1746  | total loss: [1m[32m0.20026[0m[0m | time: 126.501s
| Adam | epoch: 005 | loss: 0.20026 - acc: 0.9313 -- iter: 21632/22500
Training Step: 1747  | total loss: [1m[32m0.20857[0m[0m | time: 126.851s
| Adam | epoch: 005 | loss: 0.20857 - acc: 0.9225 -- iter: 21696/22500
Training Step: 1748  | total loss: [1m[32m0.21054[0m[0m | time: 127.199s
| Adam | epoch: 005 | loss: 0.21054 - acc: 0.9209 -- iter: 21760/22500
Training Step: 1749  | total loss: [1m[32m0.21350[0m[0m | time: 127.547s
| Adam | epoch: 005 | loss: 0.21350 - acc: 0.9210 -- iter: 21824/22500
Training Step: 1750  | total loss: [1m[32m0.21093[0m[0m | time: 127.892s
| Adam | epoch: 005 | loss: 0.21093 - acc: 0.9195 -- iter: 21888/22500
Training Step: 1751  | total loss: [1m[32m0.21018[0m[0m | time: 128.237s
| Adam | epoch: 005 | loss: 0.21018 - acc: 0.9213 -- iter: 21952/22500
Training Step: 1752  | total loss: [1m[32m0.20615[0m[0m | time: 128.587s
| Adam | epoch: 005 | loss: 0.20615 - acc: 0.9229 -- iter: 22016/22500
Training Step: 1753  | total loss: [1m[32m0.20835[0m[0m | time: 128.933s
| Adam | epoch: 005 | loss: 0.20835 - acc: 0.9228 -- iter: 22080/22500
Training Step: 1754  | total loss: [1m[32m0.20577[0m[0m | time: 129.281s
| Adam | epoch: 005 | loss: 0.20577 - acc: 0.9274 -- iter: 22144/22500
Training Step: 1755  | total loss: [1m[32m0.20325[0m[0m | time: 129.668s
| Adam | epoch: 005 | loss: 0.20325 - acc: 0.9284 -- iter: 22208/22500
Training Step: 1756  | total loss: [1m[32m0.20417[0m[0m | time: 130.033s
| Adam | epoch: 005 | loss: 0.20417 - acc: 0.9278 -- iter: 22272/22500
Training Step: 1757  | total loss: [1m[32m0.19654[0m[0m | time: 130.400s
| Adam | epoch: 005 | loss: 0.19654 - acc: 0.9319 -- iter: 22336/22500
Training Step: 1758  | total loss: [1m[32m0.19346[0m[0m | time: 130.746s
| Adam | epoch: 005 | loss: 0.19346 - acc: 0.9356 -- iter: 22400/22500
Training Step: 1759  | total loss: [1m[32m0.18719[0m[0m | time: 131.095s
| Adam | epoch: 005 | loss: 0.18719 - acc: 0.9404 -- iter: 22464/22500
Training Step: 1760  | total loss: [1m[32m0.17870[0m[0m | time: 134.602s
| Adam | epoch: 005 | loss: 0.17870 - acc: 0.9433 | val_loss: 0.62255 - val_acc: 0.8056 -- iter: 22500/22500
--
Training Step: 1761  | total loss: [1m[32m0.17034[0m[0m | time: 0.355s
| Adam | epoch: 006 | loss: 0.17034 - acc: 0.9458 -- iter: 00064/22500
Training Step: 1762  | total loss: [1m[32m0.17955[0m[0m | time: 0.704s
| Adam | epoch: 006 | loss: 0.17955 - acc: 0.9403 -- iter: 00128/22500
Training Step: 1763  | total loss: [1m[32m0.19337[0m[0m | time: 1.052s
| Adam | epoch: 006 | loss: 0.19337 - acc: 0.9369 -- iter: 00192/22500
Training Step: 1764  | total loss: [1m[32m0.19441[0m[0m | time: 1.322s
| Adam | epoch: 006 | loss: 0.19441 - acc: 0.9370 -- iter: 00256/22500
Training Step: 1765  | total loss: [1m[32m0.23039[0m[0m | time: 1.598s
| Adam | epoch: 006 | loss: 0.23039 - acc: 0.9266 -- iter: 00320/22500
Training Step: 1766  | total loss: [1m[32m0.24760[0m[0m | time: 1.943s
| Adam | epoch: 006 | loss: 0.24760 - acc: 0.9200 -- iter: 00384/22500
Training Step: 1767  | total loss: [1m[32m0.24269[0m[0m | time: 2.294s
| Adam | epoch: 006 | loss: 0.24269 - acc: 0.9218 -- iter: 00448/22500
Training Step: 1768  | total loss: [1m[32m0.23496[0m[0m | time: 2.652s
| Adam | epoch: 006 | loss: 0.23496 - acc: 0.9249 -- iter: 00512/22500
Training Step: 1769  | total loss: [1m[32m0.23481[0m[0m | time: 3.022s
| Adam | epoch: 006 | loss: 0.23481 - acc: 0.9215 -- iter: 00576/22500
Training Step: 1770  | total loss: [1m[32m0.24118[0m[0m | time: 3.367s
| Adam | epoch: 006 | loss: 0.24118 - acc: 0.9153 -- iter: 00640/22500
Training Step: 1771  | total loss: [1m[32m0.23797[0m[0m | time: 3.715s
| Adam | epoch: 006 | loss: 0.23797 - acc: 0.9206 -- iter: 00704/22500
Training Step: 1772  | total loss: [1m[32m0.23558[0m[0m | time: 4.163s
| Adam | epoch: 006 | loss: 0.23558 - acc: 0.9223 -- iter: 00768/22500
Training Step: 1773  | total loss: [1m[32m0.23549[0m[0m | time: 4.562s
| Adam | epoch: 006 | loss: 0.23549 - acc: 0.9223 -- iter: 00832/22500
Training Step: 1774  | total loss: [1m[32m0.23676[0m[0m | time: 5.025s
| Adam | epoch: 006 | loss: 0.23676 - acc: 0.9207 -- iter: 00896/22500
Training Step: 1775  | total loss: [1m[32m0.23553[0m[0m | time: 5.457s
| Adam | epoch: 006 | loss: 0.23553 - acc: 0.9239 -- iter: 00960/22500
Training Step: 1776  | total loss: [1m[32m0.23936[0m[0m | time: 5.844s
| Adam | epoch: 006 | loss: 0.23936 - acc: 0.9237 -- iter: 01024/22500
Training Step: 1777  | total loss: [1m[32m0.23571[0m[0m | time: 6.397s
| Adam | epoch: 006 | loss: 0.23571 - acc: 0.9267 -- iter: 01088/22500
Training Step: 1778  | total loss: [1m[32m0.22617[0m[0m | time: 6.819s
| Adam | epoch: 006 | loss: 0.22617 - acc: 0.9293 -- iter: 01152/22500
Training Step: 1779  | total loss: [1m[32m0.23278[0m[0m | time: 7.272s
| Adam | epoch: 006 | loss: 0.23278 - acc: 0.9286 -- iter: 01216/22500
Training Step: 1780  | total loss: [1m[32m0.23680[0m[0m | time: 7.767s
| Adam | epoch: 006 | loss: 0.23680 - acc: 0.9263 -- iter: 01280/22500
Training Step: 1781  | total loss: [1m[32m0.26294[0m[0m | time: 8.230s
| Adam | epoch: 006 | loss: 0.26294 - acc: 0.9149 -- iter: 01344/22500
Training Step: 1782  | total loss: [1m[32m0.24836[0m[0m | time: 8.716s
| Adam | epoch: 006 | loss: 0.24836 - acc: 0.9188 -- iter: 01408/22500
Training Step: 1783  | total loss: [1m[32m0.23665[0m[0m | time: 9.185s
| Adam | epoch: 006 | loss: 0.23665 - acc: 0.9222 -- iter: 01472/22500
Training Step: 1784  | total loss: [1m[32m0.24032[0m[0m | time: 9.619s
| Adam | epoch: 006 | loss: 0.24032 - acc: 0.9190 -- iter: 01536/22500
Training Step: 1785  | total loss: [1m[32m0.23972[0m[0m | time: 10.111s
| Adam | epoch: 006 | loss: 0.23972 - acc: 0.9193 -- iter: 01600/22500
Training Step: 1786  | total loss: [1m[32m0.24281[0m[0m | time: 10.604s
| Adam | epoch: 006 | loss: 0.24281 - acc: 0.9165 -- iter: 01664/22500
Training Step: 1787  | total loss: [1m[32m0.24195[0m[0m | time: 11.022s
| Adam | epoch: 006 | loss: 0.24195 - acc: 0.9170 -- iter: 01728/22500
Training Step: 1788  | total loss: [1m[32m0.23891[0m[0m | time: 11.467s
| Adam | epoch: 006 | loss: 0.23891 - acc: 0.9190 -- iter: 01792/22500
Training Step: 1789  | total loss: [1m[32m0.22956[0m[0m | time: 11.922s
| Adam | epoch: 006 | loss: 0.22956 - acc: 0.9225 -- iter: 01856/22500
Training Step: 1790  | total loss: [1m[32m0.22196[0m[0m | time: 12.324s
| Adam | epoch: 006 | loss: 0.22196 - acc: 0.9271 -- iter: 01920/22500
Training Step: 1791  | total loss: [1m[32m0.22654[0m[0m | time: 12.699s
| Adam | epoch: 006 | loss: 0.22654 - acc: 0.9281 -- iter: 01984/22500
Training Step: 1792  | total loss: [1m[32m0.23212[0m[0m | time: 13.062s
| Adam | epoch: 006 | loss: 0.23212 - acc: 0.9259 -- iter: 02048/22500
Training Step: 1793  | total loss: [1m[32m0.23807[0m[0m | time: 13.414s
| Adam | epoch: 006 | loss: 0.23807 - acc: 0.9255 -- iter: 02112/22500
Training Step: 1794  | total loss: [1m[32m0.23707[0m[0m | time: 13.772s
| Adam | epoch: 006 | loss: 0.23707 - acc: 0.9283 -- iter: 02176/22500
Training Step: 1795  | total loss: [1m[32m0.23435[0m[0m | time: 14.121s
| Adam | epoch: 006 | loss: 0.23435 - acc: 0.9261 -- iter: 02240/22500
Training Step: 1796  | total loss: [1m[32m0.22969[0m[0m | time: 14.469s
| Adam | epoch: 006 | loss: 0.22969 - acc: 0.9304 -- iter: 02304/22500
Training Step: 1797  | total loss: [1m[32m0.23278[0m[0m | time: 14.817s
| Adam | epoch: 006 | loss: 0.23278 - acc: 0.9295 -- iter: 02368/22500
Training Step: 1798  | total loss: [1m[32m0.22755[0m[0m | time: 15.166s
| Adam | epoch: 006 | loss: 0.22755 - acc: 0.9303 -- iter: 02432/22500
Training Step: 1799  | total loss: [1m[32m0.22157[0m[0m | time: 15.519s
| Adam | epoch: 006 | loss: 0.22157 - acc: 0.9326 -- iter: 02496/22500
Training Step: 1800  | total loss: [1m[32m0.22046[0m[0m | time: 19.186s
| Adam | epoch: 006 | loss: 0.22046 - acc: 0.9362 | val_loss: 0.56184 - val_acc: 0.7972 -- iter: 02560/22500
--
Training Step: 1801  | total loss: [1m[32m0.22310[0m[0m | time: 19.542s
| Adam | epoch: 006 | loss: 0.22310 - acc: 0.9332 -- iter: 02624/22500
Training Step: 1802  | total loss: [1m[32m0.20584[0m[0m | time: 19.954s
| Adam | epoch: 006 | loss: 0.20584 - acc: 0.9399 -- iter: 02688/22500
Training Step: 1803  | total loss: [1m[32m0.19441[0m[0m | time: 20.344s
| Adam | epoch: 006 | loss: 0.19441 - acc: 0.9443 -- iter: 02752/22500
Training Step: 1804  | total loss: [1m[32m0.18644[0m[0m | time: 20.719s
| Adam | epoch: 006 | loss: 0.18644 - acc: 0.9483 -- iter: 02816/22500
Training Step: 1805  | total loss: [1m[32m0.20365[0m[0m | time: 21.064s
| Adam | epoch: 006 | loss: 0.20365 - acc: 0.9426 -- iter: 02880/22500
Training Step: 1806  | total loss: [1m[32m0.22730[0m[0m | time: 21.417s
| Adam | epoch: 006 | loss: 0.22730 - acc: 0.9327 -- iter: 02944/22500
Training Step: 1807  | total loss: [1m[32m0.22373[0m[0m | time: 21.768s
| Adam | epoch: 006 | loss: 0.22373 - acc: 0.9316 -- iter: 03008/22500
Training Step: 1808  | total loss: [1m[32m0.22939[0m[0m | time: 22.114s
| Adam | epoch: 006 | loss: 0.22939 - acc: 0.9291 -- iter: 03072/22500
Training Step: 1809  | total loss: [1m[32m0.22186[0m[0m | time: 22.461s
| Adam | epoch: 006 | loss: 0.22186 - acc: 0.9299 -- iter: 03136/22500
Training Step: 1810  | total loss: [1m[32m0.22932[0m[0m | time: 22.807s
| Adam | epoch: 006 | loss: 0.22932 - acc: 0.9291 -- iter: 03200/22500
Training Step: 1811  | total loss: [1m[32m0.22081[0m[0m | time: 23.190s
| Adam | epoch: 006 | loss: 0.22081 - acc: 0.9299 -- iter: 03264/22500
Training Step: 1812  | total loss: [1m[32m0.22377[0m[0m | time: 23.556s
| Adam | epoch: 006 | loss: 0.22377 - acc: 0.9291 -- iter: 03328/22500
Training Step: 1813  | total loss: [1m[32m0.21805[0m[0m | time: 23.913s
| Adam | epoch: 006 | loss: 0.21805 - acc: 0.9284 -- iter: 03392/22500
Training Step: 1814  | total loss: [1m[32m0.21386[0m[0m | time: 24.277s
| Adam | epoch: 006 | loss: 0.21386 - acc: 0.9293 -- iter: 03456/22500
Training Step: 1815  | total loss: [1m[32m0.21011[0m[0m | time: 24.657s
| Adam | epoch: 006 | loss: 0.21011 - acc: 0.9301 -- iter: 03520/22500
Training Step: 1816  | total loss: [1m[32m0.21315[0m[0m | time: 25.007s
| Adam | epoch: 006 | loss: 0.21315 - acc: 0.9293 -- iter: 03584/22500
Training Step: 1817  | total loss: [1m[32m0.20010[0m[0m | time: 25.361s
| Adam | epoch: 006 | loss: 0.20010 - acc: 0.9348 -- iter: 03648/22500
Training Step: 1818  | total loss: [1m[32m0.20388[0m[0m | time: 25.708s
| Adam | epoch: 006 | loss: 0.20388 - acc: 0.9351 -- iter: 03712/22500
Training Step: 1819  | total loss: [1m[32m0.20065[0m[0m | time: 26.057s
| Adam | epoch: 006 | loss: 0.20065 - acc: 0.9353 -- iter: 03776/22500
Training Step: 1820  | total loss: [1m[32m0.20447[0m[0m | time: 26.409s
| Adam | epoch: 006 | loss: 0.20447 - acc: 0.9293 -- iter: 03840/22500
Training Step: 1821  | total loss: [1m[32m0.19849[0m[0m | time: 26.752s
| Adam | epoch: 006 | loss: 0.19849 - acc: 0.9332 -- iter: 03904/22500
Training Step: 1822  | total loss: [1m[32m0.18906[0m[0m | time: 27.101s
| Adam | epoch: 006 | loss: 0.18906 - acc: 0.9352 -- iter: 03968/22500
Training Step: 1823  | total loss: [1m[32m0.19223[0m[0m | time: 27.452s
| Adam | epoch: 006 | loss: 0.19223 - acc: 0.9339 -- iter: 04032/22500
Training Step: 1824  | total loss: [1m[32m0.19508[0m[0m | time: 27.796s
| Adam | epoch: 006 | loss: 0.19508 - acc: 0.9311 -- iter: 04096/22500
Training Step: 1825  | total loss: [1m[32m0.20112[0m[0m | time: 28.165s
| Adam | epoch: 006 | loss: 0.20112 - acc: 0.9271 -- iter: 04160/22500
Training Step: 1826  | total loss: [1m[32m0.20132[0m[0m | time: 28.537s
| Adam | epoch: 006 | loss: 0.20132 - acc: 0.9266 -- iter: 04224/22500
Training Step: 1827  | total loss: [1m[32m0.20480[0m[0m | time: 28.885s
| Adam | epoch: 006 | loss: 0.20480 - acc: 0.9292 -- iter: 04288/22500
Training Step: 1828  | total loss: [1m[32m0.22507[0m[0m | time: 29.233s
| Adam | epoch: 006 | loss: 0.22507 - acc: 0.9222 -- iter: 04352/22500
Training Step: 1829  | total loss: [1m[32m0.21658[0m[0m | time: 29.577s
| Adam | epoch: 006 | loss: 0.21658 - acc: 0.9253 -- iter: 04416/22500
Training Step: 1830  | total loss: [1m[32m0.20670[0m[0m | time: 29.926s
| Adam | epoch: 006 | loss: 0.20670 - acc: 0.9297 -- iter: 04480/22500
Training Step: 1831  | total loss: [1m[32m0.19799[0m[0m | time: 30.272s
| Adam | epoch: 006 | loss: 0.19799 - acc: 0.9336 -- iter: 04544/22500
Training Step: 1832  | total loss: [1m[32m0.20759[0m[0m | time: 30.622s
| Adam | epoch: 006 | loss: 0.20759 - acc: 0.9308 -- iter: 04608/22500
Training Step: 1833  | total loss: [1m[32m0.21415[0m[0m | time: 30.968s
| Adam | epoch: 006 | loss: 0.21415 - acc: 0.9299 -- iter: 04672/22500
Training Step: 1834  | total loss: [1m[32m0.21339[0m[0m | time: 31.315s
| Adam | epoch: 006 | loss: 0.21339 - acc: 0.9307 -- iter: 04736/22500
Training Step: 1835  | total loss: [1m[32m0.21400[0m[0m | time: 31.664s
| Adam | epoch: 006 | loss: 0.21400 - acc: 0.9283 -- iter: 04800/22500
Training Step: 1836  | total loss: [1m[32m0.21409[0m[0m | time: 32.009s
| Adam | epoch: 006 | loss: 0.21409 - acc: 0.9245 -- iter: 04864/22500
Training Step: 1837  | total loss: [1m[32m0.21419[0m[0m | time: 32.374s
| Adam | epoch: 006 | loss: 0.21419 - acc: 0.9227 -- iter: 04928/22500
Training Step: 1838  | total loss: [1m[32m0.21611[0m[0m | time: 32.744s
| Adam | epoch: 006 | loss: 0.21611 - acc: 0.9226 -- iter: 04992/22500
Training Step: 1839  | total loss: [1m[32m0.20953[0m[0m | time: 33.090s
| Adam | epoch: 006 | loss: 0.20953 - acc: 0.9256 -- iter: 05056/22500
Training Step: 1840  | total loss: [1m[32m0.20635[0m[0m | time: 33.439s
| Adam | epoch: 006 | loss: 0.20635 - acc: 0.9268 -- iter: 05120/22500
Training Step: 1841  | total loss: [1m[32m0.20387[0m[0m | time: 33.785s
| Adam | epoch: 006 | loss: 0.20387 - acc: 0.9263 -- iter: 05184/22500
Training Step: 1842  | total loss: [1m[32m0.20807[0m[0m | time: 34.133s
| Adam | epoch: 006 | loss: 0.20807 - acc: 0.9259 -- iter: 05248/22500
Training Step: 1843  | total loss: [1m[32m0.20550[0m[0m | time: 34.502s
| Adam | epoch: 006 | loss: 0.20550 - acc: 0.9255 -- iter: 05312/22500
Training Step: 1844  | total loss: [1m[32m0.21112[0m[0m | time: 34.872s
| Adam | epoch: 006 | loss: 0.21112 - acc: 0.9267 -- iter: 05376/22500
Training Step: 1845  | total loss: [1m[32m0.21577[0m[0m | time: 35.221s
| Adam | epoch: 006 | loss: 0.21577 - acc: 0.9231 -- iter: 05440/22500
Training Step: 1846  | total loss: [1m[32m0.21640[0m[0m | time: 35.570s
| Adam | epoch: 006 | loss: 0.21640 - acc: 0.9245 -- iter: 05504/22500
Training Step: 1847  | total loss: [1m[32m0.22203[0m[0m | time: 35.922s
| Adam | epoch: 006 | loss: 0.22203 - acc: 0.9196 -- iter: 05568/22500
Training Step: 1848  | total loss: [1m[32m0.23024[0m[0m | time: 36.289s
| Adam | epoch: 006 | loss: 0.23024 - acc: 0.9167 -- iter: 05632/22500
Training Step: 1849  | total loss: [1m[32m0.22364[0m[0m | time: 36.661s
| Adam | epoch: 006 | loss: 0.22364 - acc: 0.9219 -- iter: 05696/22500
Training Step: 1850  | total loss: [1m[32m0.22002[0m[0m | time: 37.012s
| Adam | epoch: 006 | loss: 0.22002 - acc: 0.9234 -- iter: 05760/22500
Training Step: 1851  | total loss: [1m[32m0.21385[0m[0m | time: 37.363s
| Adam | epoch: 006 | loss: 0.21385 - acc: 0.9249 -- iter: 05824/22500
Training Step: 1852  | total loss: [1m[32m0.20840[0m[0m | time: 37.712s
| Adam | epoch: 006 | loss: 0.20840 - acc: 0.9277 -- iter: 05888/22500
Training Step: 1853  | total loss: [1m[32m0.19944[0m[0m | time: 38.060s
| Adam | epoch: 006 | loss: 0.19944 - acc: 0.9302 -- iter: 05952/22500
Training Step: 1854  | total loss: [1m[32m0.20101[0m[0m | time: 38.407s
| Adam | epoch: 006 | loss: 0.20101 - acc: 0.9278 -- iter: 06016/22500
Training Step: 1855  | total loss: [1m[32m0.20530[0m[0m | time: 38.754s
| Adam | epoch: 006 | loss: 0.20530 - acc: 0.9272 -- iter: 06080/22500
Training Step: 1856  | total loss: [1m[32m0.19953[0m[0m | time: 39.106s
| Adam | epoch: 006 | loss: 0.19953 - acc: 0.9267 -- iter: 06144/22500
Training Step: 1857  | total loss: [1m[32m0.20128[0m[0m | time: 39.454s
| Adam | epoch: 006 | loss: 0.20128 - acc: 0.9247 -- iter: 06208/22500
Training Step: 1858  | total loss: [1m[32m0.19684[0m[0m | time: 39.802s
| Adam | epoch: 006 | loss: 0.19684 - acc: 0.9259 -- iter: 06272/22500
Training Step: 1859  | total loss: [1m[32m0.18426[0m[0m | time: 40.166s
| Adam | epoch: 006 | loss: 0.18426 - acc: 0.9318 -- iter: 06336/22500
Training Step: 1860  | total loss: [1m[32m0.17938[0m[0m | time: 40.528s
| Adam | epoch: 006 | loss: 0.17938 - acc: 0.9355 -- iter: 06400/22500
Training Step: 1861  | total loss: [1m[32m0.18494[0m[0m | time: 40.878s
| Adam | epoch: 006 | loss: 0.18494 - acc: 0.9357 -- iter: 06464/22500
Training Step: 1862  | total loss: [1m[32m0.17985[0m[0m | time: 41.229s
| Adam | epoch: 006 | loss: 0.17985 - acc: 0.9390 -- iter: 06528/22500
Training Step: 1863  | total loss: [1m[32m0.17202[0m[0m | time: 41.570s
| Adam | epoch: 006 | loss: 0.17202 - acc: 0.9435 -- iter: 06592/22500
Training Step: 1864  | total loss: [1m[32m0.17924[0m[0m | time: 41.916s
| Adam | epoch: 006 | loss: 0.17924 - acc: 0.9382 -- iter: 06656/22500
Training Step: 1865  | total loss: [1m[32m0.17113[0m[0m | time: 42.262s
| Adam | epoch: 006 | loss: 0.17113 - acc: 0.9428 -- iter: 06720/22500
Training Step: 1866  | total loss: [1m[32m0.17773[0m[0m | time: 42.610s
| Adam | epoch: 006 | loss: 0.17773 - acc: 0.9408 -- iter: 06784/22500
Training Step: 1867  | total loss: [1m[32m0.17466[0m[0m | time: 42.958s
| Adam | epoch: 006 | loss: 0.17466 - acc: 0.9420 -- iter: 06848/22500
Training Step: 1868  | total loss: [1m[32m0.16676[0m[0m | time: 43.309s
| Adam | epoch: 006 | loss: 0.16676 - acc: 0.9447 -- iter: 06912/22500
Training Step: 1869  | total loss: [1m[32m0.17606[0m[0m | time: 43.667s
| Adam | epoch: 006 | loss: 0.17606 - acc: 0.9393 -- iter: 06976/22500
Training Step: 1870  | total loss: [1m[32m0.18406[0m[0m | time: 44.033s
| Adam | epoch: 006 | loss: 0.18406 - acc: 0.9328 -- iter: 07040/22500
Training Step: 1871  | total loss: [1m[32m0.18891[0m[0m | time: 44.401s
| Adam | epoch: 006 | loss: 0.18891 - acc: 0.9333 -- iter: 07104/22500
Training Step: 1872  | total loss: [1m[32m0.18245[0m[0m | time: 44.757s
| Adam | epoch: 006 | loss: 0.18245 - acc: 0.9353 -- iter: 07168/22500
Training Step: 1873  | total loss: [1m[32m0.20346[0m[0m | time: 45.103s
| Adam | epoch: 006 | loss: 0.20346 - acc: 0.9308 -- iter: 07232/22500
Training Step: 1874  | total loss: [1m[32m0.20548[0m[0m | time: 45.452s
| Adam | epoch: 006 | loss: 0.20548 - acc: 0.9330 -- iter: 07296/22500
Training Step: 1875  | total loss: [1m[32m0.20346[0m[0m | time: 45.801s
| Adam | epoch: 006 | loss: 0.20346 - acc: 0.9319 -- iter: 07360/22500
Training Step: 1876  | total loss: [1m[32m0.20345[0m[0m | time: 46.148s
| Adam | epoch: 006 | loss: 0.20345 - acc: 0.9309 -- iter: 07424/22500
Training Step: 1877  | total loss: [1m[32m0.20096[0m[0m | time: 46.495s
| Adam | epoch: 006 | loss: 0.20096 - acc: 0.9331 -- iter: 07488/22500
Training Step: 1878  | total loss: [1m[32m0.20537[0m[0m | time: 46.845s
| Adam | epoch: 006 | loss: 0.20537 - acc: 0.9289 -- iter: 07552/22500
Training Step: 1879  | total loss: [1m[32m0.20096[0m[0m | time: 47.198s
| Adam | epoch: 006 | loss: 0.20096 - acc: 0.9298 -- iter: 07616/22500
Training Step: 1880  | total loss: [1m[32m0.21052[0m[0m | time: 47.542s
| Adam | epoch: 006 | loss: 0.21052 - acc: 0.9243 -- iter: 07680/22500
Training Step: 1881  | total loss: [1m[32m0.20751[0m[0m | time: 47.894s
| Adam | epoch: 006 | loss: 0.20751 - acc: 0.9225 -- iter: 07744/22500
Training Step: 1882  | total loss: [1m[32m0.19892[0m[0m | time: 48.259s
| Adam | epoch: 006 | loss: 0.19892 - acc: 0.9287 -- iter: 07808/22500
Training Step: 1883  | total loss: [1m[32m0.20390[0m[0m | time: 48.624s
| Adam | epoch: 006 | loss: 0.20390 - acc: 0.9233 -- iter: 07872/22500
Training Step: 1884  | total loss: [1m[32m0.20550[0m[0m | time: 48.975s
| Adam | epoch: 006 | loss: 0.20550 - acc: 0.9200 -- iter: 07936/22500
Training Step: 1885  | total loss: [1m[32m0.20664[0m[0m | time: 49.326s
| Adam | epoch: 006 | loss: 0.20664 - acc: 0.9218 -- iter: 08000/22500
Training Step: 1886  | total loss: [1m[32m0.21703[0m[0m | time: 49.673s
| Adam | epoch: 006 | loss: 0.21703 - acc: 0.9155 -- iter: 08064/22500
Training Step: 1887  | total loss: [1m[32m0.21244[0m[0m | time: 50.023s
| Adam | epoch: 006 | loss: 0.21244 - acc: 0.9177 -- iter: 08128/22500
Training Step: 1888  | total loss: [1m[32m0.20770[0m[0m | time: 50.377s
| Adam | epoch: 006 | loss: 0.20770 - acc: 0.9213 -- iter: 08192/22500
Training Step: 1889  | total loss: [1m[32m0.22160[0m[0m | time: 50.722s
| Adam | epoch: 006 | loss: 0.22160 - acc: 0.9166 -- iter: 08256/22500
Training Step: 1890  | total loss: [1m[32m0.21703[0m[0m | time: 51.072s
| Adam | epoch: 006 | loss: 0.21703 - acc: 0.9187 -- iter: 08320/22500
Training Step: 1891  | total loss: [1m[32m0.21128[0m[0m | time: 51.420s
| Adam | epoch: 006 | loss: 0.21128 - acc: 0.9222 -- iter: 08384/22500
Training Step: 1892  | total loss: [1m[32m0.20996[0m[0m | time: 51.764s
| Adam | epoch: 006 | loss: 0.20996 - acc: 0.9221 -- iter: 08448/22500
Training Step: 1893  | total loss: [1m[32m0.20546[0m[0m | time: 52.135s
| Adam | epoch: 006 | loss: 0.20546 - acc: 0.9252 -- iter: 08512/22500
Training Step: 1894  | total loss: [1m[32m0.19696[0m[0m | time: 52.508s
| Adam | epoch: 006 | loss: 0.19696 - acc: 0.9296 -- iter: 08576/22500
Training Step: 1895  | total loss: [1m[32m0.20464[0m[0m | time: 52.855s
| Adam | epoch: 006 | loss: 0.20464 - acc: 0.9273 -- iter: 08640/22500
Training Step: 1896  | total loss: [1m[32m0.19797[0m[0m | time: 53.205s
| Adam | epoch: 006 | loss: 0.19797 - acc: 0.9298 -- iter: 08704/22500
Training Step: 1897  | total loss: [1m[32m0.19047[0m[0m | time: 53.553s
| Adam | epoch: 006 | loss: 0.19047 - acc: 0.9337 -- iter: 08768/22500
Training Step: 1898  | total loss: [1m[32m0.19435[0m[0m | time: 53.904s
| Adam | epoch: 006 | loss: 0.19435 - acc: 0.9325 -- iter: 08832/22500
Training Step: 1899  | total loss: [1m[32m0.19189[0m[0m | time: 54.252s
| Adam | epoch: 006 | loss: 0.19189 - acc: 0.9346 -- iter: 08896/22500
Training Step: 1900  | total loss: [1m[32m0.18623[0m[0m | time: 54.597s
| Adam | epoch: 006 | loss: 0.18623 - acc: 0.9349 -- iter: 08960/22500
Training Step: 1901  | total loss: [1m[32m0.18199[0m[0m | time: 54.945s
| Adam | epoch: 006 | loss: 0.18199 - acc: 0.9352 -- iter: 09024/22500
Training Step: 1902  | total loss: [1m[32m0.18132[0m[0m | time: 55.294s
| Adam | epoch: 006 | loss: 0.18132 - acc: 0.9338 -- iter: 09088/22500
Training Step: 1903  | total loss: [1m[32m0.17289[0m[0m | time: 55.657s
| Adam | epoch: 006 | loss: 0.17289 - acc: 0.9358 -- iter: 09152/22500
Training Step: 1904  | total loss: [1m[32m0.16953[0m[0m | time: 56.030s
| Adam | epoch: 006 | loss: 0.16953 - acc: 0.9359 -- iter: 09216/22500
Training Step: 1905  | total loss: [1m[32m0.16217[0m[0m | time: 56.400s
| Adam | epoch: 006 | loss: 0.16217 - acc: 0.9392 -- iter: 09280/22500
Training Step: 1906  | total loss: [1m[32m0.16245[0m[0m | time: 56.754s
| Adam | epoch: 006 | loss: 0.16245 - acc: 0.9390 -- iter: 09344/22500
Training Step: 1907  | total loss: [1m[32m0.16100[0m[0m | time: 57.100s
| Adam | epoch: 006 | loss: 0.16100 - acc: 0.9405 -- iter: 09408/22500
Training Step: 1908  | total loss: [1m[32m0.17679[0m[0m | time: 57.449s
| Adam | epoch: 006 | loss: 0.17679 - acc: 0.9370 -- iter: 09472/22500
Training Step: 1909  | total loss: [1m[32m0.19131[0m[0m | time: 57.802s
| Adam | epoch: 006 | loss: 0.19131 - acc: 0.9355 -- iter: 09536/22500
Training Step: 1910  | total loss: [1m[32m0.19288[0m[0m | time: 58.153s
| Adam | epoch: 006 | loss: 0.19288 - acc: 0.9373 -- iter: 09600/22500
Training Step: 1911  | total loss: [1m[32m0.19939[0m[0m | time: 58.507s
| Adam | epoch: 006 | loss: 0.19939 - acc: 0.9357 -- iter: 09664/22500
Training Step: 1912  | total loss: [1m[32m0.20012[0m[0m | time: 58.856s
| Adam | epoch: 006 | loss: 0.20012 - acc: 0.9375 -- iter: 09728/22500
Training Step: 1913  | total loss: [1m[32m0.20746[0m[0m | time: 59.203s
| Adam | epoch: 006 | loss: 0.20746 - acc: 0.9312 -- iter: 09792/22500
Training Step: 1914  | total loss: [1m[32m0.20340[0m[0m | time: 59.548s
| Adam | epoch: 006 | loss: 0.20340 - acc: 0.9319 -- iter: 09856/22500
Training Step: 1915  | total loss: [1m[32m0.19780[0m[0m | time: 59.902s
| Adam | epoch: 006 | loss: 0.19780 - acc: 0.9324 -- iter: 09920/22500
Training Step: 1916  | total loss: [1m[32m0.19966[0m[0m | time: 60.250s
| Adam | epoch: 006 | loss: 0.19966 - acc: 0.9329 -- iter: 09984/22500
Training Step: 1917  | total loss: [1m[32m0.20211[0m[0m | time: 60.596s
| Adam | epoch: 006 | loss: 0.20211 - acc: 0.9271 -- iter: 10048/22500
Training Step: 1918  | total loss: [1m[32m0.19192[0m[0m | time: 60.947s
| Adam | epoch: 006 | loss: 0.19192 - acc: 0.9329 -- iter: 10112/22500
Training Step: 1919  | total loss: [1m[32m0.19242[0m[0m | time: 61.297s
| Adam | epoch: 006 | loss: 0.19242 - acc: 0.9302 -- iter: 10176/22500
Training Step: 1920  | total loss: [1m[32m0.18920[0m[0m | time: 61.646s
| Adam | epoch: 006 | loss: 0.18920 - acc: 0.9325 -- iter: 10240/22500
Training Step: 1921  | total loss: [1m[32m0.18809[0m[0m | time: 61.995s
| Adam | epoch: 006 | loss: 0.18809 - acc: 0.9346 -- iter: 10304/22500
Training Step: 1922  | total loss: [1m[32m0.18238[0m[0m | time: 62.346s
| Adam | epoch: 006 | loss: 0.18238 - acc: 0.9364 -- iter: 10368/22500
Training Step: 1923  | total loss: [1m[32m0.18766[0m[0m | time: 62.690s
| Adam | epoch: 006 | loss: 0.18766 - acc: 0.9365 -- iter: 10432/22500
Training Step: 1924  | total loss: [1m[32m0.18554[0m[0m | time: 63.039s
| Adam | epoch: 006 | loss: 0.18554 - acc: 0.9397 -- iter: 10496/22500
Training Step: 1925  | total loss: [1m[32m0.19735[0m[0m | time: 63.391s
| Adam | epoch: 006 | loss: 0.19735 - acc: 0.9380 -- iter: 10560/22500
Training Step: 1926  | total loss: [1m[32m0.20522[0m[0m | time: 63.741s
| Adam | epoch: 006 | loss: 0.20522 - acc: 0.9348 -- iter: 10624/22500
Training Step: 1927  | total loss: [1m[32m0.20402[0m[0m | time: 64.131s
| Adam | epoch: 006 | loss: 0.20402 - acc: 0.9319 -- iter: 10688/22500
Training Step: 1928  | total loss: [1m[32m0.20347[0m[0m | time: 64.505s
| Adam | epoch: 006 | loss: 0.20347 - acc: 0.9294 -- iter: 10752/22500
Training Step: 1929  | total loss: [1m[32m0.20925[0m[0m | time: 64.853s
| Adam | epoch: 006 | loss: 0.20925 - acc: 0.9271 -- iter: 10816/22500
Training Step: 1930  | total loss: [1m[32m0.20720[0m[0m | time: 65.201s
| Adam | epoch: 006 | loss: 0.20720 - acc: 0.9297 -- iter: 10880/22500
Training Step: 1931  | total loss: [1m[32m0.20864[0m[0m | time: 65.550s
| Adam | epoch: 006 | loss: 0.20864 - acc: 0.9289 -- iter: 10944/22500
Training Step: 1932  | total loss: [1m[32m0.20050[0m[0m | time: 65.901s
| Adam | epoch: 006 | loss: 0.20050 - acc: 0.9329 -- iter: 11008/22500
Training Step: 1933  | total loss: [1m[32m0.18840[0m[0m | time: 66.252s
| Adam | epoch: 006 | loss: 0.18840 - acc: 0.9365 -- iter: 11072/22500
Training Step: 1934  | total loss: [1m[32m0.19427[0m[0m | time: 66.601s
| Adam | epoch: 006 | loss: 0.19427 - acc: 0.9350 -- iter: 11136/22500
Training Step: 1935  | total loss: [1m[32m0.20788[0m[0m | time: 66.948s
| Adam | epoch: 006 | loss: 0.20788 - acc: 0.9259 -- iter: 11200/22500
Training Step: 1936  | total loss: [1m[32m0.21210[0m[0m | time: 67.299s
| Adam | epoch: 006 | loss: 0.21210 - acc: 0.9223 -- iter: 11264/22500
Training Step: 1937  | total loss: [1m[32m0.20908[0m[0m | time: 67.646s
| Adam | epoch: 006 | loss: 0.20908 - acc: 0.9239 -- iter: 11328/22500
Training Step: 1938  | total loss: [1m[32m0.21930[0m[0m | time: 68.015s
| Adam | epoch: 006 | loss: 0.21930 - acc: 0.9221 -- iter: 11392/22500
Training Step: 1939  | total loss: [1m[32m0.22208[0m[0m | time: 68.386s
| Adam | epoch: 006 | loss: 0.22208 - acc: 0.9236 -- iter: 11456/22500
Training Step: 1940  | total loss: [1m[32m0.21501[0m[0m | time: 68.734s
| Adam | epoch: 006 | loss: 0.21501 - acc: 0.9266 -- iter: 11520/22500
Training Step: 1941  | total loss: [1m[32m0.21850[0m[0m | time: 69.091s
| Adam | epoch: 006 | loss: 0.21850 - acc: 0.9214 -- iter: 11584/22500
Training Step: 1942  | total loss: [1m[32m0.21231[0m[0m | time: 69.442s
| Adam | epoch: 006 | loss: 0.21231 - acc: 0.9230 -- iter: 11648/22500
Training Step: 1943  | total loss: [1m[32m0.21263[0m[0m | time: 69.768s
| Adam | epoch: 006 | loss: 0.21263 - acc: 0.9229 -- iter: 11712/22500
Training Step: 1944  | total loss: [1m[32m0.20874[0m[0m | time: 70.050s
| Adam | epoch: 006 | loss: 0.20874 - acc: 0.9244 -- iter: 11776/22500
Training Step: 1945  | total loss: [1m[32m0.21187[0m[0m | time: 70.336s
| Adam | epoch: 006 | loss: 0.21187 - acc: 0.9241 -- iter: 11840/22500
Training Step: 1946  | total loss: [1m[32m0.21353[0m[0m | time: 70.617s
| Adam | epoch: 006 | loss: 0.21353 - acc: 0.9223 -- iter: 11904/22500
Training Step: 1947  | total loss: [1m[32m0.20941[0m[0m | time: 70.898s
| Adam | epoch: 006 | loss: 0.20941 - acc: 0.9239 -- iter: 11968/22500
Training Step: 1948  | total loss: [1m[32m0.21325[0m[0m | time: 71.243s
| Adam | epoch: 006 | loss: 0.21325 - acc: 0.9205 -- iter: 12032/22500
Training Step: 1949  | total loss: [1m[32m0.21042[0m[0m | time: 71.609s
| Adam | epoch: 006 | loss: 0.21042 - acc: 0.9207 -- iter: 12096/22500
Training Step: 1950  | total loss: [1m[32m0.20493[0m[0m | time: 71.974s
| Adam | epoch: 006 | loss: 0.20493 - acc: 0.9239 -- iter: 12160/22500
Training Step: 1951  | total loss: [1m[32m0.19611[0m[0m | time: 72.323s
| Adam | epoch: 006 | loss: 0.19611 - acc: 0.9268 -- iter: 12224/22500
Training Step: 1952  | total loss: [1m[32m0.19524[0m[0m | time: 72.672s
| Adam | epoch: 006 | loss: 0.19524 - acc: 0.9263 -- iter: 12288/22500
Training Step: 1953  | total loss: [1m[32m0.19971[0m[0m | time: 73.021s
| Adam | epoch: 006 | loss: 0.19971 - acc: 0.9228 -- iter: 12352/22500
Training Step: 1954  | total loss: [1m[32m0.20099[0m[0m | time: 73.378s
| Adam | epoch: 006 | loss: 0.20099 - acc: 0.9242 -- iter: 12416/22500
Training Step: 1955  | total loss: [1m[32m0.21413[0m[0m | time: 73.733s
| Adam | epoch: 006 | loss: 0.21413 - acc: 0.9162 -- iter: 12480/22500
Training Step: 1956  | total loss: [1m[32m0.22788[0m[0m | time: 74.078s
| Adam | epoch: 006 | loss: 0.22788 - acc: 0.9121 -- iter: 12544/22500
Training Step: 1957  | total loss: [1m[32m0.22989[0m[0m | time: 74.433s
| Adam | epoch: 006 | loss: 0.22989 - acc: 0.9146 -- iter: 12608/22500
Training Step: 1958  | total loss: [1m[32m0.22264[0m[0m | time: 74.785s
| Adam | epoch: 006 | loss: 0.22264 - acc: 0.9200 -- iter: 12672/22500
Training Step: 1959  | total loss: [1m[32m0.22334[0m[0m | time: 75.131s
| Adam | epoch: 006 | loss: 0.22334 - acc: 0.9187 -- iter: 12736/22500
Training Step: 1960  | total loss: [1m[32m0.22924[0m[0m | time: 75.487s
| Adam | epoch: 006 | loss: 0.22924 - acc: 0.9158 -- iter: 12800/22500
Training Step: 1961  | total loss: [1m[32m0.21916[0m[0m | time: 75.837s
| Adam | epoch: 006 | loss: 0.21916 - acc: 0.9196 -- iter: 12864/22500
Training Step: 1962  | total loss: [1m[32m0.21278[0m[0m | time: 76.188s
| Adam | epoch: 006 | loss: 0.21278 - acc: 0.9198 -- iter: 12928/22500
Training Step: 1963  | total loss: [1m[32m0.20242[0m[0m | time: 76.542s
| Adam | epoch: 006 | loss: 0.20242 - acc: 0.9263 -- iter: 12992/22500
Training Step: 1964  | total loss: [1m[32m0.19266[0m[0m | time: 76.896s
| Adam | epoch: 006 | loss: 0.19266 - acc: 0.9289 -- iter: 13056/22500
Training Step: 1965  | total loss: [1m[32m0.19422[0m[0m | time: 77.243s
| Adam | epoch: 006 | loss: 0.19422 - acc: 0.9267 -- iter: 13120/22500
Training Step: 1966  | total loss: [1m[32m0.21312[0m[0m | time: 77.594s
| Adam | epoch: 006 | loss: 0.21312 - acc: 0.9231 -- iter: 13184/22500
Training Step: 1967  | total loss: [1m[32m0.23660[0m[0m | time: 77.948s
| Adam | epoch: 006 | loss: 0.23660 - acc: 0.9183 -- iter: 13248/22500
Training Step: 1968  | total loss: [1m[32m0.24120[0m[0m | time: 78.296s
| Adam | epoch: 006 | loss: 0.24120 - acc: 0.9171 -- iter: 13312/22500
Training Step: 1969  | total loss: [1m[32m0.25478[0m[0m | time: 78.642s
| Adam | epoch: 006 | loss: 0.25478 - acc: 0.9144 -- iter: 13376/22500
Training Step: 1970  | total loss: [1m[32m0.25012[0m[0m | time: 78.989s
| Adam | epoch: 006 | loss: 0.25012 - acc: 0.9167 -- iter: 13440/22500
Training Step: 1971  | total loss: [1m[32m0.25003[0m[0m | time: 79.339s
| Adam | epoch: 006 | loss: 0.25003 - acc: 0.9141 -- iter: 13504/22500
Training Step: 1972  | total loss: [1m[32m0.24497[0m[0m | time: 79.707s
| Adam | epoch: 006 | loss: 0.24497 - acc: 0.9149 -- iter: 13568/22500
Training Step: 1973  | total loss: [1m[32m0.23684[0m[0m | time: 80.078s
| Adam | epoch: 006 | loss: 0.23684 - acc: 0.9172 -- iter: 13632/22500
Training Step: 1974  | total loss: [1m[32m0.23283[0m[0m | time: 80.430s
| Adam | epoch: 006 | loss: 0.23283 - acc: 0.9176 -- iter: 13696/22500
Training Step: 1975  | total loss: [1m[32m0.23920[0m[0m | time: 80.780s
| Adam | epoch: 006 | loss: 0.23920 - acc: 0.9149 -- iter: 13760/22500
Training Step: 1976  | total loss: [1m[32m0.25118[0m[0m | time: 81.132s
| Adam | epoch: 006 | loss: 0.25118 - acc: 0.9047 -- iter: 13824/22500
Training Step: 1977  | total loss: [1m[32m0.26785[0m[0m | time: 81.511s
| Adam | epoch: 006 | loss: 0.26785 - acc: 0.8908 -- iter: 13888/22500
Training Step: 1978  | total loss: [1m[32m0.26227[0m[0m | time: 81.906s
| Adam | epoch: 006 | loss: 0.26227 - acc: 0.8908 -- iter: 13952/22500
Training Step: 1979  | total loss: [1m[32m0.26290[0m[0m | time: 82.280s
| Adam | epoch: 006 | loss: 0.26290 - acc: 0.8923 -- iter: 14016/22500
Training Step: 1980  | total loss: [1m[32m0.26150[0m[0m | time: 82.653s
| Adam | epoch: 006 | loss: 0.26150 - acc: 0.8906 -- iter: 14080/22500
Training Step: 1981  | total loss: [1m[32m0.25534[0m[0m | time: 83.025s
| Adam | epoch: 006 | loss: 0.25534 - acc: 0.8921 -- iter: 14144/22500
Training Step: 1982  | total loss: [1m[32m0.25714[0m[0m | time: 83.454s
| Adam | epoch: 006 | loss: 0.25714 - acc: 0.8936 -- iter: 14208/22500
Training Step: 1983  | total loss: [1m[32m0.24934[0m[0m | time: 83.798s
| Adam | epoch: 006 | loss: 0.24934 - acc: 0.8964 -- iter: 14272/22500
Training Step: 1984  | total loss: [1m[32m0.23419[0m[0m | time: 84.131s
| Adam | epoch: 006 | loss: 0.23419 - acc: 0.9052 -- iter: 14336/22500
Training Step: 1985  | total loss: [1m[32m0.21859[0m[0m | time: 84.442s
| Adam | epoch: 006 | loss: 0.21859 - acc: 0.9131 -- iter: 14400/22500
Training Step: 1986  | total loss: [1m[32m0.21325[0m[0m | time: 84.741s
| Adam | epoch: 006 | loss: 0.21325 - acc: 0.9155 -- iter: 14464/22500
Training Step: 1987  | total loss: [1m[32m0.20528[0m[0m | time: 85.080s
| Adam | epoch: 006 | loss: 0.20528 - acc: 0.9193 -- iter: 14528/22500
Training Step: 1988  | total loss: [1m[32m0.20659[0m[0m | time: 85.427s
| Adam | epoch: 006 | loss: 0.20659 - acc: 0.9180 -- iter: 14592/22500
Training Step: 1989  | total loss: [1m[32m0.21393[0m[0m | time: 85.786s
| Adam | epoch: 006 | loss: 0.21393 - acc: 0.9153 -- iter: 14656/22500
Training Step: 1990  | total loss: [1m[32m0.23803[0m[0m | time: 86.161s
| Adam | epoch: 006 | loss: 0.23803 - acc: 0.9081 -- iter: 14720/22500
Training Step: 1991  | total loss: [1m[32m0.22028[0m[0m | time: 86.531s
| Adam | epoch: 006 | loss: 0.22028 - acc: 0.9173 -- iter: 14784/22500
Training Step: 1992  | total loss: [1m[32m0.22239[0m[0m | time: 86.908s
| Adam | epoch: 006 | loss: 0.22239 - acc: 0.9162 -- iter: 14848/22500
Training Step: 1993  | total loss: [1m[32m0.22325[0m[0m | time: 87.287s
| Adam | epoch: 006 | loss: 0.22325 - acc: 0.9168 -- iter: 14912/22500
Training Step: 1994  | total loss: [1m[32m0.21716[0m[0m | time: 87.683s
| Adam | epoch: 006 | loss: 0.21716 - acc: 0.9173 -- iter: 14976/22500
Training Step: 1995  | total loss: [1m[32m0.21188[0m[0m | time: 88.082s
| Adam | epoch: 006 | loss: 0.21188 - acc: 0.9209 -- iter: 15040/22500
Training Step: 1996  | total loss: [1m[32m0.20688[0m[0m | time: 88.463s
| Adam | epoch: 006 | loss: 0.20688 - acc: 0.9256 -- iter: 15104/22500
Training Step: 1997  | total loss: [1m[32m0.20469[0m[0m | time: 88.843s
| Adam | epoch: 006 | loss: 0.20469 - acc: 0.9268 -- iter: 15168/22500
Training Step: 1998  | total loss: [1m[32m0.19563[0m[0m | time: 89.208s
| Adam | epoch: 006 | loss: 0.19563 - acc: 0.9326 -- iter: 15232/22500
Training Step: 1999  | total loss: [1m[32m0.21240[0m[0m | time: 89.558s
| Adam | epoch: 006 | loss: 0.21240 - acc: 0.9284 -- iter: 15296/22500
Training Step: 2000  | total loss: [1m[32m0.21486[0m[0m | time: 93.127s
| Adam | epoch: 006 | loss: 0.21486 - acc: 0.9277 | val_loss: 0.55529 - val_acc: 0.8048 -- iter: 15360/22500
--
Training Step: 2001  | total loss: [1m[32m0.21678[0m[0m | time: 93.478s
| Adam | epoch: 006 | loss: 0.21678 - acc: 0.9272 -- iter: 15424/22500
Training Step: 2002  | total loss: [1m[32m0.21340[0m[0m | time: 93.829s
| Adam | epoch: 006 | loss: 0.21340 - acc: 0.9266 -- iter: 15488/22500
Training Step: 2003  | total loss: [1m[32m0.22318[0m[0m | time: 94.177s
| Adam | epoch: 006 | loss: 0.22318 - acc: 0.9262 -- iter: 15552/22500
Training Step: 2004  | total loss: [1m[32m0.21336[0m[0m | time: 94.553s
| Adam | epoch: 006 | loss: 0.21336 - acc: 0.9288 -- iter: 15616/22500
Training Step: 2005  | total loss: [1m[32m0.20870[0m[0m | time: 94.918s
| Adam | epoch: 006 | loss: 0.20870 - acc: 0.9282 -- iter: 15680/22500
Training Step: 2006  | total loss: [1m[32m0.21850[0m[0m | time: 95.304s
| Adam | epoch: 006 | loss: 0.21850 - acc: 0.9228 -- iter: 15744/22500
Training Step: 2007  | total loss: [1m[32m0.20867[0m[0m | time: 95.672s
| Adam | epoch: 006 | loss: 0.20867 - acc: 0.9274 -- iter: 15808/22500
Training Step: 2008  | total loss: [1m[32m0.20186[0m[0m | time: 96.022s
| Adam | epoch: 006 | loss: 0.20186 - acc: 0.9284 -- iter: 15872/22500
Training Step: 2009  | total loss: [1m[32m0.20053[0m[0m | time: 96.371s
| Adam | epoch: 006 | loss: 0.20053 - acc: 0.9293 -- iter: 15936/22500
Training Step: 2010  | total loss: [1m[32m0.21341[0m[0m | time: 96.725s
| Adam | epoch: 006 | loss: 0.21341 - acc: 0.9239 -- iter: 16000/22500
Training Step: 2011  | total loss: [1m[32m0.21106[0m[0m | time: 97.077s
| Adam | epoch: 006 | loss: 0.21106 - acc: 0.9237 -- iter: 16064/22500
Training Step: 2012  | total loss: [1m[32m0.21630[0m[0m | time: 97.429s
| Adam | epoch: 006 | loss: 0.21630 - acc: 0.9220 -- iter: 16128/22500
Training Step: 2013  | total loss: [1m[32m0.22897[0m[0m | time: 97.785s
| Adam | epoch: 006 | loss: 0.22897 - acc: 0.9157 -- iter: 16192/22500
Training Step: 2014  | total loss: [1m[32m0.23600[0m[0m | time: 98.136s
| Adam | epoch: 006 | loss: 0.23600 - acc: 0.9101 -- iter: 16256/22500
Training Step: 2015  | total loss: [1m[32m0.22859[0m[0m | time: 98.486s
| Adam | epoch: 006 | loss: 0.22859 - acc: 0.9144 -- iter: 16320/22500
Training Step: 2016  | total loss: [1m[32m0.23102[0m[0m | time: 98.842s
| Adam | epoch: 006 | loss: 0.23102 - acc: 0.9167 -- iter: 16384/22500
Training Step: 2017  | total loss: [1m[32m0.22968[0m[0m | time: 99.215s
| Adam | epoch: 006 | loss: 0.22968 - acc: 0.9188 -- iter: 16448/22500
Training Step: 2018  | total loss: [1m[32m0.22075[0m[0m | time: 99.737s
| Adam | epoch: 006 | loss: 0.22075 - acc: 0.9253 -- iter: 16512/22500
Training Step: 2019  | total loss: [1m[32m0.21269[0m[0m | time: 100.055s
| Adam | epoch: 006 | loss: 0.21269 - acc: 0.9297 -- iter: 16576/22500
Training Step: 2020  | total loss: [1m[32m0.23433[0m[0m | time: 100.380s
| Adam | epoch: 006 | loss: 0.23433 - acc: 0.9226 -- iter: 16640/22500
Training Step: 2021  | total loss: [1m[32m0.24136[0m[0m | time: 100.742s
| Adam | epoch: 006 | loss: 0.24136 - acc: 0.9210 -- iter: 16704/22500
Training Step: 2022  | total loss: [1m[32m0.23914[0m[0m | time: 101.125s
| Adam | epoch: 006 | loss: 0.23914 - acc: 0.9211 -- iter: 16768/22500
Training Step: 2023  | total loss: [1m[32m0.24356[0m[0m | time: 101.487s
| Adam | epoch: 006 | loss: 0.24356 - acc: 0.9227 -- iter: 16832/22500
Training Step: 2024  | total loss: [1m[32m0.24929[0m[0m | time: 101.840s
| Adam | epoch: 006 | loss: 0.24929 - acc: 0.9195 -- iter: 16896/22500
Training Step: 2025  | total loss: [1m[32m0.24356[0m[0m | time: 102.199s
| Adam | epoch: 006 | loss: 0.24356 - acc: 0.9213 -- iter: 16960/22500
Training Step: 2026  | total loss: [1m[32m0.25754[0m[0m | time: 102.570s
| Adam | epoch: 006 | loss: 0.25754 - acc: 0.9167 -- iter: 17024/22500
Training Step: 2027  | total loss: [1m[32m0.24973[0m[0m | time: 102.932s
| Adam | epoch: 006 | loss: 0.24973 - acc: 0.9156 -- iter: 17088/22500
Training Step: 2028  | total loss: [1m[32m0.24798[0m[0m | time: 103.289s
| Adam | epoch: 006 | loss: 0.24798 - acc: 0.9147 -- iter: 17152/22500
Training Step: 2029  | total loss: [1m[32m0.23668[0m[0m | time: 103.672s
| Adam | epoch: 006 | loss: 0.23668 - acc: 0.9185 -- iter: 17216/22500
Training Step: 2030  | total loss: [1m[32m0.23177[0m[0m | time: 104.052s
| Adam | epoch: 006 | loss: 0.23177 - acc: 0.9204 -- iter: 17280/22500
Training Step: 2031  | total loss: [1m[32m0.23466[0m[0m | time: 104.402s
| Adam | epoch: 006 | loss: 0.23466 - acc: 0.9206 -- iter: 17344/22500
Training Step: 2032  | total loss: [1m[32m0.22336[0m[0m | time: 104.762s
| Adam | epoch: 006 | loss: 0.22336 - acc: 0.9254 -- iter: 17408/22500
Training Step: 2033  | total loss: [1m[32m0.22999[0m[0m | time: 105.119s
| Adam | epoch: 006 | loss: 0.22999 - acc: 0.9235 -- iter: 17472/22500
Training Step: 2034  | total loss: [1m[32m0.22811[0m[0m | time: 105.471s
| Adam | epoch: 006 | loss: 0.22811 - acc: 0.9249 -- iter: 17536/22500
Training Step: 2035  | total loss: [1m[32m0.21743[0m[0m | time: 105.834s
| Adam | epoch: 006 | loss: 0.21743 - acc: 0.9293 -- iter: 17600/22500
Training Step: 2036  | total loss: [1m[32m0.22117[0m[0m | time: 106.193s
| Adam | epoch: 006 | loss: 0.22117 - acc: 0.9285 -- iter: 17664/22500
Training Step: 2037  | total loss: [1m[32m0.22814[0m[0m | time: 106.583s
| Adam | epoch: 006 | loss: 0.22814 - acc: 0.9216 -- iter: 17728/22500
Training Step: 2038  | total loss: [1m[32m0.23836[0m[0m | time: 106.945s
| Adam | epoch: 006 | loss: 0.23836 - acc: 0.9185 -- iter: 17792/22500
Training Step: 2039  | total loss: [1m[32m0.23124[0m[0m | time: 107.297s
| Adam | epoch: 006 | loss: 0.23124 - acc: 0.9220 -- iter: 17856/22500
Training Step: 2040  | total loss: [1m[32m0.22908[0m[0m | time: 107.669s
| Adam | epoch: 006 | loss: 0.22908 - acc: 0.9220 -- iter: 17920/22500
Training Step: 2041  | total loss: [1m[32m0.23067[0m[0m | time: 108.047s
| Adam | epoch: 006 | loss: 0.23067 - acc: 0.9204 -- iter: 17984/22500
Training Step: 2042  | total loss: [1m[32m0.22428[0m[0m | time: 108.406s
| Adam | epoch: 006 | loss: 0.22428 - acc: 0.9237 -- iter: 18048/22500
Training Step: 2043  | total loss: [1m[32m0.22814[0m[0m | time: 108.755s
| Adam | epoch: 006 | loss: 0.22814 - acc: 0.9188 -- iter: 18112/22500
Training Step: 2044  | total loss: [1m[32m0.22329[0m[0m | time: 109.107s
| Adam | epoch: 006 | loss: 0.22329 - acc: 0.9191 -- iter: 18176/22500
Training Step: 2045  | total loss: [1m[32m0.22798[0m[0m | time: 109.458s
| Adam | epoch: 006 | loss: 0.22798 - acc: 0.9194 -- iter: 18240/22500
Training Step: 2046  | total loss: [1m[32m0.22847[0m[0m | time: 109.811s
| Adam | epoch: 006 | loss: 0.22847 - acc: 0.9181 -- iter: 18304/22500
Training Step: 2047  | total loss: [1m[32m0.23693[0m[0m | time: 110.160s
| Adam | epoch: 006 | loss: 0.23693 - acc: 0.9122 -- iter: 18368/22500
Training Step: 2048  | total loss: [1m[32m0.23296[0m[0m | time: 110.507s
| Adam | epoch: 006 | loss: 0.23296 - acc: 0.9116 -- iter: 18432/22500
Training Step: 2049  | total loss: [1m[32m0.23364[0m[0m | time: 110.854s
| Adam | epoch: 006 | loss: 0.23364 - acc: 0.9126 -- iter: 18496/22500
Training Step: 2050  | total loss: [1m[32m0.22645[0m[0m | time: 111.203s
| Adam | epoch: 006 | loss: 0.22645 - acc: 0.9151 -- iter: 18560/22500
Training Step: 2051  | total loss: [1m[32m0.23225[0m[0m | time: 111.569s
| Adam | epoch: 006 | loss: 0.23225 - acc: 0.9127 -- iter: 18624/22500
Training Step: 2052  | total loss: [1m[32m0.22797[0m[0m | time: 111.936s
| Adam | epoch: 006 | loss: 0.22797 - acc: 0.9152 -- iter: 18688/22500
Training Step: 2053  | total loss: [1m[32m0.23177[0m[0m | time: 112.284s
| Adam | epoch: 006 | loss: 0.23177 - acc: 0.9174 -- iter: 18752/22500
Training Step: 2054  | total loss: [1m[32m0.22982[0m[0m | time: 112.635s
| Adam | epoch: 006 | loss: 0.22982 - acc: 0.9178 -- iter: 18816/22500
Training Step: 2055  | total loss: [1m[32m0.22859[0m[0m | time: 112.982s
| Adam | epoch: 006 | loss: 0.22859 - acc: 0.9198 -- iter: 18880/22500
Training Step: 2056  | total loss: [1m[32m0.22992[0m[0m | time: 113.336s
| Adam | epoch: 006 | loss: 0.22992 - acc: 0.9153 -- iter: 18944/22500
Training Step: 2057  | total loss: [1m[32m0.22916[0m[0m | time: 113.690s
| Adam | epoch: 006 | loss: 0.22916 - acc: 0.9175 -- iter: 19008/22500
Training Step: 2058  | total loss: [1m[32m0.23197[0m[0m | time: 114.038s
| Adam | epoch: 006 | loss: 0.23197 - acc: 0.9148 -- iter: 19072/22500
Training Step: 2059  | total loss: [1m[32m0.24030[0m[0m | time: 114.385s
| Adam | epoch: 006 | loss: 0.24030 - acc: 0.9109 -- iter: 19136/22500
Training Step: 2060  | total loss: [1m[32m0.24203[0m[0m | time: 114.731s
| Adam | epoch: 006 | loss: 0.24203 - acc: 0.9104 -- iter: 19200/22500
Training Step: 2061  | total loss: [1m[32m0.23632[0m[0m | time: 115.080s
| Adam | epoch: 006 | loss: 0.23632 - acc: 0.9131 -- iter: 19264/22500
Training Step: 2062  | total loss: [1m[32m0.24301[0m[0m | time: 115.430s
| Adam | epoch: 006 | loss: 0.24301 - acc: 0.9077 -- iter: 19328/22500
Training Step: 2063  | total loss: [1m[32m0.25058[0m[0m | time: 115.778s
| Adam | epoch: 006 | loss: 0.25058 - acc: 0.9013 -- iter: 19392/22500
Training Step: 2064  | total loss: [1m[32m0.24772[0m[0m | time: 116.125s
| Adam | epoch: 006 | loss: 0.24772 - acc: 0.8987 -- iter: 19456/22500
Training Step: 2065  | total loss: [1m[32m0.24754[0m[0m | time: 116.473s
| Adam | epoch: 006 | loss: 0.24754 - acc: 0.9010 -- iter: 19520/22500
Training Step: 2066  | total loss: [1m[32m0.24758[0m[0m | time: 116.822s
| Adam | epoch: 006 | loss: 0.24758 - acc: 0.9047 -- iter: 19584/22500
Training Step: 2067  | total loss: [1m[32m0.24681[0m[0m | time: 117.169s
| Adam | epoch: 006 | loss: 0.24681 - acc: 0.9033 -- iter: 19648/22500
Training Step: 2068  | total loss: [1m[32m0.24472[0m[0m | time: 117.520s
| Adam | epoch: 006 | loss: 0.24472 - acc: 0.9020 -- iter: 19712/22500
Training Step: 2069  | total loss: [1m[32m0.23500[0m[0m | time: 117.871s
| Adam | epoch: 006 | loss: 0.23500 - acc: 0.9071 -- iter: 19776/22500
Training Step: 2070  | total loss: [1m[32m0.23680[0m[0m | time: 118.221s
| Adam | epoch: 006 | loss: 0.23680 - acc: 0.9070 -- iter: 19840/22500
Training Step: 2071  | total loss: [1m[32m0.23808[0m[0m | time: 118.573s
| Adam | epoch: 006 | loss: 0.23808 - acc: 0.9038 -- iter: 19904/22500
Training Step: 2072  | total loss: [1m[32m0.24042[0m[0m | time: 118.923s
| Adam | epoch: 006 | loss: 0.24042 - acc: 0.9041 -- iter: 19968/22500
Training Step: 2073  | total loss: [1m[32m0.24635[0m[0m | time: 119.271s
| Adam | epoch: 006 | loss: 0.24635 - acc: 0.9043 -- iter: 20032/22500
Training Step: 2074  | total loss: [1m[32m0.24671[0m[0m | time: 119.651s
| Adam | epoch: 006 | loss: 0.24671 - acc: 0.9045 -- iter: 20096/22500
Training Step: 2075  | total loss: [1m[32m0.24343[0m[0m | time: 120.018s
| Adam | epoch: 006 | loss: 0.24343 - acc: 0.9062 -- iter: 20160/22500
Training Step: 2076  | total loss: [1m[32m0.24447[0m[0m | time: 120.365s
| Adam | epoch: 006 | loss: 0.24447 - acc: 0.9062 -- iter: 20224/22500
Training Step: 2077  | total loss: [1m[32m0.23580[0m[0m | time: 120.714s
| Adam | epoch: 006 | loss: 0.23580 - acc: 0.9078 -- iter: 20288/22500
Training Step: 2078  | total loss: [1m[32m0.24127[0m[0m | time: 121.061s
| Adam | epoch: 006 | loss: 0.24127 - acc: 0.9045 -- iter: 20352/22500
Training Step: 2079  | total loss: [1m[32m0.23394[0m[0m | time: 121.412s
| Adam | epoch: 006 | loss: 0.23394 - acc: 0.9094 -- iter: 20416/22500
Training Step: 2080  | total loss: [1m[32m0.22976[0m[0m | time: 121.759s
| Adam | epoch: 006 | loss: 0.22976 - acc: 0.9106 -- iter: 20480/22500
Training Step: 2081  | total loss: [1m[32m0.21894[0m[0m | time: 122.107s
| Adam | epoch: 006 | loss: 0.21894 - acc: 0.9164 -- iter: 20544/22500
Training Step: 2082  | total loss: [1m[32m0.21184[0m[0m | time: 122.462s
| Adam | epoch: 006 | loss: 0.21184 - acc: 0.9185 -- iter: 20608/22500
Training Step: 2083  | total loss: [1m[32m0.21347[0m[0m | time: 122.808s
| Adam | epoch: 006 | loss: 0.21347 - acc: 0.9126 -- iter: 20672/22500
Training Step: 2084  | total loss: [1m[32m0.23908[0m[0m | time: 123.156s
| Adam | epoch: 006 | loss: 0.23908 - acc: 0.9057 -- iter: 20736/22500
Training Step: 2085  | total loss: [1m[32m0.23081[0m[0m | time: 123.526s
| Adam | epoch: 006 | loss: 0.23081 - acc: 0.9105 -- iter: 20800/22500
Training Step: 2086  | total loss: [1m[32m0.33552[0m[0m | time: 123.896s
| Adam | epoch: 006 | loss: 0.33552 - acc: 0.8710 -- iter: 20864/22500
Training Step: 2087  | total loss: [1m[32m0.32231[0m[0m | time: 124.242s
| Adam | epoch: 006 | loss: 0.32231 - acc: 0.8761 -- iter: 20928/22500
Training Step: 2088  | total loss: [1m[32m0.30912[0m[0m | time: 124.627s
| Adam | epoch: 006 | loss: 0.30912 - acc: 0.8853 -- iter: 20992/22500
Training Step: 2089  | total loss: [1m[32m0.29916[0m[0m | time: 124.989s
| Adam | epoch: 006 | loss: 0.29916 - acc: 0.8890 -- iter: 21056/22500
Training Step: 2090  | total loss: [1m[32m0.29303[0m[0m | time: 125.344s
| Adam | epoch: 006 | loss: 0.29303 - acc: 0.8923 -- iter: 21120/22500
Training Step: 2091  | total loss: [1m[32m0.28161[0m[0m | time: 125.707s
| Adam | epoch: 006 | loss: 0.28161 - acc: 0.8999 -- iter: 21184/22500
Training Step: 2092  | total loss: [1m[32m0.27805[0m[0m | time: 126.062s
| Adam | epoch: 006 | loss: 0.27805 - acc: 0.9053 -- iter: 21248/22500
Training Step: 2093  | total loss: [1m[32m0.26409[0m[0m | time: 126.416s
| Adam | epoch: 006 | loss: 0.26409 - acc: 0.9100 -- iter: 21312/22500
Training Step: 2094  | total loss: [1m[32m0.26010[0m[0m | time: 126.766s
| Adam | epoch: 006 | loss: 0.26010 - acc: 0.9112 -- iter: 21376/22500
Training Step: 2095  | total loss: [1m[32m0.25098[0m[0m | time: 127.111s
| Adam | epoch: 006 | loss: 0.25098 - acc: 0.9139 -- iter: 21440/22500
Training Step: 2096  | total loss: [1m[32m0.23943[0m[0m | time: 127.483s
| Adam | epoch: 006 | loss: 0.23943 - acc: 0.9162 -- iter: 21504/22500
Training Step: 2097  | total loss: [1m[32m0.22641[0m[0m | time: 127.852s
| Adam | epoch: 006 | loss: 0.22641 - acc: 0.9168 -- iter: 21568/22500
Training Step: 2098  | total loss: [1m[32m0.21938[0m[0m | time: 128.215s
| Adam | epoch: 006 | loss: 0.21938 - acc: 0.9204 -- iter: 21632/22500
Training Step: 2099  | total loss: [1m[32m0.20829[0m[0m | time: 128.567s
| Adam | epoch: 006 | loss: 0.20829 - acc: 0.9237 -- iter: 21696/22500
Training Step: 2100  | total loss: [1m[32m0.19501[0m[0m | time: 128.919s
| Adam | epoch: 006 | loss: 0.19501 - acc: 0.9282 -- iter: 21760/22500
Training Step: 2101  | total loss: [1m[32m0.20685[0m[0m | time: 129.278s
| Adam | epoch: 006 | loss: 0.20685 - acc: 0.9244 -- iter: 21824/22500
Training Step: 2102  | total loss: [1m[32m0.19142[0m[0m | time: 129.624s
| Adam | epoch: 006 | loss: 0.19142 - acc: 0.9304 -- iter: 21888/22500
Training Step: 2103  | total loss: [1m[32m0.19105[0m[0m | time: 129.972s
| Adam | epoch: 006 | loss: 0.19105 - acc: 0.9311 -- iter: 21952/22500
Training Step: 2104  | total loss: [1m[32m0.17851[0m[0m | time: 130.325s
| Adam | epoch: 006 | loss: 0.17851 - acc: 0.9349 -- iter: 22016/22500
Training Step: 2105  | total loss: [1m[32m0.16880[0m[0m | time: 130.677s
| Adam | epoch: 006 | loss: 0.16880 - acc: 0.9398 -- iter: 22080/22500
Training Step: 2106  | total loss: [1m[32m0.17869[0m[0m | time: 131.028s
| Adam | epoch: 006 | loss: 0.17869 - acc: 0.9365 -- iter: 22144/22500
Training Step: 2107  | total loss: [1m[32m0.17822[0m[0m | time: 131.378s
| Adam | epoch: 006 | loss: 0.17822 - acc: 0.9366 -- iter: 22208/22500
Training Step: 2108  | total loss: [1m[32m0.18147[0m[0m | time: 131.722s
| Adam | epoch: 006 | loss: 0.18147 - acc: 0.9382 -- iter: 22272/22500
Training Step: 2109  | total loss: [1m[32m0.17769[0m[0m | time: 132.071s
| Adam | epoch: 006 | loss: 0.17769 - acc: 0.9382 -- iter: 22336/22500
Training Step: 2110  | total loss: [1m[32m0.17645[0m[0m | time: 132.417s
| Adam | epoch: 006 | loss: 0.17645 - acc: 0.9397 -- iter: 22400/22500
Training Step: 2111  | total loss: [1m[32m0.17857[0m[0m | time: 132.764s
| Adam | epoch: 006 | loss: 0.17857 - acc: 0.9394 -- iter: 22464/22500
Training Step: 2112  | total loss: [1m[32m0.18332[0m[0m | time: 136.292s
| Adam | epoch: 006 | loss: 0.18332 - acc: 0.9377 | val_loss: 0.56769 - val_acc: 0.7976 -- iter: 22500/22500
--
Training Step: 2113  | total loss: [1m[32m0.17195[0m[0m | time: 0.354s
| Adam | epoch: 007 | loss: 0.17195 - acc: 0.9424 -- iter: 00064/22500
Training Step: 2114  | total loss: [1m[32m0.16954[0m[0m | time: 0.699s
| Adam | epoch: 007 | loss: 0.16954 - acc: 0.9403 -- iter: 00128/22500
Training Step: 2115  | total loss: [1m[32m0.16089[0m[0m | time: 1.043s
| Adam | epoch: 007 | loss: 0.16089 - acc: 0.9432 -- iter: 00192/22500
Training Step: 2116  | total loss: [1m[32m0.15445[0m[0m | time: 1.386s
| Adam | epoch: 007 | loss: 0.15445 - acc: 0.9473 -- iter: 00256/22500
Training Step: 2117  | total loss: [1m[32m0.16691[0m[0m | time: 1.662s
| Adam | epoch: 007 | loss: 0.16691 - acc: 0.9416 -- iter: 00320/22500
Training Step: 2118  | total loss: [1m[32m0.15344[0m[0m | time: 1.947s
| Adam | epoch: 007 | loss: 0.15344 - acc: 0.9474 -- iter: 00384/22500
Training Step: 2119  | total loss: [1m[32m0.14171[0m[0m | time: 2.314s
| Adam | epoch: 007 | loss: 0.14171 - acc: 0.9527 -- iter: 00448/22500
Training Step: 2120  | total loss: [1m[32m0.14198[0m[0m | time: 2.681s
| Adam | epoch: 007 | loss: 0.14198 - acc: 0.9527 -- iter: 00512/22500
Training Step: 2121  | total loss: [1m[32m0.14492[0m[0m | time: 3.033s
| Adam | epoch: 007 | loss: 0.14492 - acc: 0.9528 -- iter: 00576/22500
Training Step: 2122  | total loss: [1m[32m0.15597[0m[0m | time: 3.384s
| Adam | epoch: 007 | loss: 0.15597 - acc: 0.9497 -- iter: 00640/22500
Training Step: 2123  | total loss: [1m[32m0.15965[0m[0m | time: 3.729s
| Adam | epoch: 007 | loss: 0.15965 - acc: 0.9485 -- iter: 00704/22500
Training Step: 2124  | total loss: [1m[32m0.15665[0m[0m | time: 4.079s
| Adam | epoch: 007 | loss: 0.15665 - acc: 0.9489 -- iter: 00768/22500
Training Step: 2125  | total loss: [1m[32m0.15216[0m[0m | time: 4.429s
| Adam | epoch: 007 | loss: 0.15216 - acc: 0.9462 -- iter: 00832/22500
Training Step: 2126  | total loss: [1m[32m0.14915[0m[0m | time: 4.775s
| Adam | epoch: 007 | loss: 0.14915 - acc: 0.9469 -- iter: 00896/22500
Training Step: 2127  | total loss: [1m[32m0.14401[0m[0m | time: 5.126s
| Adam | epoch: 007 | loss: 0.14401 - acc: 0.9491 -- iter: 00960/22500
Training Step: 2128  | total loss: [1m[32m0.13820[0m[0m | time: 5.476s
| Adam | epoch: 007 | loss: 0.13820 - acc: 0.9511 -- iter: 01024/22500
Training Step: 2129  | total loss: [1m[32m0.14660[0m[0m | time: 5.824s
| Adam | epoch: 007 | loss: 0.14660 - acc: 0.9513 -- iter: 01088/22500
Training Step: 2130  | total loss: [1m[32m0.14431[0m[0m | time: 6.173s
| Adam | epoch: 007 | loss: 0.14431 - acc: 0.9546 -- iter: 01152/22500
Training Step: 2131  | total loss: [1m[32m0.14129[0m[0m | time: 6.518s
| Adam | epoch: 007 | loss: 0.14129 - acc: 0.9544 -- iter: 01216/22500
Training Step: 2132  | total loss: [1m[32m0.14083[0m[0m | time: 6.867s
| Adam | epoch: 007 | loss: 0.14083 - acc: 0.9512 -- iter: 01280/22500
Training Step: 2133  | total loss: [1m[32m0.14605[0m[0m | time: 7.245s
| Adam | epoch: 007 | loss: 0.14605 - acc: 0.9498 -- iter: 01344/22500
Training Step: 2134  | total loss: [1m[32m0.14601[0m[0m | time: 7.594s
| Adam | epoch: 007 | loss: 0.14601 - acc: 0.9501 -- iter: 01408/22500
Training Step: 2135  | total loss: [1m[32m0.14407[0m[0m | time: 7.950s
| Adam | epoch: 007 | loss: 0.14407 - acc: 0.9504 -- iter: 01472/22500
Training Step: 2136  | total loss: [1m[32m0.14669[0m[0m | time: 8.307s
| Adam | epoch: 007 | loss: 0.14669 - acc: 0.9476 -- iter: 01536/22500
Training Step: 2137  | total loss: [1m[32m0.14496[0m[0m | time: 8.664s
| Adam | epoch: 007 | loss: 0.14496 - acc: 0.9481 -- iter: 01600/22500
Training Step: 2138  | total loss: [1m[32m0.13275[0m[0m | time: 9.016s
| Adam | epoch: 007 | loss: 0.13275 - acc: 0.9533 -- iter: 01664/22500
Training Step: 2139  | total loss: [1m[32m0.13113[0m[0m | time: 9.368s
| Adam | epoch: 007 | loss: 0.13113 - acc: 0.9533 -- iter: 01728/22500
Training Step: 2140  | total loss: [1m[32m0.12877[0m[0m | time: 9.716s
| Adam | epoch: 007 | loss: 0.12877 - acc: 0.9549 -- iter: 01792/22500
Training Step: 2141  | total loss: [1m[32m0.12482[0m[0m | time: 10.061s
| Adam | epoch: 007 | loss: 0.12482 - acc: 0.9562 -- iter: 01856/22500
Training Step: 2142  | total loss: [1m[32m0.11704[0m[0m | time: 10.445s
| Adam | epoch: 007 | loss: 0.11704 - acc: 0.9606 -- iter: 01920/22500
Training Step: 2143  | total loss: [1m[32m0.12130[0m[0m | time: 10.813s
| Adam | epoch: 007 | loss: 0.12130 - acc: 0.9552 -- iter: 01984/22500
Training Step: 2144  | total loss: [1m[32m0.11922[0m[0m | time: 11.166s
| Adam | epoch: 007 | loss: 0.11922 - acc: 0.9581 -- iter: 02048/22500
Training Step: 2145  | total loss: [1m[32m0.12992[0m[0m | time: 11.515s
| Adam | epoch: 007 | loss: 0.12992 - acc: 0.9529 -- iter: 02112/22500
Training Step: 2146  | total loss: [1m[32m0.13182[0m[0m | time: 11.888s
| Adam | epoch: 007 | loss: 0.13182 - acc: 0.9529 -- iter: 02176/22500
Training Step: 2147  | total loss: [1m[32m0.13809[0m[0m | time: 12.317s
| Adam | epoch: 007 | loss: 0.13809 - acc: 0.9545 -- iter: 02240/22500
Training Step: 2148  | total loss: [1m[32m0.14233[0m[0m | time: 12.706s
| Adam | epoch: 007 | loss: 0.14233 - acc: 0.9497 -- iter: 02304/22500
Training Step: 2149  | total loss: [1m[32m0.14265[0m[0m | time: 13.063s
| Adam | epoch: 007 | loss: 0.14265 - acc: 0.9500 -- iter: 02368/22500
Training Step: 2150  | total loss: [1m[32m0.13737[0m[0m | time: 13.422s
| Adam | epoch: 007 | loss: 0.13737 - acc: 0.9535 -- iter: 02432/22500
Training Step: 2151  | total loss: [1m[32m0.13244[0m[0m | time: 13.816s
| Adam | epoch: 007 | loss: 0.13244 - acc: 0.9550 -- iter: 02496/22500
Training Step: 2152  | total loss: [1m[32m0.14475[0m[0m | time: 14.203s
| Adam | epoch: 007 | loss: 0.14475 - acc: 0.9532 -- iter: 02560/22500
Training Step: 2153  | total loss: [1m[32m0.14070[0m[0m | time: 14.617s
| Adam | epoch: 007 | loss: 0.14070 - acc: 0.9548 -- iter: 02624/22500
Training Step: 2154  | total loss: [1m[32m0.13465[0m[0m | time: 15.025s
| Adam | epoch: 007 | loss: 0.13465 - acc: 0.9562 -- iter: 02688/22500
Training Step: 2155  | total loss: [1m[32m0.12820[0m[0m | time: 15.407s
| Adam | epoch: 007 | loss: 0.12820 - acc: 0.9574 -- iter: 02752/22500
Training Step: 2156  | total loss: [1m[32m0.13601[0m[0m | time: 15.789s
| Adam | epoch: 007 | loss: 0.13601 - acc: 0.9523 -- iter: 02816/22500
Training Step: 2157  | total loss: [1m[32m0.13876[0m[0m | time: 16.190s
| Adam | epoch: 007 | loss: 0.13876 - acc: 0.9508 -- iter: 02880/22500
Training Step: 2158  | total loss: [1m[32m0.13641[0m[0m | time: 16.575s
| Adam | epoch: 007 | loss: 0.13641 - acc: 0.9479 -- iter: 02944/22500
Training Step: 2159  | total loss: [1m[32m0.13543[0m[0m | time: 16.967s
| Adam | epoch: 007 | loss: 0.13543 - acc: 0.9500 -- iter: 03008/22500
Training Step: 2160  | total loss: [1m[32m0.14071[0m[0m | time: 17.358s
| Adam | epoch: 007 | loss: 0.14071 - acc: 0.9503 -- iter: 03072/22500
Training Step: 2161  | total loss: [1m[32m0.14282[0m[0m | time: 17.745s
| Adam | epoch: 007 | loss: 0.14282 - acc: 0.9506 -- iter: 03136/22500
Training Step: 2162  | total loss: [1m[32m0.13493[0m[0m | time: 18.134s
| Adam | epoch: 007 | loss: 0.13493 - acc: 0.9540 -- iter: 03200/22500
Training Step: 2163  | total loss: [1m[32m0.13179[0m[0m | time: 18.543s
| Adam | epoch: 007 | loss: 0.13179 - acc: 0.9539 -- iter: 03264/22500
Training Step: 2164  | total loss: [1m[32m0.13439[0m[0m | time: 18.942s
| Adam | epoch: 007 | loss: 0.13439 - acc: 0.9523 -- iter: 03328/22500
Training Step: 2165  | total loss: [1m[32m0.13900[0m[0m | time: 19.361s
| Adam | epoch: 007 | loss: 0.13900 - acc: 0.9477 -- iter: 03392/22500
Training Step: 2166  | total loss: [1m[32m0.14089[0m[0m | time: 19.763s
| Adam | epoch: 007 | loss: 0.14089 - acc: 0.9466 -- iter: 03456/22500
Training Step: 2167  | total loss: [1m[32m0.14621[0m[0m | time: 20.145s
| Adam | epoch: 007 | loss: 0.14621 - acc: 0.9457 -- iter: 03520/22500
Training Step: 2168  | total loss: [1m[32m0.14020[0m[0m | time: 20.542s
| Adam | epoch: 007 | loss: 0.14020 - acc: 0.9480 -- iter: 03584/22500
Training Step: 2169  | total loss: [1m[32m0.14960[0m[0m | time: 20.926s
| Adam | epoch: 007 | loss: 0.14960 - acc: 0.9423 -- iter: 03648/22500
Training Step: 2170  | total loss: [1m[32m0.14526[0m[0m | time: 21.329s
| Adam | epoch: 007 | loss: 0.14526 - acc: 0.9418 -- iter: 03712/22500
Training Step: 2171  | total loss: [1m[32m0.14595[0m[0m | time: 21.722s
| Adam | epoch: 007 | loss: 0.14595 - acc: 0.9414 -- iter: 03776/22500
Training Step: 2172  | total loss: [1m[32m0.15084[0m[0m | time: 22.107s
| Adam | epoch: 007 | loss: 0.15084 - acc: 0.9394 -- iter: 03840/22500
Training Step: 2173  | total loss: [1m[32m0.14837[0m[0m | time: 22.491s
| Adam | epoch: 007 | loss: 0.14837 - acc: 0.9408 -- iter: 03904/22500
Training Step: 2174  | total loss: [1m[32m0.15432[0m[0m | time: 22.902s
| Adam | epoch: 007 | loss: 0.15432 - acc: 0.9405 -- iter: 03968/22500
Training Step: 2175  | total loss: [1m[32m0.15237[0m[0m | time: 23.306s
| Adam | epoch: 007 | loss: 0.15237 - acc: 0.9402 -- iter: 04032/22500
Training Step: 2176  | total loss: [1m[32m0.15184[0m[0m | time: 23.711s
| Adam | epoch: 007 | loss: 0.15184 - acc: 0.9430 -- iter: 04096/22500
Training Step: 2177  | total loss: [1m[32m0.15049[0m[0m | time: 24.114s
| Adam | epoch: 007 | loss: 0.15049 - acc: 0.9440 -- iter: 04160/22500
Training Step: 2178  | total loss: [1m[32m0.15339[0m[0m | time: 24.503s
| Adam | epoch: 007 | loss: 0.15339 - acc: 0.9434 -- iter: 04224/22500
Training Step: 2179  | total loss: [1m[32m0.14930[0m[0m | time: 24.895s
| Adam | epoch: 007 | loss: 0.14930 - acc: 0.9459 -- iter: 04288/22500
Training Step: 2180  | total loss: [1m[32m0.15608[0m[0m | time: 25.317s
| Adam | epoch: 007 | loss: 0.15608 - acc: 0.9435 -- iter: 04352/22500
Training Step: 2181  | total loss: [1m[32m0.15414[0m[0m | time: 25.736s
| Adam | epoch: 007 | loss: 0.15414 - acc: 0.9445 -- iter: 04416/22500
Training Step: 2182  | total loss: [1m[32m0.14902[0m[0m | time: 26.116s
| Adam | epoch: 007 | loss: 0.14902 - acc: 0.9453 -- iter: 04480/22500
Training Step: 2183  | total loss: [1m[32m0.15034[0m[0m | time: 26.510s
| Adam | epoch: 007 | loss: 0.15034 - acc: 0.9461 -- iter: 04544/22500
Training Step: 2184  | total loss: [1m[32m0.14576[0m[0m | time: 26.891s
| Adam | epoch: 007 | loss: 0.14576 - acc: 0.9484 -- iter: 04608/22500
Training Step: 2185  | total loss: [1m[32m0.14519[0m[0m | time: 27.284s
| Adam | epoch: 007 | loss: 0.14519 - acc: 0.9489 -- iter: 04672/22500
Training Step: 2186  | total loss: [1m[32m0.14845[0m[0m | time: 27.677s
| Adam | epoch: 007 | loss: 0.14845 - acc: 0.9462 -- iter: 04736/22500
Training Step: 2187  | total loss: [1m[32m0.15629[0m[0m | time: 28.071s
| Adam | epoch: 007 | loss: 0.15629 - acc: 0.9453 -- iter: 04800/22500
Training Step: 2188  | total loss: [1m[32m0.14617[0m[0m | time: 28.476s
| Adam | epoch: 007 | loss: 0.14617 - acc: 0.9492 -- iter: 04864/22500
Training Step: 2189  | total loss: [1m[32m0.13693[0m[0m | time: 28.856s
| Adam | epoch: 007 | loss: 0.13693 - acc: 0.9527 -- iter: 04928/22500
Training Step: 2190  | total loss: [1m[32m0.14244[0m[0m | time: 29.208s
| Adam | epoch: 007 | loss: 0.14244 - acc: 0.9512 -- iter: 04992/22500
Training Step: 2191  | total loss: [1m[32m0.15264[0m[0m | time: 29.550s
| Adam | epoch: 007 | loss: 0.15264 - acc: 0.9467 -- iter: 05056/22500
Training Step: 2192  | total loss: [1m[32m0.16315[0m[0m | time: 29.968s
| Adam | epoch: 007 | loss: 0.16315 - acc: 0.9458 -- iter: 05120/22500
Training Step: 2193  | total loss: [1m[32m0.16978[0m[0m | time: 30.304s
| Adam | epoch: 007 | loss: 0.16978 - acc: 0.9434 -- iter: 05184/22500
Training Step: 2194  | total loss: [1m[32m0.17839[0m[0m | time: 30.664s
| Adam | epoch: 007 | loss: 0.17839 - acc: 0.9397 -- iter: 05248/22500
Training Step: 2195  | total loss: [1m[32m0.18085[0m[0m | time: 31.040s
| Adam | epoch: 007 | loss: 0.18085 - acc: 0.9379 -- iter: 05312/22500
Training Step: 2196  | total loss: [1m[32m0.18650[0m[0m | time: 31.404s
| Adam | epoch: 007 | loss: 0.18650 - acc: 0.9347 -- iter: 05376/22500
Training Step: 2197  | total loss: [1m[32m0.19073[0m[0m | time: 31.773s
| Adam | epoch: 007 | loss: 0.19073 - acc: 0.9272 -- iter: 05440/22500
Training Step: 2198  | total loss: [1m[32m0.19469[0m[0m | time: 32.134s
| Adam | epoch: 007 | loss: 0.19469 - acc: 0.9235 -- iter: 05504/22500
Training Step: 2199  | total loss: [1m[32m0.18671[0m[0m | time: 32.505s
| Adam | epoch: 007 | loss: 0.18671 - acc: 0.9281 -- iter: 05568/22500
Training Step: 2200  | total loss: [1m[32m0.18841[0m[0m | time: 36.043s
| Adam | epoch: 007 | loss: 0.18841 - acc: 0.9259 | val_loss: 0.58727 - val_acc: 0.7872 -- iter: 05632/22500
--
Training Step: 2201  | total loss: [1m[32m0.18443[0m[0m | time: 36.396s
| Adam | epoch: 007 | loss: 0.18443 - acc: 0.9255 -- iter: 05696/22500
Training Step: 2202  | total loss: [1m[32m0.18013[0m[0m | time: 36.744s
| Adam | epoch: 007 | loss: 0.18013 - acc: 0.9267 -- iter: 05760/22500
Training Step: 2203  | total loss: [1m[32m0.18349[0m[0m | time: 37.091s
| Adam | epoch: 007 | loss: 0.18349 - acc: 0.9278 -- iter: 05824/22500
Training Step: 2204  | total loss: [1m[32m0.20350[0m[0m | time: 37.445s
| Adam | epoch: 007 | loss: 0.20350 - acc: 0.9240 -- iter: 05888/22500
Training Step: 2205  | total loss: [1m[32m0.20456[0m[0m | time: 37.794s
| Adam | epoch: 007 | loss: 0.20456 - acc: 0.9254 -- iter: 05952/22500
Training Step: 2206  | total loss: [1m[32m0.20879[0m[0m | time: 38.140s
| Adam | epoch: 007 | loss: 0.20879 - acc: 0.9250 -- iter: 06016/22500
Training Step: 2207  | total loss: [1m[32m0.20745[0m[0m | time: 38.487s
| Adam | epoch: 007 | loss: 0.20745 - acc: 0.9263 -- iter: 06080/22500
Training Step: 2208  | total loss: [1m[32m0.20185[0m[0m | time: 38.838s
| Adam | epoch: 007 | loss: 0.20185 - acc: 0.9274 -- iter: 06144/22500
Training Step: 2209  | total loss: [1m[32m0.20112[0m[0m | time: 39.185s
| Adam | epoch: 007 | loss: 0.20112 - acc: 0.9300 -- iter: 06208/22500
Training Step: 2210  | total loss: [1m[32m0.18820[0m[0m | time: 39.549s
| Adam | epoch: 007 | loss: 0.18820 - acc: 0.9354 -- iter: 06272/22500
Training Step: 2211  | total loss: [1m[32m0.17780[0m[0m | time: 39.922s
| Adam | epoch: 007 | loss: 0.17780 - acc: 0.9403 -- iter: 06336/22500
Training Step: 2212  | total loss: [1m[32m0.18435[0m[0m | time: 40.272s
| Adam | epoch: 007 | loss: 0.18435 - acc: 0.9400 -- iter: 06400/22500
Training Step: 2213  | total loss: [1m[32m0.20130[0m[0m | time: 40.619s
| Adam | epoch: 007 | loss: 0.20130 - acc: 0.9335 -- iter: 06464/22500
Training Step: 2214  | total loss: [1m[32m0.20334[0m[0m | time: 40.966s
| Adam | epoch: 007 | loss: 0.20334 - acc: 0.9324 -- iter: 06528/22500
Training Step: 2215  | total loss: [1m[32m0.20043[0m[0m | time: 41.313s
| Adam | epoch: 007 | loss: 0.20043 - acc: 0.9313 -- iter: 06592/22500
Training Step: 2216  | total loss: [1m[32m0.19069[0m[0m | time: 41.656s
| Adam | epoch: 007 | loss: 0.19069 - acc: 0.9351 -- iter: 06656/22500
Training Step: 2217  | total loss: [1m[32m0.19390[0m[0m | time: 42.003s
| Adam | epoch: 007 | loss: 0.19390 - acc: 0.9337 -- iter: 06720/22500
Training Step: 2218  | total loss: [1m[32m0.18167[0m[0m | time: 42.358s
| Adam | epoch: 007 | loss: 0.18167 - acc: 0.9404 -- iter: 06784/22500
Training Step: 2219  | total loss: [1m[32m0.18327[0m[0m | time: 42.706s
| Adam | epoch: 007 | loss: 0.18327 - acc: 0.9385 -- iter: 06848/22500
Training Step: 2220  | total loss: [1m[32m0.17824[0m[0m | time: 43.051s
| Adam | epoch: 007 | loss: 0.17824 - acc: 0.9415 -- iter: 06912/22500
Training Step: 2221  | total loss: [1m[32m0.18434[0m[0m | time: 43.399s
| Adam | epoch: 007 | loss: 0.18434 - acc: 0.9364 -- iter: 06976/22500
Training Step: 2222  | total loss: [1m[32m0.19261[0m[0m | time: 43.744s
| Adam | epoch: 007 | loss: 0.19261 - acc: 0.9272 -- iter: 07040/22500
Training Step: 2223  | total loss: [1m[32m0.18759[0m[0m | time: 44.094s
| Adam | epoch: 007 | loss: 0.18759 - acc: 0.9298 -- iter: 07104/22500
Training Step: 2224  | total loss: [1m[32m0.19246[0m[0m | time: 44.444s
| Adam | epoch: 007 | loss: 0.19246 - acc: 0.9274 -- iter: 07168/22500
Training Step: 2225  | total loss: [1m[32m0.18260[0m[0m | time: 44.789s
| Adam | epoch: 007 | loss: 0.18260 - acc: 0.9300 -- iter: 07232/22500
Training Step: 2226  | total loss: [1m[32m0.19334[0m[0m | time: 45.136s
| Adam | epoch: 007 | loss: 0.19334 - acc: 0.9276 -- iter: 07296/22500
Training Step: 2227  | total loss: [1m[32m0.18797[0m[0m | time: 45.488s
| Adam | epoch: 007 | loss: 0.18797 - acc: 0.9302 -- iter: 07360/22500
Training Step: 2228  | total loss: [1m[32m0.18788[0m[0m | time: 45.833s
| Adam | epoch: 007 | loss: 0.18788 - acc: 0.9340 -- iter: 07424/22500
Training Step: 2229  | total loss: [1m[32m0.19573[0m[0m | time: 46.180s
| Adam | epoch: 007 | loss: 0.19573 - acc: 0.9312 -- iter: 07488/22500
Training Step: 2230  | total loss: [1m[32m0.18798[0m[0m | time: 46.523s
| Adam | epoch: 007 | loss: 0.18798 - acc: 0.9350 -- iter: 07552/22500
Training Step: 2231  | total loss: [1m[32m0.19335[0m[0m | time: 46.870s
| Adam | epoch: 007 | loss: 0.19335 - acc: 0.9321 -- iter: 07616/22500
Training Step: 2232  | total loss: [1m[32m0.19140[0m[0m | time: 47.222s
| Adam | epoch: 007 | loss: 0.19140 - acc: 0.9342 -- iter: 07680/22500
Training Step: 2233  | total loss: [1m[32m0.18130[0m[0m | time: 47.605s
| Adam | epoch: 007 | loss: 0.18130 - acc: 0.9392 -- iter: 07744/22500
Training Step: 2234  | total loss: [1m[32m0.18408[0m[0m | time: 47.971s
| Adam | epoch: 007 | loss: 0.18408 - acc: 0.9359 -- iter: 07808/22500
Training Step: 2235  | total loss: [1m[32m0.18068[0m[0m | time: 48.320s
| Adam | epoch: 007 | loss: 0.18068 - acc: 0.9377 -- iter: 07872/22500
Training Step: 2236  | total loss: [1m[32m0.18397[0m[0m | time: 48.670s
| Adam | epoch: 007 | loss: 0.18397 - acc: 0.9345 -- iter: 07936/22500
Training Step: 2237  | total loss: [1m[32m0.18037[0m[0m | time: 49.015s
| Adam | epoch: 007 | loss: 0.18037 - acc: 0.9379 -- iter: 08000/22500
Training Step: 2238  | total loss: [1m[32m0.18331[0m[0m | time: 49.361s
| Adam | epoch: 007 | loss: 0.18331 - acc: 0.9363 -- iter: 08064/22500
Training Step: 2239  | total loss: [1m[32m0.18566[0m[0m | time: 49.708s
| Adam | epoch: 007 | loss: 0.18566 - acc: 0.9318 -- iter: 08128/22500
Training Step: 2240  | total loss: [1m[32m0.18451[0m[0m | time: 50.056s
| Adam | epoch: 007 | loss: 0.18451 - acc: 0.9323 -- iter: 08192/22500
Training Step: 2241  | total loss: [1m[32m0.17488[0m[0m | time: 50.403s
| Adam | epoch: 007 | loss: 0.17488 - acc: 0.9344 -- iter: 08256/22500
Training Step: 2242  | total loss: [1m[32m0.16348[0m[0m | time: 50.754s
| Adam | epoch: 007 | loss: 0.16348 - acc: 0.9410 -- iter: 08320/22500
Training Step: 2243  | total loss: [1m[32m0.16061[0m[0m | time: 51.099s
| Adam | epoch: 007 | loss: 0.16061 - acc: 0.9438 -- iter: 08384/22500
Training Step: 2244  | total loss: [1m[32m0.15951[0m[0m | time: 51.468s
| Adam | epoch: 007 | loss: 0.15951 - acc: 0.9447 -- iter: 08448/22500
Training Step: 2245  | total loss: [1m[32m0.15354[0m[0m | time: 51.832s
| Adam | epoch: 007 | loss: 0.15354 - acc: 0.9455 -- iter: 08512/22500
Training Step: 2246  | total loss: [1m[32m0.16066[0m[0m | time: 52.180s
| Adam | epoch: 007 | loss: 0.16066 - acc: 0.9385 -- iter: 08576/22500
Training Step: 2247  | total loss: [1m[32m0.16116[0m[0m | time: 52.533s
| Adam | epoch: 007 | loss: 0.16116 - acc: 0.9353 -- iter: 08640/22500
Training Step: 2248  | total loss: [1m[32m0.16468[0m[0m | time: 52.877s
| Adam | epoch: 007 | loss: 0.16468 - acc: 0.9370 -- iter: 08704/22500
Training Step: 2249  | total loss: [1m[32m0.16273[0m[0m | time: 53.231s
| Adam | epoch: 007 | loss: 0.16273 - acc: 0.9387 -- iter: 08768/22500
Training Step: 2250  | total loss: [1m[32m0.16656[0m[0m | time: 53.581s
| Adam | epoch: 007 | loss: 0.16656 - acc: 0.9385 -- iter: 08832/22500
Training Step: 2251  | total loss: [1m[32m0.17015[0m[0m | time: 53.929s
| Adam | epoch: 007 | loss: 0.17015 - acc: 0.9416 -- iter: 08896/22500
Training Step: 2252  | total loss: [1m[32m0.18631[0m[0m | time: 54.280s
| Adam | epoch: 007 | loss: 0.18631 - acc: 0.9365 -- iter: 08960/22500
Training Step: 2253  | total loss: [1m[32m0.18631[0m[0m | time: 54.627s
| Adam | epoch: 007 | loss: 0.18631 - acc: 0.9381 -- iter: 09024/22500
Training Step: 2254  | total loss: [1m[32m0.17260[0m[0m | time: 54.974s
| Adam | epoch: 007 | loss: 0.17260 - acc: 0.9443 -- iter: 09088/22500
Training Step: 2255  | total loss: [1m[32m0.16573[0m[0m | time: 55.340s
| Adam | epoch: 007 | loss: 0.16573 - acc: 0.9452 -- iter: 09152/22500
Training Step: 2256  | total loss: [1m[32m0.16414[0m[0m | time: 55.707s
| Adam | epoch: 007 | loss: 0.16414 - acc: 0.9476 -- iter: 09216/22500
Training Step: 2257  | total loss: [1m[32m0.17762[0m[0m | time: 56.054s
| Adam | epoch: 007 | loss: 0.17762 - acc: 0.9450 -- iter: 09280/22500
Training Step: 2258  | total loss: [1m[32m0.16533[0m[0m | time: 56.404s
| Adam | epoch: 007 | loss: 0.16533 - acc: 0.9489 -- iter: 09344/22500
Training Step: 2259  | total loss: [1m[32m0.16246[0m[0m | time: 56.750s
| Adam | epoch: 007 | loss: 0.16246 - acc: 0.9462 -- iter: 09408/22500
Training Step: 2260  | total loss: [1m[32m0.17617[0m[0m | time: 57.100s
| Adam | epoch: 007 | loss: 0.17617 - acc: 0.9407 -- iter: 09472/22500
Training Step: 2261  | total loss: [1m[32m0.17808[0m[0m | time: 57.458s
| Adam | epoch: 007 | loss: 0.17808 - acc: 0.9419 -- iter: 09536/22500
Training Step: 2262  | total loss: [1m[32m0.19713[0m[0m | time: 57.810s
| Adam | epoch: 007 | loss: 0.19713 - acc: 0.9352 -- iter: 09600/22500
Training Step: 2263  | total loss: [1m[32m0.19639[0m[0m | time: 58.164s
| Adam | epoch: 007 | loss: 0.19639 - acc: 0.9354 -- iter: 09664/22500
Training Step: 2264  | total loss: [1m[32m0.19532[0m[0m | time: 58.513s
| Adam | epoch: 007 | loss: 0.19532 - acc: 0.9341 -- iter: 09728/22500
Training Step: 2265  | total loss: [1m[32m0.18384[0m[0m | time: 58.860s
| Adam | epoch: 007 | loss: 0.18384 - acc: 0.9391 -- iter: 09792/22500
Training Step: 2266  | total loss: [1m[32m0.18171[0m[0m | time: 59.210s
| Adam | epoch: 007 | loss: 0.18171 - acc: 0.9405 -- iter: 09856/22500
Training Step: 2267  | total loss: [1m[32m0.18514[0m[0m | time: 59.577s
| Adam | epoch: 007 | loss: 0.18514 - acc: 0.9402 -- iter: 09920/22500
Training Step: 2268  | total loss: [1m[32m0.18049[0m[0m | time: 59.945s
| Adam | epoch: 007 | loss: 0.18049 - acc: 0.9431 -- iter: 09984/22500
Training Step: 2269  | total loss: [1m[32m0.16960[0m[0m | time: 60.339s
| Adam | epoch: 007 | loss: 0.16960 - acc: 0.9456 -- iter: 10048/22500
Training Step: 2270  | total loss: [1m[32m0.16625[0m[0m | time: 60.724s
| Adam | epoch: 007 | loss: 0.16625 - acc: 0.9479 -- iter: 10112/22500
Training Step: 2271  | total loss: [1m[32m0.17499[0m[0m | time: 61.077s
| Adam | epoch: 007 | loss: 0.17499 - acc: 0.9453 -- iter: 10176/22500
Training Step: 2272  | total loss: [1m[32m0.17281[0m[0m | time: 61.446s
| Adam | epoch: 007 | loss: 0.17281 - acc: 0.9461 -- iter: 10240/22500
Training Step: 2273  | total loss: [1m[32m0.17571[0m[0m | time: 61.791s
| Adam | epoch: 007 | loss: 0.17571 - acc: 0.9421 -- iter: 10304/22500
Training Step: 2274  | total loss: [1m[32m0.17294[0m[0m | time: 62.140s
| Adam | epoch: 007 | loss: 0.17294 - acc: 0.9448 -- iter: 10368/22500
Training Step: 2275  | total loss: [1m[32m0.17598[0m[0m | time: 62.490s
| Adam | epoch: 007 | loss: 0.17598 - acc: 0.9409 -- iter: 10432/22500
Training Step: 2276  | total loss: [1m[32m0.18278[0m[0m | time: 62.834s
| Adam | epoch: 007 | loss: 0.18278 - acc: 0.9390 -- iter: 10496/22500
Training Step: 2277  | total loss: [1m[32m0.18097[0m[0m | time: 63.182s
| Adam | epoch: 007 | loss: 0.18097 - acc: 0.9373 -- iter: 10560/22500
Training Step: 2278  | total loss: [1m[32m0.18516[0m[0m | time: 63.547s
| Adam | epoch: 007 | loss: 0.18516 - acc: 0.9373 -- iter: 10624/22500
Training Step: 2279  | total loss: [1m[32m0.18336[0m[0m | time: 63.912s
| Adam | epoch: 007 | loss: 0.18336 - acc: 0.9389 -- iter: 10688/22500
Training Step: 2280  | total loss: [1m[32m0.18157[0m[0m | time: 64.267s
| Adam | epoch: 007 | loss: 0.18157 - acc: 0.9419 -- iter: 10752/22500
Training Step: 2281  | total loss: [1m[32m0.18450[0m[0m | time: 64.615s
| Adam | epoch: 007 | loss: 0.18450 - acc: 0.9368 -- iter: 10816/22500
Training Step: 2282  | total loss: [1m[32m0.19356[0m[0m | time: 64.963s
| Adam | epoch: 007 | loss: 0.19356 - acc: 0.9368 -- iter: 10880/22500
Training Step: 2283  | total loss: [1m[32m0.18315[0m[0m | time: 65.315s
| Adam | epoch: 007 | loss: 0.18315 - acc: 0.9416 -- iter: 10944/22500
Training Step: 2284  | total loss: [1m[32m0.17281[0m[0m | time: 65.662s
| Adam | epoch: 007 | loss: 0.17281 - acc: 0.9443 -- iter: 11008/22500
Training Step: 2285  | total loss: [1m[32m0.16734[0m[0m | time: 66.012s
| Adam | epoch: 007 | loss: 0.16734 - acc: 0.9483 -- iter: 11072/22500
Training Step: 2286  | total loss: [1m[32m0.15966[0m[0m | time: 66.358s
| Adam | epoch: 007 | loss: 0.15966 - acc: 0.9519 -- iter: 11136/22500
Training Step: 2287  | total loss: [1m[32m0.14810[0m[0m | time: 66.704s
| Adam | epoch: 007 | loss: 0.14810 - acc: 0.9567 -- iter: 11200/22500
Training Step: 2288  | total loss: [1m[32m0.15078[0m[0m | time: 67.054s
| Adam | epoch: 007 | loss: 0.15078 - acc: 0.9548 -- iter: 11264/22500
Training Step: 2289  | total loss: [1m[32m0.15404[0m[0m | time: 67.451s
| Adam | epoch: 007 | loss: 0.15404 - acc: 0.9531 -- iter: 11328/22500
Training Step: 2290  | total loss: [1m[32m0.15964[0m[0m | time: 67.817s
| Adam | epoch: 007 | loss: 0.15964 - acc: 0.9468 -- iter: 11392/22500
Training Step: 2291  | total loss: [1m[32m0.15567[0m[0m | time: 68.175s
| Adam | epoch: 007 | loss: 0.15567 - acc: 0.9506 -- iter: 11456/22500
Training Step: 2292  | total loss: [1m[32m0.14413[0m[0m | time: 68.522s
| Adam | epoch: 007 | loss: 0.14413 - acc: 0.9555 -- iter: 11520/22500
Training Step: 2293  | total loss: [1m[32m0.14738[0m[0m | time: 68.870s
| Adam | epoch: 007 | loss: 0.14738 - acc: 0.9522 -- iter: 11584/22500
Training Step: 2294  | total loss: [1m[32m0.15881[0m[0m | time: 69.221s
| Adam | epoch: 007 | loss: 0.15881 - acc: 0.9476 -- iter: 11648/22500
Training Step: 2295  | total loss: [1m[32m0.17084[0m[0m | time: 69.569s
| Adam | epoch: 007 | loss: 0.17084 - acc: 0.9419 -- iter: 11712/22500
Training Step: 2296  | total loss: [1m[32m0.16932[0m[0m | time: 69.913s
| Adam | epoch: 007 | loss: 0.16932 - acc: 0.9430 -- iter: 11776/22500
Training Step: 2297  | total loss: [1m[32m0.18643[0m[0m | time: 70.261s
| Adam | epoch: 007 | loss: 0.18643 - acc: 0.9378 -- iter: 11840/22500
Training Step: 2298  | total loss: [1m[32m0.18793[0m[0m | time: 70.612s
| Adam | epoch: 007 | loss: 0.18793 - acc: 0.9377 -- iter: 11904/22500
Training Step: 2299  | total loss: [1m[32m0.19212[0m[0m | time: 70.958s
| Adam | epoch: 007 | loss: 0.19212 - acc: 0.9362 -- iter: 11968/22500
Training Step: 2300  | total loss: [1m[32m0.20832[0m[0m | time: 71.326s
| Adam | epoch: 007 | loss: 0.20832 - acc: 0.9316 -- iter: 12032/22500
Training Step: 2301  | total loss: [1m[32m0.19898[0m[0m | time: 71.694s
| Adam | epoch: 007 | loss: 0.19898 - acc: 0.9353 -- iter: 12096/22500
Training Step: 2302  | total loss: [1m[32m0.18918[0m[0m | time: 72.040s
| Adam | epoch: 007 | loss: 0.18918 - acc: 0.9402 -- iter: 12160/22500
Training Step: 2303  | total loss: [1m[32m0.18672[0m[0m | time: 72.396s
| Adam | epoch: 007 | loss: 0.18672 - acc: 0.9415 -- iter: 12224/22500
Training Step: 2304  | total loss: [1m[32m0.19106[0m[0m | time: 72.748s
| Adam | epoch: 007 | loss: 0.19106 - acc: 0.9380 -- iter: 12288/22500
Training Step: 2305  | total loss: [1m[32m0.18533[0m[0m | time: 73.094s
| Adam | epoch: 007 | loss: 0.18533 - acc: 0.9411 -- iter: 12352/22500
Training Step: 2306  | total loss: [1m[32m0.17762[0m[0m | time: 73.442s
| Adam | epoch: 007 | loss: 0.17762 - acc: 0.9423 -- iter: 12416/22500
Training Step: 2307  | total loss: [1m[32m0.18060[0m[0m | time: 73.794s
| Adam | epoch: 007 | loss: 0.18060 - acc: 0.9418 -- iter: 12480/22500
Training Step: 2308  | total loss: [1m[32m0.18353[0m[0m | time: 74.138s
| Adam | epoch: 007 | loss: 0.18353 - acc: 0.9382 -- iter: 12544/22500
Training Step: 2309  | total loss: [1m[32m0.18105[0m[0m | time: 74.483s
| Adam | epoch: 007 | loss: 0.18105 - acc: 0.9366 -- iter: 12608/22500
Training Step: 2310  | total loss: [1m[32m0.18147[0m[0m | time: 74.835s
| Adam | epoch: 007 | loss: 0.18147 - acc: 0.9351 -- iter: 12672/22500
Training Step: 2311  | total loss: [1m[32m0.18808[0m[0m | time: 75.197s
| Adam | epoch: 007 | loss: 0.18808 - acc: 0.9291 -- iter: 12736/22500
Training Step: 2312  | total loss: [1m[32m0.17771[0m[0m | time: 75.565s
| Adam | epoch: 007 | loss: 0.17771 - acc: 0.9331 -- iter: 12800/22500
Training Step: 2313  | total loss: [1m[32m0.18337[0m[0m | time: 75.931s
| Adam | epoch: 007 | loss: 0.18337 - acc: 0.9335 -- iter: 12864/22500
Training Step: 2314  | total loss: [1m[32m0.17770[0m[0m | time: 76.282s
| Adam | epoch: 007 | loss: 0.17770 - acc: 0.9370 -- iter: 12928/22500
Training Step: 2315  | total loss: [1m[32m0.17423[0m[0m | time: 76.627s
| Adam | epoch: 007 | loss: 0.17423 - acc: 0.9371 -- iter: 12992/22500
Training Step: 2316  | total loss: [1m[32m0.18447[0m[0m | time: 76.973s
| Adam | epoch: 007 | loss: 0.18447 - acc: 0.9356 -- iter: 13056/22500
Training Step: 2317  | total loss: [1m[32m0.17899[0m[0m | time: 77.322s
| Adam | epoch: 007 | loss: 0.17899 - acc: 0.9373 -- iter: 13120/22500
Training Step: 2318  | total loss: [1m[32m0.17119[0m[0m | time: 77.688s
| Adam | epoch: 007 | loss: 0.17119 - acc: 0.9420 -- iter: 13184/22500
Training Step: 2319  | total loss: [1m[32m0.18504[0m[0m | time: 78.031s
| Adam | epoch: 007 | loss: 0.18504 - acc: 0.9385 -- iter: 13248/22500
Training Step: 2320  | total loss: [1m[32m0.17232[0m[0m | time: 78.382s
| Adam | epoch: 007 | loss: 0.17232 - acc: 0.9430 -- iter: 13312/22500
Training Step: 2321  | total loss: [1m[32m0.17314[0m[0m | time: 78.730s
| Adam | epoch: 007 | loss: 0.17314 - acc: 0.9441 -- iter: 13376/22500
Training Step: 2322  | total loss: [1m[32m0.16631[0m[0m | time: 79.073s
| Adam | epoch: 007 | loss: 0.16631 - acc: 0.9465 -- iter: 13440/22500
Training Step: 2323  | total loss: [1m[32m0.16659[0m[0m | time: 79.443s
| Adam | epoch: 007 | loss: 0.16659 - acc: 0.9472 -- iter: 13504/22500
Training Step: 2324  | total loss: [1m[32m0.16866[0m[0m | time: 79.808s
| Adam | epoch: 007 | loss: 0.16866 - acc: 0.9478 -- iter: 13568/22500
Training Step: 2325  | total loss: [1m[32m0.17369[0m[0m | time: 80.153s
| Adam | epoch: 007 | loss: 0.17369 - acc: 0.9452 -- iter: 13632/22500
Training Step: 2326  | total loss: [1m[32m0.17420[0m[0m | time: 80.499s
| Adam | epoch: 007 | loss: 0.17420 - acc: 0.9444 -- iter: 13696/22500
Training Step: 2327  | total loss: [1m[32m0.17105[0m[0m | time: 80.847s
| Adam | epoch: 007 | loss: 0.17105 - acc: 0.9453 -- iter: 13760/22500
Training Step: 2328  | total loss: [1m[32m0.19252[0m[0m | time: 81.197s
| Adam | epoch: 007 | loss: 0.19252 - acc: 0.9336 -- iter: 13824/22500
Training Step: 2329  | total loss: [1m[32m0.19086[0m[0m | time: 81.548s
| Adam | epoch: 007 | loss: 0.19086 - acc: 0.9371 -- iter: 13888/22500
Training Step: 2330  | total loss: [1m[32m0.18501[0m[0m | time: 81.895s
| Adam | epoch: 007 | loss: 0.18501 - acc: 0.9403 -- iter: 13952/22500
Training Step: 2331  | total loss: [1m[32m0.19148[0m[0m | time: 82.243s
| Adam | epoch: 007 | loss: 0.19148 - acc: 0.9369 -- iter: 14016/22500
Training Step: 2332  | total loss: [1m[32m0.19078[0m[0m | time: 82.587s
| Adam | epoch: 007 | loss: 0.19078 - acc: 0.9369 -- iter: 14080/22500
Training Step: 2333  | total loss: [1m[32m0.19180[0m[0m | time: 82.942s
| Adam | epoch: 007 | loss: 0.19180 - acc: 0.9370 -- iter: 14144/22500
Training Step: 2334  | total loss: [1m[32m0.19329[0m[0m | time: 83.366s
| Adam | epoch: 007 | loss: 0.19329 - acc: 0.9370 -- iter: 14208/22500
Training Step: 2335  | total loss: [1m[32m0.19188[0m[0m | time: 83.719s
| Adam | epoch: 007 | loss: 0.19188 - acc: 0.9355 -- iter: 14272/22500
Training Step: 2336  | total loss: [1m[32m0.19292[0m[0m | time: 84.068s
| Adam | epoch: 007 | loss: 0.19292 - acc: 0.9310 -- iter: 14336/22500
Training Step: 2337  | total loss: [1m[32m0.19651[0m[0m | time: 84.422s
| Adam | epoch: 007 | loss: 0.19651 - acc: 0.9317 -- iter: 14400/22500
Training Step: 2338  | total loss: [1m[32m0.20126[0m[0m | time: 84.766s
| Adam | epoch: 007 | loss: 0.20126 - acc: 0.9323 -- iter: 14464/22500
Training Step: 2339  | total loss: [1m[32m0.19298[0m[0m | time: 85.120s
| Adam | epoch: 007 | loss: 0.19298 - acc: 0.9359 -- iter: 14528/22500
Training Step: 2340  | total loss: [1m[32m0.18346[0m[0m | time: 85.471s
| Adam | epoch: 007 | loss: 0.18346 - acc: 0.9408 -- iter: 14592/22500
Training Step: 2341  | total loss: [1m[32m0.18271[0m[0m | time: 85.817s
| Adam | epoch: 007 | loss: 0.18271 - acc: 0.9373 -- iter: 14656/22500
Training Step: 2342  | total loss: [1m[32m0.19060[0m[0m | time: 86.167s
| Adam | epoch: 007 | loss: 0.19060 - acc: 0.9342 -- iter: 14720/22500
Training Step: 2343  | total loss: [1m[32m0.19032[0m[0m | time: 86.515s
| Adam | epoch: 007 | loss: 0.19032 - acc: 0.9345 -- iter: 14784/22500
Training Step: 2344  | total loss: [1m[32m0.18569[0m[0m | time: 86.858s
| Adam | epoch: 007 | loss: 0.18569 - acc: 0.9348 -- iter: 14848/22500
Training Step: 2345  | total loss: [1m[32m0.18820[0m[0m | time: 87.219s
| Adam | epoch: 007 | loss: 0.18820 - acc: 0.9320 -- iter: 14912/22500
Training Step: 2346  | total loss: [1m[32m0.18221[0m[0m | time: 87.577s
| Adam | epoch: 007 | loss: 0.18221 - acc: 0.9325 -- iter: 14976/22500
Training Step: 2347  | total loss: [1m[32m0.17411[0m[0m | time: 87.921s
| Adam | epoch: 007 | loss: 0.17411 - acc: 0.9361 -- iter: 15040/22500
Training Step: 2348  | total loss: [1m[32m0.16842[0m[0m | time: 88.287s
| Adam | epoch: 007 | loss: 0.16842 - acc: 0.9394 -- iter: 15104/22500
Training Step: 2349  | total loss: [1m[32m0.16857[0m[0m | time: 88.648s
| Adam | epoch: 007 | loss: 0.16857 - acc: 0.9408 -- iter: 15168/22500
Training Step: 2350  | total loss: [1m[32m0.16565[0m[0m | time: 89.002s
| Adam | epoch: 007 | loss: 0.16565 - acc: 0.9420 -- iter: 15232/22500
Training Step: 2351  | total loss: [1m[32m0.15921[0m[0m | time: 89.363s
| Adam | epoch: 007 | loss: 0.15921 - acc: 0.9447 -- iter: 15296/22500
Training Step: 2352  | total loss: [1m[32m0.15742[0m[0m | time: 89.713s
| Adam | epoch: 007 | loss: 0.15742 - acc: 0.9455 -- iter: 15360/22500
Training Step: 2353  | total loss: [1m[32m0.17210[0m[0m | time: 90.057s
| Adam | epoch: 007 | loss: 0.17210 - acc: 0.9416 -- iter: 15424/22500
Training Step: 2354  | total loss: [1m[32m0.16926[0m[0m | time: 90.406s
| Adam | epoch: 007 | loss: 0.16926 - acc: 0.9428 -- iter: 15488/22500
Training Step: 2355  | total loss: [1m[32m0.15926[0m[0m | time: 90.756s
| Adam | epoch: 007 | loss: 0.15926 - acc: 0.9438 -- iter: 15552/22500
Training Step: 2356  | total loss: [1m[32m0.14879[0m[0m | time: 91.101s
| Adam | epoch: 007 | loss: 0.14879 - acc: 0.9478 -- iter: 15616/22500
Training Step: 2357  | total loss: [1m[32m0.14424[0m[0m | time: 91.485s
| Adam | epoch: 007 | loss: 0.14424 - acc: 0.9484 -- iter: 15680/22500
Training Step: 2358  | total loss: [1m[32m0.14267[0m[0m | time: 91.855s
| Adam | epoch: 007 | loss: 0.14267 - acc: 0.9489 -- iter: 15744/22500
Training Step: 2359  | total loss: [1m[32m0.15194[0m[0m | time: 92.206s
| Adam | epoch: 007 | loss: 0.15194 - acc: 0.9462 -- iter: 15808/22500
Training Step: 2360  | total loss: [1m[32m0.15993[0m[0m | time: 92.553s
| Adam | epoch: 007 | loss: 0.15993 - acc: 0.9469 -- iter: 15872/22500
Training Step: 2361  | total loss: [1m[32m0.15195[0m[0m | time: 92.897s
| Adam | epoch: 007 | loss: 0.15195 - acc: 0.9475 -- iter: 15936/22500
Training Step: 2362  | total loss: [1m[32m0.16729[0m[0m | time: 93.248s
| Adam | epoch: 007 | loss: 0.16729 - acc: 0.9418 -- iter: 16000/22500
Training Step: 2363  | total loss: [1m[32m0.17319[0m[0m | time: 93.599s
| Adam | epoch: 007 | loss: 0.17319 - acc: 0.9367 -- iter: 16064/22500
Training Step: 2364  | total loss: [1m[32m0.17219[0m[0m | time: 93.944s
| Adam | epoch: 007 | loss: 0.17219 - acc: 0.9399 -- iter: 16128/22500
Training Step: 2365  | total loss: [1m[32m0.16485[0m[0m | time: 94.293s
| Adam | epoch: 007 | loss: 0.16485 - acc: 0.9412 -- iter: 16192/22500
Training Step: 2366  | total loss: [1m[32m0.16101[0m[0m | time: 94.643s
| Adam | epoch: 007 | loss: 0.16101 - acc: 0.9440 -- iter: 16256/22500
Training Step: 2367  | total loss: [1m[32m0.17436[0m[0m | time: 94.989s
| Adam | epoch: 007 | loss: 0.17436 - acc: 0.9402 -- iter: 16320/22500
Training Step: 2368  | total loss: [1m[32m0.16824[0m[0m | time: 95.353s
| Adam | epoch: 007 | loss: 0.16824 - acc: 0.9430 -- iter: 16384/22500
Training Step: 2369  | total loss: [1m[32m0.16557[0m[0m | time: 95.694s
| Adam | epoch: 007 | loss: 0.16557 - acc: 0.9409 -- iter: 16448/22500
Training Step: 2370  | total loss: [1m[32m0.17203[0m[0m | time: 96.095s
| Adam | epoch: 007 | loss: 0.17203 - acc: 0.9406 -- iter: 16512/22500
Training Step: 2371  | total loss: [1m[32m0.17815[0m[0m | time: 96.420s
| Adam | epoch: 007 | loss: 0.17815 - acc: 0.9372 -- iter: 16576/22500
Training Step: 2372  | total loss: [1m[32m0.17395[0m[0m | time: 96.704s
| Adam | epoch: 007 | loss: 0.17395 - acc: 0.9403 -- iter: 16640/22500
Training Step: 2373  | total loss: [1m[32m0.17324[0m[0m | time: 97.033s
| Adam | epoch: 007 | loss: 0.17324 - acc: 0.9400 -- iter: 16704/22500
Training Step: 2374  | total loss: [1m[32m0.18275[0m[0m | time: 97.376s
| Adam | epoch: 007 | loss: 0.18275 - acc: 0.9398 -- iter: 16768/22500
Training Step: 2375  | total loss: [1m[32m0.17638[0m[0m | time: 97.723s
| Adam | epoch: 007 | loss: 0.17638 - acc: 0.9427 -- iter: 16832/22500
Training Step: 2376  | total loss: [1m[32m0.17556[0m[0m | time: 98.070s
| Adam | epoch: 007 | loss: 0.17556 - acc: 0.9437 -- iter: 16896/22500
Training Step: 2377  | total loss: [1m[32m0.17356[0m[0m | time: 98.420s
| Adam | epoch: 007 | loss: 0.17356 - acc: 0.9431 -- iter: 16960/22500
Training Step: 2378  | total loss: [1m[32m0.17616[0m[0m | time: 98.768s
| Adam | epoch: 007 | loss: 0.17616 - acc: 0.9394 -- iter: 17024/22500
Training Step: 2379  | total loss: [1m[32m0.18576[0m[0m | time: 99.141s
| Adam | epoch: 007 | loss: 0.18576 - acc: 0.9361 -- iter: 17088/22500
Training Step: 2380  | total loss: [1m[32m0.18536[0m[0m | time: 99.506s
| Adam | epoch: 007 | loss: 0.18536 - acc: 0.9347 -- iter: 17152/22500
Training Step: 2381  | total loss: [1m[32m0.17376[0m[0m | time: 99.858s
| Adam | epoch: 007 | loss: 0.17376 - acc: 0.9396 -- iter: 17216/22500
Training Step: 2382  | total loss: [1m[32m0.18521[0m[0m | time: 100.203s
| Adam | epoch: 007 | loss: 0.18521 - acc: 0.9363 -- iter: 17280/22500
Training Step: 2383  | total loss: [1m[32m0.17306[0m[0m | time: 100.549s
| Adam | epoch: 007 | loss: 0.17306 - acc: 0.9411 -- iter: 17344/22500
Training Step: 2384  | total loss: [1m[32m0.16610[0m[0m | time: 100.894s
| Adam | epoch: 007 | loss: 0.16610 - acc: 0.9439 -- iter: 17408/22500
Training Step: 2385  | total loss: [1m[32m0.16032[0m[0m | time: 101.244s
| Adam | epoch: 007 | loss: 0.16032 - acc: 0.9464 -- iter: 17472/22500
Training Step: 2386  | total loss: [1m[32m0.16719[0m[0m | time: 101.588s
| Adam | epoch: 007 | loss: 0.16719 - acc: 0.9439 -- iter: 17536/22500
Training Step: 2387  | total loss: [1m[32m0.16322[0m[0m | time: 101.934s
| Adam | epoch: 007 | loss: 0.16322 - acc: 0.9464 -- iter: 17600/22500
Training Step: 2388  | total loss: [1m[32m0.16697[0m[0m | time: 102.293s
| Adam | epoch: 007 | loss: 0.16697 - acc: 0.9424 -- iter: 17664/22500
Training Step: 2389  | total loss: [1m[32m0.16627[0m[0m | time: 102.636s
| Adam | epoch: 007 | loss: 0.16627 - acc: 0.9435 -- iter: 17728/22500
Training Step: 2390  | total loss: [1m[32m0.17229[0m[0m | time: 102.982s
| Adam | epoch: 007 | loss: 0.17229 - acc: 0.9429 -- iter: 17792/22500
Training Step: 2391  | total loss: [1m[32m0.17232[0m[0m | time: 103.331s
| Adam | epoch: 007 | loss: 0.17232 - acc: 0.9423 -- iter: 17856/22500
Training Step: 2392  | total loss: [1m[32m0.17025[0m[0m | time: 103.683s
| Adam | epoch: 007 | loss: 0.17025 - acc: 0.9403 -- iter: 17920/22500
Training Step: 2393  | total loss: [1m[32m0.17065[0m[0m | time: 104.027s
| Adam | epoch: 007 | loss: 0.17065 - acc: 0.9400 -- iter: 17984/22500
Training Step: 2394  | total loss: [1m[32m0.17580[0m[0m | time: 104.376s
| Adam | epoch: 007 | loss: 0.17580 - acc: 0.9398 -- iter: 18048/22500
Training Step: 2395  | total loss: [1m[32m0.17234[0m[0m | time: 104.726s
| Adam | epoch: 007 | loss: 0.17234 - acc: 0.9411 -- iter: 18112/22500
Training Step: 2396  | total loss: [1m[32m0.16632[0m[0m | time: 105.072s
| Adam | epoch: 007 | loss: 0.16632 - acc: 0.9439 -- iter: 18176/22500
Training Step: 2397  | total loss: [1m[32m0.17223[0m[0m | time: 105.425s
| Adam | epoch: 007 | loss: 0.17223 - acc: 0.9417 -- iter: 18240/22500
Training Step: 2398  | total loss: [1m[32m0.16822[0m[0m | time: 105.770s
| Adam | epoch: 007 | loss: 0.16822 - acc: 0.9444 -- iter: 18304/22500
Training Step: 2399  | total loss: [1m[32m0.15893[0m[0m | time: 106.114s
| Adam | epoch: 007 | loss: 0.15893 - acc: 0.9484 -- iter: 18368/22500
Training Step: 2400  | total loss: [1m[32m0.16178[0m[0m | time: 109.597s
| Adam | epoch: 007 | loss: 0.16178 - acc: 0.9488 | val_loss: 0.60304 - val_acc: 0.7972 -- iter: 18432/22500
--
Training Step: 2401  | total loss: [1m[32m0.17041[0m[0m | time: 109.946s
| Adam | epoch: 007 | loss: 0.17041 - acc: 0.9430 -- iter: 18496/22500
Training Step: 2402  | total loss: [1m[32m0.17557[0m[0m | time: 110.320s
| Adam | epoch: 007 | loss: 0.17557 - acc: 0.9378 -- iter: 18560/22500
Training Step: 2403  | total loss: [1m[32m0.17138[0m[0m | time: 110.686s
| Adam | epoch: 007 | loss: 0.17138 - acc: 0.9409 -- iter: 18624/22500
Training Step: 2404  | total loss: [1m[32m0.16235[0m[0m | time: 111.034s
| Adam | epoch: 007 | loss: 0.16235 - acc: 0.9452 -- iter: 18688/22500
Training Step: 2405  | total loss: [1m[32m0.15877[0m[0m | time: 111.391s
| Adam | epoch: 007 | loss: 0.15877 - acc: 0.9445 -- iter: 18752/22500
Training Step: 2406  | total loss: [1m[32m0.16033[0m[0m | time: 111.740s
| Adam | epoch: 007 | loss: 0.16033 - acc: 0.9438 -- iter: 18816/22500
Training Step: 2407  | total loss: [1m[32m0.15481[0m[0m | time: 112.089s
| Adam | epoch: 007 | loss: 0.15481 - acc: 0.9447 -- iter: 18880/22500
Training Step: 2408  | total loss: [1m[32m0.16398[0m[0m | time: 112.437s
| Adam | epoch: 007 | loss: 0.16398 - acc: 0.9440 -- iter: 18944/22500
Training Step: 2409  | total loss: [1m[32m0.16500[0m[0m | time: 112.781s
| Adam | epoch: 007 | loss: 0.16500 - acc: 0.9433 -- iter: 19008/22500
Training Step: 2410  | total loss: [1m[32m0.16576[0m[0m | time: 113.132s
| Adam | epoch: 007 | loss: 0.16576 - acc: 0.9427 -- iter: 19072/22500
Training Step: 2411  | total loss: [1m[32m0.16996[0m[0m | time: 113.483s
| Adam | epoch: 007 | loss: 0.16996 - acc: 0.9422 -- iter: 19136/22500
Training Step: 2412  | total loss: [1m[32m0.17258[0m[0m | time: 113.835s
| Adam | epoch: 007 | loss: 0.17258 - acc: 0.9417 -- iter: 19200/22500
Training Step: 2413  | total loss: [1m[32m0.17392[0m[0m | time: 114.181s
| Adam | epoch: 007 | loss: 0.17392 - acc: 0.9398 -- iter: 19264/22500
Training Step: 2414  | total loss: [1m[32m0.17894[0m[0m | time: 114.530s
| Adam | epoch: 007 | loss: 0.17894 - acc: 0.9380 -- iter: 19328/22500
Training Step: 2415  | total loss: [1m[32m0.18984[0m[0m | time: 114.878s
| Adam | epoch: 007 | loss: 0.18984 - acc: 0.9364 -- iter: 19392/22500
Training Step: 2416  | total loss: [1m[32m0.18452[0m[0m | time: 115.227s
| Adam | epoch: 007 | loss: 0.18452 - acc: 0.9365 -- iter: 19456/22500
Training Step: 2417  | total loss: [1m[32m0.18466[0m[0m | time: 115.576s
| Adam | epoch: 007 | loss: 0.18466 - acc: 0.9335 -- iter: 19520/22500
Training Step: 2418  | total loss: [1m[32m0.17918[0m[0m | time: 115.922s
| Adam | epoch: 007 | loss: 0.17918 - acc: 0.9385 -- iter: 19584/22500
Training Step: 2419  | total loss: [1m[32m0.17723[0m[0m | time: 116.270s
| Adam | epoch: 007 | loss: 0.17723 - acc: 0.9416 -- iter: 19648/22500
Training Step: 2420  | total loss: [1m[32m0.17632[0m[0m | time: 116.620s
| Adam | epoch: 007 | loss: 0.17632 - acc: 0.9427 -- iter: 19712/22500
Training Step: 2421  | total loss: [1m[32m0.17037[0m[0m | time: 116.966s
| Adam | epoch: 007 | loss: 0.17037 - acc: 0.9469 -- iter: 19776/22500
Training Step: 2422  | total loss: [1m[32m0.16229[0m[0m | time: 117.325s
| Adam | epoch: 007 | loss: 0.16229 - acc: 0.9491 -- iter: 19840/22500
Training Step: 2423  | total loss: [1m[32m0.15829[0m[0m | time: 117.674s
| Adam | epoch: 007 | loss: 0.15829 - acc: 0.9510 -- iter: 19904/22500
Training Step: 2424  | total loss: [1m[32m0.15332[0m[0m | time: 118.022s
| Adam | epoch: 007 | loss: 0.15332 - acc: 0.9528 -- iter: 19968/22500
Training Step: 2425  | total loss: [1m[32m0.16005[0m[0m | time: 118.413s
| Adam | epoch: 007 | loss: 0.16005 - acc: 0.9497 -- iter: 20032/22500
Training Step: 2426  | total loss: [1m[32m0.15094[0m[0m | time: 118.779s
| Adam | epoch: 007 | loss: 0.15094 - acc: 0.9516 -- iter: 20096/22500
Training Step: 2427  | total loss: [1m[32m0.15713[0m[0m | time: 119.130s
| Adam | epoch: 007 | loss: 0.15713 - acc: 0.9502 -- iter: 20160/22500
Training Step: 2428  | total loss: [1m[32m0.15631[0m[0m | time: 119.479s
| Adam | epoch: 007 | loss: 0.15631 - acc: 0.9489 -- iter: 20224/22500
Training Step: 2429  | total loss: [1m[32m0.15577[0m[0m | time: 119.826s
| Adam | epoch: 007 | loss: 0.15577 - acc: 0.9478 -- iter: 20288/22500
Training Step: 2430  | total loss: [1m[32m0.16698[0m[0m | time: 120.174s
| Adam | epoch: 007 | loss: 0.16698 - acc: 0.9421 -- iter: 20352/22500
Training Step: 2431  | total loss: [1m[32m0.17539[0m[0m | time: 120.525s
| Adam | epoch: 007 | loss: 0.17539 - acc: 0.9385 -- iter: 20416/22500
Training Step: 2432  | total loss: [1m[32m0.16891[0m[0m | time: 120.872s
| Adam | epoch: 007 | loss: 0.16891 - acc: 0.9415 -- iter: 20480/22500
Training Step: 2433  | total loss: [1m[32m0.17152[0m[0m | time: 121.221s
| Adam | epoch: 007 | loss: 0.17152 - acc: 0.9411 -- iter: 20544/22500
Training Step: 2434  | total loss: [1m[32m0.17058[0m[0m | time: 121.574s
| Adam | epoch: 007 | loss: 0.17058 - acc: 0.9423 -- iter: 20608/22500
Training Step: 2435  | total loss: [1m[32m0.18123[0m[0m | time: 121.920s
| Adam | epoch: 007 | loss: 0.18123 - acc: 0.9371 -- iter: 20672/22500
Training Step: 2436  | total loss: [1m[32m0.18623[0m[0m | time: 122.283s
| Adam | epoch: 007 | loss: 0.18623 - acc: 0.9309 -- iter: 20736/22500
Training Step: 2437  | total loss: [1m[32m0.18248[0m[0m | time: 122.654s
| Adam | epoch: 007 | loss: 0.18248 - acc: 0.9332 -- iter: 20800/22500
Training Step: 2438  | total loss: [1m[32m0.20247[0m[0m | time: 123.004s
| Adam | epoch: 007 | loss: 0.20247 - acc: 0.9258 -- iter: 20864/22500
Training Step: 2439  | total loss: [1m[32m0.20004[0m[0m | time: 123.351s
| Adam | epoch: 007 | loss: 0.20004 - acc: 0.9285 -- iter: 20928/22500
Training Step: 2440  | total loss: [1m[32m0.18688[0m[0m | time: 123.705s
| Adam | epoch: 007 | loss: 0.18688 - acc: 0.9341 -- iter: 20992/22500
Training Step: 2441  | total loss: [1m[32m0.18186[0m[0m | time: 124.054s
| Adam | epoch: 007 | loss: 0.18186 - acc: 0.9360 -- iter: 21056/22500
Training Step: 2442  | total loss: [1m[32m0.17494[0m[0m | time: 124.404s
| Adam | epoch: 007 | loss: 0.17494 - acc: 0.9408 -- iter: 21120/22500
Training Step: 2443  | total loss: [1m[32m0.17073[0m[0m | time: 124.755s
| Adam | epoch: 007 | loss: 0.17073 - acc: 0.9421 -- iter: 21184/22500
Training Step: 2444  | total loss: [1m[32m0.16117[0m[0m | time: 125.103s
| Adam | epoch: 007 | loss: 0.16117 - acc: 0.9447 -- iter: 21248/22500
Training Step: 2445  | total loss: [1m[32m0.15177[0m[0m | time: 125.460s
| Adam | epoch: 007 | loss: 0.15177 - acc: 0.9487 -- iter: 21312/22500
Training Step: 2446  | total loss: [1m[32m0.14420[0m[0m | time: 125.814s
| Adam | epoch: 007 | loss: 0.14420 - acc: 0.9523 -- iter: 21376/22500
Training Step: 2447  | total loss: [1m[32m0.13606[0m[0m | time: 126.183s
| Adam | epoch: 007 | loss: 0.13606 - acc: 0.9570 -- iter: 21440/22500
Training Step: 2448  | total loss: [1m[32m0.15142[0m[0m | time: 126.554s
| Adam | epoch: 007 | loss: 0.15142 - acc: 0.9520 -- iter: 21504/22500
Training Step: 2449  | total loss: [1m[32m0.14236[0m[0m | time: 126.905s
| Adam | epoch: 007 | loss: 0.14236 - acc: 0.9552 -- iter: 21568/22500
Training Step: 2450  | total loss: [1m[32m0.14586[0m[0m | time: 127.291s
| Adam | epoch: 007 | loss: 0.14586 - acc: 0.9550 -- iter: 21632/22500
Training Step: 2451  | total loss: [1m[32m0.14123[0m[0m | time: 127.644s
| Adam | epoch: 007 | loss: 0.14123 - acc: 0.9564 -- iter: 21696/22500
Training Step: 2452  | total loss: [1m[32m0.14610[0m[0m | time: 128.008s
| Adam | epoch: 007 | loss: 0.14610 - acc: 0.9576 -- iter: 21760/22500
Training Step: 2453  | total loss: [1m[32m0.14805[0m[0m | time: 128.361s
| Adam | epoch: 007 | loss: 0.14805 - acc: 0.9556 -- iter: 21824/22500
Training Step: 2454  | total loss: [1m[32m0.14319[0m[0m | time: 128.713s
| Adam | epoch: 007 | loss: 0.14319 - acc: 0.9569 -- iter: 21888/22500
Training Step: 2455  | total loss: [1m[32m0.13842[0m[0m | time: 129.069s
| Adam | epoch: 007 | loss: 0.13842 - acc: 0.9581 -- iter: 21952/22500
Training Step: 2456  | total loss: [1m[32m0.13662[0m[0m | time: 129.422s
| Adam | epoch: 007 | loss: 0.13662 - acc: 0.9560 -- iter: 22016/22500
Training Step: 2457  | total loss: [1m[32m0.14712[0m[0m | time: 129.767s
| Adam | epoch: 007 | loss: 0.14712 - acc: 0.9511 -- iter: 22080/22500
Training Step: 2458  | total loss: [1m[32m0.14425[0m[0m | time: 130.117s
| Adam | epoch: 007 | loss: 0.14425 - acc: 0.9528 -- iter: 22144/22500
Training Step: 2459  | total loss: [1m[32m0.13574[0m[0m | time: 130.470s
| Adam | epoch: 007 | loss: 0.13574 - acc: 0.9560 -- iter: 22208/22500
Training Step: 2460  | total loss: [1m[32m0.14255[0m[0m | time: 130.816s
| Adam | epoch: 007 | loss: 0.14255 - acc: 0.9526 -- iter: 22272/22500
Training Step: 2461  | total loss: [1m[32m0.13972[0m[0m | time: 131.196s
| Adam | epoch: 007 | loss: 0.13972 - acc: 0.9542 -- iter: 22336/22500
Training Step: 2462  | total loss: [1m[32m0.12989[0m[0m | time: 131.547s
| Adam | epoch: 007 | loss: 0.12989 - acc: 0.9588 -- iter: 22400/22500
Training Step: 2463  | total loss: [1m[32m0.13076[0m[0m | time: 131.895s
| Adam | epoch: 007 | loss: 0.13076 - acc: 0.9582 -- iter: 22464/22500
Training Step: 2464  | total loss: [1m[32m0.12666[0m[0m | time: 135.394s
| Adam | epoch: 007 | loss: 0.12666 - acc: 0.9593 | val_loss: 0.61536 - val_acc: 0.8040 -- iter: 22500/22500
--
Training Step: 2465  | total loss: [1m[32m0.12290[0m[0m | time: 0.351s
| Adam | epoch: 008 | loss: 0.12290 - acc: 0.9618 -- iter: 00064/22500
Training Step: 2466  | total loss: [1m[32m0.12058[0m[0m | time: 0.700s
| Adam | epoch: 008 | loss: 0.12058 - acc: 0.9609 -- iter: 00128/22500
Training Step: 2467  | total loss: [1m[32m0.12010[0m[0m | time: 1.046s
| Adam | epoch: 008 | loss: 0.12010 - acc: 0.9617 -- iter: 00192/22500
Training Step: 2468  | total loss: [1m[32m0.11521[0m[0m | time: 1.396s
| Adam | epoch: 008 | loss: 0.11521 - acc: 0.9608 -- iter: 00256/22500
Training Step: 2469  | total loss: [1m[32m0.11971[0m[0m | time: 1.743s
| Adam | epoch: 008 | loss: 0.11971 - acc: 0.9569 -- iter: 00320/22500
Training Step: 2470  | total loss: [1m[32m0.12041[0m[0m | time: 2.035s
| Adam | epoch: 008 | loss: 0.12041 - acc: 0.9581 -- iter: 00384/22500
Training Step: 2471  | total loss: [1m[32m0.12637[0m[0m | time: 2.322s
| Adam | epoch: 008 | loss: 0.12637 - acc: 0.9540 -- iter: 00448/22500
Training Step: 2472  | total loss: [1m[32m0.12911[0m[0m | time: 2.676s
| Adam | epoch: 008 | loss: 0.12911 - acc: 0.9530 -- iter: 00512/22500
Training Step: 2473  | total loss: [1m[32m0.13129[0m[0m | time: 3.026s
| Adam | epoch: 008 | loss: 0.13129 - acc: 0.9530 -- iter: 00576/22500
Training Step: 2474  | total loss: [1m[32m0.12958[0m[0m | time: 3.372s
| Adam | epoch: 008 | loss: 0.12958 - acc: 0.9546 -- iter: 00640/22500
Training Step: 2475  | total loss: [1m[32m0.12765[0m[0m | time: 3.719s
| Adam | epoch: 008 | loss: 0.12765 - acc: 0.9560 -- iter: 00704/22500
Training Step: 2476  | total loss: [1m[32m0.12603[0m[0m | time: 4.069s
| Adam | epoch: 008 | loss: 0.12603 - acc: 0.9557 -- iter: 00768/22500
Training Step: 2477  | total loss: [1m[32m0.13085[0m[0m | time: 4.416s
| Adam | epoch: 008 | loss: 0.13085 - acc: 0.9570 -- iter: 00832/22500
Training Step: 2478  | total loss: [1m[32m0.13393[0m[0m | time: 4.764s
| Adam | epoch: 008 | loss: 0.13393 - acc: 0.9566 -- iter: 00896/22500
Training Step: 2479  | total loss: [1m[32m0.14504[0m[0m | time: 5.115s
| Adam | epoch: 008 | loss: 0.14504 - acc: 0.9532 -- iter: 00960/22500
Training Step: 2480  | total loss: [1m[32m0.14053[0m[0m | time: 5.463s
| Adam | epoch: 008 | loss: 0.14053 - acc: 0.9532 -- iter: 01024/22500
Training Step: 2481  | total loss: [1m[32m0.14634[0m[0m | time: 5.816s
| Adam | epoch: 008 | loss: 0.14634 - acc: 0.9516 -- iter: 01088/22500
Training Step: 2482  | total loss: [1m[32m0.13524[0m[0m | time: 6.167s
| Adam | epoch: 008 | loss: 0.13524 - acc: 0.9549 -- iter: 01152/22500
Training Step: 2483  | total loss: [1m[32m0.13565[0m[0m | time: 6.514s
| Adam | epoch: 008 | loss: 0.13565 - acc: 0.9547 -- iter: 01216/22500
Training Step: 2484  | total loss: [1m[32m0.12999[0m[0m | time: 6.863s
| Adam | epoch: 008 | loss: 0.12999 - acc: 0.9561 -- iter: 01280/22500
Training Step: 2485  | total loss: [1m[32m0.14136[0m[0m | time: 7.212s
| Adam | epoch: 008 | loss: 0.14136 - acc: 0.9527 -- iter: 01344/22500
Training Step: 2486  | total loss: [1m[32m0.14179[0m[0m | time: 7.561s
| Adam | epoch: 008 | loss: 0.14179 - acc: 0.9527 -- iter: 01408/22500
Training Step: 2487  | total loss: [1m[32m0.13317[0m[0m | time: 7.920s
| Adam | epoch: 008 | loss: 0.13317 - acc: 0.9559 -- iter: 01472/22500
Training Step: 2488  | total loss: [1m[32m0.12405[0m[0m | time: 8.266s
| Adam | epoch: 008 | loss: 0.12405 - acc: 0.9603 -- iter: 01536/22500
Training Step: 2489  | total loss: [1m[32m0.11926[0m[0m | time: 8.617s
| Adam | epoch: 008 | loss: 0.11926 - acc: 0.9627 -- iter: 01600/22500
Training Step: 2490  | total loss: [1m[32m0.11250[0m[0m | time: 8.966s
| Adam | epoch: 008 | loss: 0.11250 - acc: 0.9664 -- iter: 01664/22500
Training Step: 2491  | total loss: [1m[32m0.11135[0m[0m | time: 9.311s
| Adam | epoch: 008 | loss: 0.11135 - acc: 0.9682 -- iter: 01728/22500
Training Step: 2492  | total loss: [1m[32m0.12139[0m[0m | time: 9.656s
| Adam | epoch: 008 | loss: 0.12139 - acc: 0.9652 -- iter: 01792/22500
Training Step: 2493  | total loss: [1m[32m0.11955[0m[0m | time: 10.043s
| Adam | epoch: 008 | loss: 0.11955 - acc: 0.9655 -- iter: 01856/22500
Training Step: 2494  | total loss: [1m[32m0.12186[0m[0m | time: 10.408s
| Adam | epoch: 008 | loss: 0.12186 - acc: 0.9658 -- iter: 01920/22500
Training Step: 2495  | total loss: [1m[32m0.12167[0m[0m | time: 10.753s
| Adam | epoch: 008 | loss: 0.12167 - acc: 0.9646 -- iter: 01984/22500
Training Step: 2496  | total loss: [1m[32m0.11495[0m[0m | time: 11.104s
| Adam | epoch: 008 | loss: 0.11495 - acc: 0.9665 -- iter: 02048/22500
Training Step: 2497  | total loss: [1m[32m0.12266[0m[0m | time: 11.451s
| Adam | epoch: 008 | loss: 0.12266 - acc: 0.9636 -- iter: 02112/22500
Training Step: 2498  | total loss: [1m[32m0.13216[0m[0m | time: 11.807s
| Adam | epoch: 008 | loss: 0.13216 - acc: 0.9595 -- iter: 02176/22500
Training Step: 2499  | total loss: [1m[32m0.12673[0m[0m | time: 12.156s
| Adam | epoch: 008 | loss: 0.12673 - acc: 0.9620 -- iter: 02240/22500
Training Step: 2500  | total loss: [1m[32m0.12292[0m[0m | time: 12.502s
| Adam | epoch: 008 | loss: 0.12292 - acc: 0.9611 -- iter: 02304/22500
Training Step: 2501  | total loss: [1m[32m0.11558[0m[0m | time: 12.848s
| Adam | epoch: 008 | loss: 0.11558 - acc: 0.9634 -- iter: 02368/22500
Training Step: 2502  | total loss: [1m[32m0.11302[0m[0m | time: 13.198s
| Adam | epoch: 008 | loss: 0.11302 - acc: 0.9639 -- iter: 02432/22500
Training Step: 2503  | total loss: [1m[32m0.11024[0m[0m | time: 13.546s
| Adam | epoch: 008 | loss: 0.11024 - acc: 0.9660 -- iter: 02496/22500
Training Step: 2504  | total loss: [1m[32m0.11204[0m[0m | time: 13.912s
| Adam | epoch: 008 | loss: 0.11204 - acc: 0.9663 -- iter: 02560/22500
Training Step: 2505  | total loss: [1m[32m0.12060[0m[0m | time: 14.285s
| Adam | epoch: 008 | loss: 0.12060 - acc: 0.9603 -- iter: 02624/22500
Training Step: 2506  | total loss: [1m[32m0.11805[0m[0m | time: 14.632s
| Adam | epoch: 008 | loss: 0.11805 - acc: 0.9627 -- iter: 02688/22500
Training Step: 2507  | total loss: [1m[32m0.12921[0m[0m | time: 14.982s
| Adam | epoch: 008 | loss: 0.12921 - acc: 0.9555 -- iter: 02752/22500
Training Step: 2508  | total loss: [1m[32m0.13232[0m[0m | time: 15.329s
| Adam | epoch: 008 | loss: 0.13232 - acc: 0.9537 -- iter: 02816/22500
Training Step: 2509  | total loss: [1m[32m0.13900[0m[0m | time: 15.675s
| Adam | epoch: 008 | loss: 0.13900 - acc: 0.9536 -- iter: 02880/22500
Training Step: 2510  | total loss: [1m[32m0.13451[0m[0m | time: 16.022s
| Adam | epoch: 008 | loss: 0.13451 - acc: 0.9536 -- iter: 02944/22500
Training Step: 2511  | total loss: [1m[32m0.13225[0m[0m | time: 16.371s
| Adam | epoch: 008 | loss: 0.13225 - acc: 0.9566 -- iter: 03008/22500
Training Step: 2512  | total loss: [1m[32m0.13353[0m[0m | time: 16.720s
| Adam | epoch: 008 | loss: 0.13353 - acc: 0.9547 -- iter: 03072/22500
Training Step: 2513  | total loss: [1m[32m0.12702[0m[0m | time: 17.069s
| Adam | epoch: 008 | loss: 0.12702 - acc: 0.9561 -- iter: 03136/22500
Training Step: 2514  | total loss: [1m[32m0.12354[0m[0m | time: 17.418s
| Adam | epoch: 008 | loss: 0.12354 - acc: 0.9574 -- iter: 03200/22500
Training Step: 2515  | total loss: [1m[32m0.12569[0m[0m | time: 17.779s
| Adam | epoch: 008 | loss: 0.12569 - acc: 0.9570 -- iter: 03264/22500
Training Step: 2516  | total loss: [1m[32m0.12524[0m[0m | time: 18.153s
| Adam | epoch: 008 | loss: 0.12524 - acc: 0.9566 -- iter: 03328/22500
Training Step: 2517  | total loss: [1m[32m0.12942[0m[0m | time: 18.502s
| Adam | epoch: 008 | loss: 0.12942 - acc: 0.9547 -- iter: 03392/22500
Training Step: 2518  | total loss: [1m[32m0.13379[0m[0m | time: 18.847s
| Adam | epoch: 008 | loss: 0.13379 - acc: 0.9514 -- iter: 03456/22500
Training Step: 2519  | total loss: [1m[32m0.12738[0m[0m | time: 19.198s
| Adam | epoch: 008 | loss: 0.12738 - acc: 0.9531 -- iter: 03520/22500
Training Step: 2520  | total loss: [1m[32m0.13455[0m[0m | time: 19.548s
| Adam | epoch: 008 | loss: 0.13455 - acc: 0.9531 -- iter: 03584/22500
Training Step: 2521  | total loss: [1m[32m0.13156[0m[0m | time: 19.895s
| Adam | epoch: 008 | loss: 0.13156 - acc: 0.9547 -- iter: 03648/22500
Training Step: 2522  | total loss: [1m[32m0.12868[0m[0m | time: 20.242s
| Adam | epoch: 008 | loss: 0.12868 - acc: 0.9545 -- iter: 03712/22500
Training Step: 2523  | total loss: [1m[32m0.13180[0m[0m | time: 20.591s
| Adam | epoch: 008 | loss: 0.13180 - acc: 0.9560 -- iter: 03776/22500
Training Step: 2524  | total loss: [1m[32m0.13799[0m[0m | time: 20.942s
| Adam | epoch: 008 | loss: 0.13799 - acc: 0.9557 -- iter: 03840/22500
Training Step: 2525  | total loss: [1m[32m0.13566[0m[0m | time: 21.288s
| Adam | epoch: 008 | loss: 0.13566 - acc: 0.9554 -- iter: 03904/22500
Training Step: 2526  | total loss: [1m[32m0.13591[0m[0m | time: 21.638s
| Adam | epoch: 008 | loss: 0.13591 - acc: 0.9552 -- iter: 03968/22500
Training Step: 2527  | total loss: [1m[32m0.13750[0m[0m | time: 22.007s
| Adam | epoch: 008 | loss: 0.13750 - acc: 0.9565 -- iter: 04032/22500
Training Step: 2528  | total loss: [1m[32m0.13714[0m[0m | time: 22.375s
| Adam | epoch: 008 | loss: 0.13714 - acc: 0.9578 -- iter: 04096/22500
Training Step: 2529  | total loss: [1m[32m0.12947[0m[0m | time: 22.732s
| Adam | epoch: 008 | loss: 0.12947 - acc: 0.9604 -- iter: 04160/22500
Training Step: 2530  | total loss: [1m[32m0.12238[0m[0m | time: 23.141s
| Adam | epoch: 008 | loss: 0.12238 - acc: 0.9628 -- iter: 04224/22500
Training Step: 2531  | total loss: [1m[32m0.12711[0m[0m | time: 23.522s
| Adam | epoch: 008 | loss: 0.12711 - acc: 0.9572 -- iter: 04288/22500
Training Step: 2532  | total loss: [1m[32m0.12996[0m[0m | time: 23.874s
| Adam | epoch: 008 | loss: 0.12996 - acc: 0.9568 -- iter: 04352/22500
Training Step: 2533  | total loss: [1m[32m0.13202[0m[0m | time: 24.242s
| Adam | epoch: 008 | loss: 0.13202 - acc: 0.9533 -- iter: 04416/22500
Training Step: 2534  | total loss: [1m[32m0.12351[0m[0m | time: 24.593s
| Adam | epoch: 008 | loss: 0.12351 - acc: 0.9564 -- iter: 04480/22500
Training Step: 2535  | total loss: [1m[32m0.11686[0m[0m | time: 24.940s
| Adam | epoch: 008 | loss: 0.11686 - acc: 0.9592 -- iter: 04544/22500
Training Step: 2536  | total loss: [1m[32m0.11149[0m[0m | time: 25.289s
| Adam | epoch: 008 | loss: 0.11149 - acc: 0.9617 -- iter: 04608/22500
Training Step: 2537  | total loss: [1m[32m0.11049[0m[0m | time: 25.633s
| Adam | epoch: 008 | loss: 0.11049 - acc: 0.9624 -- iter: 04672/22500
Training Step: 2538  | total loss: [1m[32m0.11387[0m[0m | time: 26.003s
| Adam | epoch: 008 | loss: 0.11387 - acc: 0.9615 -- iter: 04736/22500
Training Step: 2539  | total loss: [1m[32m0.11241[0m[0m | time: 26.375s
| Adam | epoch: 008 | loss: 0.11241 - acc: 0.9606 -- iter: 04800/22500
Training Step: 2540  | total loss: [1m[32m0.12197[0m[0m | time: 26.791s
| Adam | epoch: 008 | loss: 0.12197 - acc: 0.9568 -- iter: 04864/22500
Training Step: 2541  | total loss: [1m[32m0.12295[0m[0m | time: 27.072s
| Adam | epoch: 008 | loss: 0.12295 - acc: 0.9533 -- iter: 04928/22500
Training Step: 2542  | total loss: [1m[32m0.11736[0m[0m | time: 27.353s
| Adam | epoch: 008 | loss: 0.11736 - acc: 0.9548 -- iter: 04992/22500
Training Step: 2543  | total loss: [1m[32m0.11504[0m[0m | time: 27.637s
| Adam | epoch: 008 | loss: 0.11504 - acc: 0.9547 -- iter: 05056/22500
Training Step: 2544  | total loss: [1m[32m0.11973[0m[0m | time: 27.919s
| Adam | epoch: 008 | loss: 0.11973 - acc: 0.9545 -- iter: 05120/22500
Training Step: 2545  | total loss: [1m[32m0.11928[0m[0m | time: 28.294s
| Adam | epoch: 008 | loss: 0.11928 - acc: 0.9559 -- iter: 05184/22500
Training Step: 2546  | total loss: [1m[32m0.12155[0m[0m | time: 28.643s
| Adam | epoch: 008 | loss: 0.12155 - acc: 0.9525 -- iter: 05248/22500
Training Step: 2547  | total loss: [1m[32m0.12886[0m[0m | time: 28.986s
| Adam | epoch: 008 | loss: 0.12886 - acc: 0.9526 -- iter: 05312/22500
Training Step: 2548  | total loss: [1m[32m0.12440[0m[0m | time: 29.333s
| Adam | epoch: 008 | loss: 0.12440 - acc: 0.9558 -- iter: 05376/22500
Training Step: 2549  | total loss: [1m[32m0.12100[0m[0m | time: 29.696s
| Adam | epoch: 008 | loss: 0.12100 - acc: 0.9555 -- iter: 05440/22500
Training Step: 2550  | total loss: [1m[32m0.11503[0m[0m | time: 30.070s
| Adam | epoch: 008 | loss: 0.11503 - acc: 0.9584 -- iter: 05504/22500
Training Step: 2551  | total loss: [1m[32m0.12657[0m[0m | time: 30.420s
| Adam | epoch: 008 | loss: 0.12657 - acc: 0.9563 -- iter: 05568/22500
Training Step: 2552  | total loss: [1m[32m0.12922[0m[0m | time: 30.768s
| Adam | epoch: 008 | loss: 0.12922 - acc: 0.9544 -- iter: 05632/22500
Training Step: 2553  | total loss: [1m[32m0.12525[0m[0m | time: 31.122s
| Adam | epoch: 008 | loss: 0.12525 - acc: 0.9559 -- iter: 05696/22500
Training Step: 2554  | total loss: [1m[32m0.13463[0m[0m | time: 31.471s
| Adam | epoch: 008 | loss: 0.13463 - acc: 0.9525 -- iter: 05760/22500
Training Step: 2555  | total loss: [1m[32m0.13287[0m[0m | time: 31.821s
| Adam | epoch: 008 | loss: 0.13287 - acc: 0.9525 -- iter: 05824/22500
Training Step: 2556  | total loss: [1m[32m0.13323[0m[0m | time: 32.167s
| Adam | epoch: 008 | loss: 0.13323 - acc: 0.9526 -- iter: 05888/22500
Training Step: 2557  | total loss: [1m[32m0.12948[0m[0m | time: 32.513s
| Adam | epoch: 008 | loss: 0.12948 - acc: 0.9526 -- iter: 05952/22500
Training Step: 2558  | total loss: [1m[32m0.12893[0m[0m | time: 32.864s
| Adam | epoch: 008 | loss: 0.12893 - acc: 0.9558 -- iter: 06016/22500
Training Step: 2559  | total loss: [1m[32m0.12183[0m[0m | time: 33.219s
| Adam | epoch: 008 | loss: 0.12183 - acc: 0.9587 -- iter: 06080/22500
Training Step: 2560  | total loss: [1m[32m0.12087[0m[0m | time: 33.578s
| Adam | epoch: 008 | loss: 0.12087 - acc: 0.9581 -- iter: 06144/22500
Training Step: 2561  | total loss: [1m[32m0.11908[0m[0m | time: 33.949s
| Adam | epoch: 008 | loss: 0.11908 - acc: 0.9592 -- iter: 06208/22500
Training Step: 2562  | total loss: [1m[32m0.11105[0m[0m | time: 34.299s
| Adam | epoch: 008 | loss: 0.11105 - acc: 0.9633 -- iter: 06272/22500
Training Step: 2563  | total loss: [1m[32m0.10706[0m[0m | time: 34.648s
| Adam | epoch: 008 | loss: 0.10706 - acc: 0.9654 -- iter: 06336/22500
Training Step: 2564  | total loss: [1m[32m0.11440[0m[0m | time: 34.997s
| Adam | epoch: 008 | loss: 0.11440 - acc: 0.9626 -- iter: 06400/22500
Training Step: 2565  | total loss: [1m[32m0.12006[0m[0m | time: 35.346s
| Adam | epoch: 008 | loss: 0.12006 - acc: 0.9569 -- iter: 06464/22500
Training Step: 2566  | total loss: [1m[32m0.11190[0m[0m | time: 35.692s
| Adam | epoch: 008 | loss: 0.11190 - acc: 0.9613 -- iter: 06528/22500
Training Step: 2567  | total loss: [1m[32m0.10806[0m[0m | time: 36.044s
| Adam | epoch: 008 | loss: 0.10806 - acc: 0.9636 -- iter: 06592/22500
Training Step: 2568  | total loss: [1m[32m0.09994[0m[0m | time: 36.387s
| Adam | epoch: 008 | loss: 0.09994 - acc: 0.9672 -- iter: 06656/22500
Training Step: 2569  | total loss: [1m[32m0.09611[0m[0m | time: 36.733s
| Adam | epoch: 008 | loss: 0.09611 - acc: 0.9674 -- iter: 06720/22500
Training Step: 2570  | total loss: [1m[32m0.09492[0m[0m | time: 37.083s
| Adam | epoch: 008 | loss: 0.09492 - acc: 0.9691 -- iter: 06784/22500
Training Step: 2571  | total loss: [1m[32m0.09737[0m[0m | time: 37.430s
| Adam | epoch: 008 | loss: 0.09737 - acc: 0.9690 -- iter: 06848/22500
Training Step: 2572  | total loss: [1m[32m0.09485[0m[0m | time: 37.788s
| Adam | epoch: 008 | loss: 0.09485 - acc: 0.9706 -- iter: 06912/22500
Training Step: 2573  | total loss: [1m[32m0.09188[0m[0m | time: 38.166s
| Adam | epoch: 008 | loss: 0.09188 - acc: 0.9704 -- iter: 06976/22500
Training Step: 2574  | total loss: [1m[32m0.09342[0m[0m | time: 38.511s
| Adam | epoch: 008 | loss: 0.09342 - acc: 0.9687 -- iter: 07040/22500
Training Step: 2575  | total loss: [1m[32m0.11039[0m[0m | time: 38.858s
| Adam | epoch: 008 | loss: 0.11039 - acc: 0.9640 -- iter: 07104/22500
Training Step: 2576  | total loss: [1m[32m0.10733[0m[0m | time: 39.207s
| Adam | epoch: 008 | loss: 0.10733 - acc: 0.9645 -- iter: 07168/22500
Training Step: 2577  | total loss: [1m[32m0.10562[0m[0m | time: 39.552s
| Adam | epoch: 008 | loss: 0.10562 - acc: 0.9649 -- iter: 07232/22500
Training Step: 2578  | total loss: [1m[32m0.11530[0m[0m | time: 39.905s
| Adam | epoch: 008 | loss: 0.11530 - acc: 0.9606 -- iter: 07296/22500
Training Step: 2579  | total loss: [1m[32m0.11011[0m[0m | time: 40.260s
| Adam | epoch: 008 | loss: 0.11011 - acc: 0.9614 -- iter: 07360/22500
Training Step: 2580  | total loss: [1m[32m0.11181[0m[0m | time: 40.612s
| Adam | epoch: 008 | loss: 0.11181 - acc: 0.9637 -- iter: 07424/22500
Training Step: 2581  | total loss: [1m[32m0.11449[0m[0m | time: 40.965s
| Adam | epoch: 008 | loss: 0.11449 - acc: 0.9658 -- iter: 07488/22500
Training Step: 2582  | total loss: [1m[32m0.12210[0m[0m | time: 41.320s
| Adam | epoch: 008 | loss: 0.12210 - acc: 0.9645 -- iter: 07552/22500
Training Step: 2583  | total loss: [1m[32m0.12204[0m[0m | time: 41.689s
| Adam | epoch: 008 | loss: 0.12204 - acc: 0.9618 -- iter: 07616/22500
Training Step: 2584  | total loss: [1m[32m0.12448[0m[0m | time: 42.067s
| Adam | epoch: 008 | loss: 0.12448 - acc: 0.9578 -- iter: 07680/22500
Training Step: 2585  | total loss: [1m[32m0.13317[0m[0m | time: 42.412s
| Adam | epoch: 008 | loss: 0.13317 - acc: 0.9573 -- iter: 07744/22500
Training Step: 2586  | total loss: [1m[32m0.12727[0m[0m | time: 42.763s
| Adam | epoch: 008 | loss: 0.12727 - acc: 0.9585 -- iter: 07808/22500
Training Step: 2587  | total loss: [1m[32m0.12906[0m[0m | time: 43.112s
| Adam | epoch: 008 | loss: 0.12906 - acc: 0.9564 -- iter: 07872/22500
Training Step: 2588  | total loss: [1m[32m0.12413[0m[0m | time: 43.460s
| Adam | epoch: 008 | loss: 0.12413 - acc: 0.9592 -- iter: 07936/22500
Training Step: 2589  | total loss: [1m[32m0.12313[0m[0m | time: 43.810s
| Adam | epoch: 008 | loss: 0.12313 - acc: 0.9601 -- iter: 08000/22500
Training Step: 2590  | total loss: [1m[32m0.12605[0m[0m | time: 44.197s
| Adam | epoch: 008 | loss: 0.12605 - acc: 0.9563 -- iter: 08064/22500
Training Step: 2591  | total loss: [1m[32m0.12789[0m[0m | time: 44.532s
| Adam | epoch: 008 | loss: 0.12789 - acc: 0.9544 -- iter: 08128/22500
Training Step: 2592  | total loss: [1m[32m0.12331[0m[0m | time: 44.894s
| Adam | epoch: 008 | loss: 0.12331 - acc: 0.9559 -- iter: 08192/22500
Training Step: 2593  | total loss: [1m[32m0.12090[0m[0m | time: 45.243s
| Adam | epoch: 008 | loss: 0.12090 - acc: 0.9556 -- iter: 08256/22500
Training Step: 2594  | total loss: [1m[32m0.11710[0m[0m | time: 45.606s
| Adam | epoch: 008 | loss: 0.11710 - acc: 0.9569 -- iter: 08320/22500
Training Step: 2595  | total loss: [1m[32m0.12288[0m[0m | time: 45.974s
| Adam | epoch: 008 | loss: 0.12288 - acc: 0.9534 -- iter: 08384/22500
Training Step: 2596  | total loss: [1m[32m0.12090[0m[0m | time: 46.328s
| Adam | epoch: 008 | loss: 0.12090 - acc: 0.9549 -- iter: 08448/22500
Training Step: 2597  | total loss: [1m[32m0.11868[0m[0m | time: 46.674s
| Adam | epoch: 008 | loss: 0.11868 - acc: 0.9579 -- iter: 08512/22500
Training Step: 2598  | total loss: [1m[32m0.11206[0m[0m | time: 47.021s
| Adam | epoch: 008 | loss: 0.11206 - acc: 0.9590 -- iter: 08576/22500
Training Step: 2599  | total loss: [1m[32m0.12157[0m[0m | time: 47.370s
| Adam | epoch: 008 | loss: 0.12157 - acc: 0.9537 -- iter: 08640/22500
Training Step: 2600  | total loss: [1m[32m0.12386[0m[0m | time: 50.880s
| Adam | epoch: 008 | loss: 0.12386 - acc: 0.9521 | val_loss: 0.81538 - val_acc: 0.7948 -- iter: 08704/22500
--
Training Step: 2601  | total loss: [1m[32m0.12685[0m[0m | time: 51.233s
| Adam | epoch: 008 | loss: 0.12685 - acc: 0.9522 -- iter: 08768/22500
Training Step: 2602  | total loss: [1m[32m0.12793[0m[0m | time: 51.581s
| Adam | epoch: 008 | loss: 0.12793 - acc: 0.9523 -- iter: 08832/22500
Training Step: 2603  | total loss: [1m[32m0.12280[0m[0m | time: 51.954s
| Adam | epoch: 008 | loss: 0.12280 - acc: 0.9555 -- iter: 08896/22500
Training Step: 2604  | total loss: [1m[32m0.12644[0m[0m | time: 52.304s
| Adam | epoch: 008 | loss: 0.12644 - acc: 0.9552 -- iter: 08960/22500
Training Step: 2605  | total loss: [1m[32m0.12822[0m[0m | time: 52.661s
| Adam | epoch: 008 | loss: 0.12822 - acc: 0.9550 -- iter: 09024/22500
Training Step: 2606  | total loss: [1m[32m0.13182[0m[0m | time: 53.047s
| Adam | epoch: 008 | loss: 0.13182 - acc: 0.9548 -- iter: 09088/22500
Training Step: 2607  | total loss: [1m[32m0.13456[0m[0m | time: 53.414s
| Adam | epoch: 008 | loss: 0.13456 - acc: 0.9515 -- iter: 09152/22500
Training Step: 2608  | total loss: [1m[32m0.14454[0m[0m | time: 53.760s
| Adam | epoch: 008 | loss: 0.14454 - acc: 0.9486 -- iter: 09216/22500
Training Step: 2609  | total loss: [1m[32m0.14857[0m[0m | time: 54.108s
| Adam | epoch: 008 | loss: 0.14857 - acc: 0.9397 -- iter: 09280/22500
Training Step: 2610  | total loss: [1m[32m0.15045[0m[0m | time: 54.452s
| Adam | epoch: 008 | loss: 0.15045 - acc: 0.9379 -- iter: 09344/22500
Training Step: 2611  | total loss: [1m[32m0.16213[0m[0m | time: 54.799s
| Adam | epoch: 008 | loss: 0.16213 - acc: 0.9347 -- iter: 09408/22500
Training Step: 2612  | total loss: [1m[32m0.15953[0m[0m | time: 55.154s
| Adam | epoch: 008 | loss: 0.15953 - acc: 0.9350 -- iter: 09472/22500
Training Step: 2613  | total loss: [1m[32m0.15855[0m[0m | time: 55.501s
| Adam | epoch: 008 | loss: 0.15855 - acc: 0.9384 -- iter: 09536/22500
Training Step: 2614  | total loss: [1m[32m0.14960[0m[0m | time: 55.849s
| Adam | epoch: 008 | loss: 0.14960 - acc: 0.9414 -- iter: 09600/22500
Training Step: 2615  | total loss: [1m[32m0.15196[0m[0m | time: 56.202s
| Adam | epoch: 008 | loss: 0.15196 - acc: 0.9379 -- iter: 09664/22500
Training Step: 2616  | total loss: [1m[32m0.16373[0m[0m | time: 56.551s
| Adam | epoch: 008 | loss: 0.16373 - acc: 0.9363 -- iter: 09728/22500
Training Step: 2617  | total loss: [1m[32m0.16206[0m[0m | time: 56.922s
| Adam | epoch: 008 | loss: 0.16206 - acc: 0.9349 -- iter: 09792/22500
Training Step: 2618  | total loss: [1m[32m0.15861[0m[0m | time: 57.295s
| Adam | epoch: 008 | loss: 0.15861 - acc: 0.9351 -- iter: 09856/22500
Training Step: 2619  | total loss: [1m[32m0.14829[0m[0m | time: 57.641s
| Adam | epoch: 008 | loss: 0.14829 - acc: 0.9416 -- iter: 09920/22500
Training Step: 2620  | total loss: [1m[32m0.14930[0m[0m | time: 57.997s
| Adam | epoch: 008 | loss: 0.14930 - acc: 0.9412 -- iter: 09984/22500
Training Step: 2621  | total loss: [1m[32m0.14377[0m[0m | time: 58.345s
| Adam | epoch: 008 | loss: 0.14377 - acc: 0.9424 -- iter: 10048/22500
Training Step: 2622  | total loss: [1m[32m0.13522[0m[0m | time: 58.695s
| Adam | epoch: 008 | loss: 0.13522 - acc: 0.9466 -- iter: 10112/22500
Training Step: 2623  | total loss: [1m[32m0.13854[0m[0m | time: 59.046s
| Adam | epoch: 008 | loss: 0.13854 - acc: 0.9472 -- iter: 10176/22500
Training Step: 2624  | total loss: [1m[32m0.14232[0m[0m | time: 59.391s
| Adam | epoch: 008 | loss: 0.14232 - acc: 0.9494 -- iter: 10240/22500
Training Step: 2625  | total loss: [1m[32m0.13645[0m[0m | time: 59.737s
| Adam | epoch: 008 | loss: 0.13645 - acc: 0.9513 -- iter: 10304/22500
Training Step: 2626  | total loss: [1m[32m0.13546[0m[0m | time: 60.086s
| Adam | epoch: 008 | loss: 0.13546 - acc: 0.9499 -- iter: 10368/22500
Training Step: 2627  | total loss: [1m[32m0.12803[0m[0m | time: 60.432s
| Adam | epoch: 008 | loss: 0.12803 - acc: 0.9534 -- iter: 10432/22500
Training Step: 2628  | total loss: [1m[32m0.14200[0m[0m | time: 60.796s
| Adam | epoch: 008 | loss: 0.14200 - acc: 0.9455 -- iter: 10496/22500
Training Step: 2629  | total loss: [1m[32m0.13820[0m[0m | time: 61.169s
| Adam | epoch: 008 | loss: 0.13820 - acc: 0.9463 -- iter: 10560/22500
Training Step: 2630  | total loss: [1m[32m0.13163[0m[0m | time: 61.521s
| Adam | epoch: 008 | loss: 0.13163 - acc: 0.9517 -- iter: 10624/22500
Training Step: 2631  | total loss: [1m[32m0.14353[0m[0m | time: 61.864s
| Adam | epoch: 008 | loss: 0.14353 - acc: 0.9487 -- iter: 10688/22500
Training Step: 2632  | total loss: [1m[32m0.14792[0m[0m | time: 62.213s
| Adam | epoch: 008 | loss: 0.14792 - acc: 0.9476 -- iter: 10752/22500
Training Step: 2633  | total loss: [1m[32m0.14070[0m[0m | time: 62.566s
| Adam | epoch: 008 | loss: 0.14070 - acc: 0.9497 -- iter: 10816/22500
Training Step: 2634  | total loss: [1m[32m0.14672[0m[0m | time: 62.915s
| Adam | epoch: 008 | loss: 0.14672 - acc: 0.9516 -- iter: 10880/22500
Training Step: 2635  | total loss: [1m[32m0.13917[0m[0m | time: 63.261s
| Adam | epoch: 008 | loss: 0.13917 - acc: 0.9549 -- iter: 10944/22500
Training Step: 2636  | total loss: [1m[32m0.13469[0m[0m | time: 63.611s
| Adam | epoch: 008 | loss: 0.13469 - acc: 0.9563 -- iter: 11008/22500
Training Step: 2637  | total loss: [1m[32m0.14601[0m[0m | time: 63.958s
| Adam | epoch: 008 | loss: 0.14601 - acc: 0.9528 -- iter: 11072/22500
Training Step: 2638  | total loss: [1m[32m0.14307[0m[0m | time: 64.308s
| Adam | epoch: 008 | loss: 0.14307 - acc: 0.9544 -- iter: 11136/22500
Training Step: 2639  | total loss: [1m[32m0.14228[0m[0m | time: 64.657s
| Adam | epoch: 008 | loss: 0.14228 - acc: 0.9543 -- iter: 11200/22500
Training Step: 2640  | total loss: [1m[32m0.14129[0m[0m | time: 65.024s
| Adam | epoch: 008 | loss: 0.14129 - acc: 0.9542 -- iter: 11264/22500
Training Step: 2641  | total loss: [1m[32m0.13558[0m[0m | time: 65.369s
| Adam | epoch: 008 | loss: 0.13558 - acc: 0.9556 -- iter: 11328/22500
Training Step: 2642  | total loss: [1m[32m0.14044[0m[0m | time: 65.656s
| Adam | epoch: 008 | loss: 0.14044 - acc: 0.9523 -- iter: 11392/22500
Training Step: 2643  | total loss: [1m[32m0.14588[0m[0m | time: 65.939s
| Adam | epoch: 008 | loss: 0.14588 - acc: 0.9492 -- iter: 11456/22500
Training Step: 2644  | total loss: [1m[32m0.14264[0m[0m | time: 66.225s
| Adam | epoch: 008 | loss: 0.14264 - acc: 0.9527 -- iter: 11520/22500
Training Step: 2645  | total loss: [1m[32m0.14108[0m[0m | time: 66.509s
| Adam | epoch: 008 | loss: 0.14108 - acc: 0.9543 -- iter: 11584/22500
Training Step: 2646  | total loss: [1m[32m0.13829[0m[0m | time: 66.868s
| Adam | epoch: 008 | loss: 0.13829 - acc: 0.9573 -- iter: 11648/22500
Training Step: 2647  | total loss: [1m[32m0.13144[0m[0m | time: 67.217s
| Adam | epoch: 008 | loss: 0.13144 - acc: 0.9600 -- iter: 11712/22500
Training Step: 2648  | total loss: [1m[32m0.13368[0m[0m | time: 67.566s
| Adam | epoch: 008 | loss: 0.13368 - acc: 0.9594 -- iter: 11776/22500
Training Step: 2649  | total loss: [1m[32m0.13569[0m[0m | time: 67.974s
| Adam | epoch: 008 | loss: 0.13569 - acc: 0.9587 -- iter: 11840/22500
Training Step: 2650  | total loss: [1m[32m0.13339[0m[0m | time: 68.307s
| Adam | epoch: 008 | loss: 0.13339 - acc: 0.9597 -- iter: 11904/22500
Training Step: 2651  | total loss: [1m[32m0.13069[0m[0m | time: 68.690s
| Adam | epoch: 008 | loss: 0.13069 - acc: 0.9591 -- iter: 11968/22500
Training Step: 2652  | total loss: [1m[32m0.13531[0m[0m | time: 69.072s
| Adam | epoch: 008 | loss: 0.13531 - acc: 0.9538 -- iter: 12032/22500
Training Step: 2653  | total loss: [1m[32m0.14522[0m[0m | time: 69.423s
| Adam | epoch: 008 | loss: 0.14522 - acc: 0.9475 -- iter: 12096/22500
Training Step: 2654  | total loss: [1m[32m0.13441[0m[0m | time: 69.779s
| Adam | epoch: 008 | loss: 0.13441 - acc: 0.9527 -- iter: 12160/22500
Training Step: 2655  | total loss: [1m[32m0.14414[0m[0m | time: 70.130s
| Adam | epoch: 008 | loss: 0.14414 - acc: 0.9496 -- iter: 12224/22500
Training Step: 2656  | total loss: [1m[32m0.13824[0m[0m | time: 70.480s
| Adam | epoch: 008 | loss: 0.13824 - acc: 0.9516 -- iter: 12288/22500
Training Step: 2657  | total loss: [1m[32m0.15331[0m[0m | time: 70.831s
| Adam | epoch: 008 | loss: 0.15331 - acc: 0.9470 -- iter: 12352/22500
Training Step: 2658  | total loss: [1m[32m0.15295[0m[0m | time: 71.181s
| Adam | epoch: 008 | loss: 0.15295 - acc: 0.9461 -- iter: 12416/22500
Training Step: 2659  | total loss: [1m[32m0.16114[0m[0m | time: 71.525s
| Adam | epoch: 008 | loss: 0.16114 - acc: 0.9436 -- iter: 12480/22500
Training Step: 2660  | total loss: [1m[32m0.15617[0m[0m | time: 71.883s
| Adam | epoch: 008 | loss: 0.15617 - acc: 0.9462 -- iter: 12544/22500
Training Step: 2661  | total loss: [1m[32m0.15362[0m[0m | time: 72.234s
| Adam | epoch: 008 | loss: 0.15362 - acc: 0.9484 -- iter: 12608/22500
Training Step: 2662  | total loss: [1m[32m0.16332[0m[0m | time: 72.602s
| Adam | epoch: 008 | loss: 0.16332 - acc: 0.9442 -- iter: 12672/22500
Training Step: 2663  | total loss: [1m[32m0.15754[0m[0m | time: 72.967s
| Adam | epoch: 008 | loss: 0.15754 - acc: 0.9451 -- iter: 12736/22500
Training Step: 2664  | total loss: [1m[32m0.14852[0m[0m | time: 73.319s
| Adam | epoch: 008 | loss: 0.14852 - acc: 0.9490 -- iter: 12800/22500
Training Step: 2665  | total loss: [1m[32m0.15032[0m[0m | time: 73.670s
| Adam | epoch: 008 | loss: 0.15032 - acc: 0.9463 -- iter: 12864/22500
Training Step: 2666  | total loss: [1m[32m0.14437[0m[0m | time: 74.019s
| Adam | epoch: 008 | loss: 0.14437 - acc: 0.9501 -- iter: 12928/22500
Training Step: 2667  | total loss: [1m[32m0.13557[0m[0m | time: 74.368s
| Adam | epoch: 008 | loss: 0.13557 - acc: 0.9535 -- iter: 12992/22500
Training Step: 2668  | total loss: [1m[32m0.13426[0m[0m | time: 74.715s
| Adam | epoch: 008 | loss: 0.13426 - acc: 0.9504 -- iter: 13056/22500
Training Step: 2669  | total loss: [1m[32m0.13589[0m[0m | time: 75.068s
| Adam | epoch: 008 | loss: 0.13589 - acc: 0.9506 -- iter: 13120/22500
Training Step: 2670  | total loss: [1m[32m0.13903[0m[0m | time: 75.415s
| Adam | epoch: 008 | loss: 0.13903 - acc: 0.9509 -- iter: 13184/22500
Training Step: 2671  | total loss: [1m[32m0.13820[0m[0m | time: 75.764s
| Adam | epoch: 008 | loss: 0.13820 - acc: 0.9496 -- iter: 13248/22500
Training Step: 2672  | total loss: [1m[32m0.14120[0m[0m | time: 76.111s
| Adam | epoch: 008 | loss: 0.14120 - acc: 0.9452 -- iter: 13312/22500
Training Step: 2673  | total loss: [1m[32m0.14077[0m[0m | time: 76.484s
| Adam | epoch: 008 | loss: 0.14077 - acc: 0.9476 -- iter: 13376/22500
Training Step: 2674  | total loss: [1m[32m0.13964[0m[0m | time: 76.852s
| Adam | epoch: 008 | loss: 0.13964 - acc: 0.9466 -- iter: 13440/22500
Training Step: 2675  | total loss: [1m[32m0.15176[0m[0m | time: 77.198s
| Adam | epoch: 008 | loss: 0.15176 - acc: 0.9394 -- iter: 13504/22500
Training Step: 2676  | total loss: [1m[32m0.15125[0m[0m | time: 77.544s
| Adam | epoch: 008 | loss: 0.15125 - acc: 0.9392 -- iter: 13568/22500
Training Step: 2677  | total loss: [1m[32m0.15000[0m[0m | time: 77.895s
| Adam | epoch: 008 | loss: 0.15000 - acc: 0.9390 -- iter: 13632/22500
Training Step: 2678  | total loss: [1m[32m0.14594[0m[0m | time: 78.245s
| Adam | epoch: 008 | loss: 0.14594 - acc: 0.9389 -- iter: 13696/22500
Training Step: 2679  | total loss: [1m[32m0.14442[0m[0m | time: 78.591s
| Adam | epoch: 008 | loss: 0.14442 - acc: 0.9403 -- iter: 13760/22500
Training Step: 2680  | total loss: [1m[32m0.14372[0m[0m | time: 78.939s
| Adam | epoch: 008 | loss: 0.14372 - acc: 0.9432 -- iter: 13824/22500
Training Step: 2681  | total loss: [1m[32m0.15588[0m[0m | time: 79.290s
| Adam | epoch: 008 | loss: 0.15588 - acc: 0.9395 -- iter: 13888/22500
Training Step: 2682  | total loss: [1m[32m0.14922[0m[0m | time: 79.640s
| Adam | epoch: 008 | loss: 0.14922 - acc: 0.9440 -- iter: 13952/22500
Training Step: 2683  | total loss: [1m[32m0.15129[0m[0m | time: 79.990s
| Adam | epoch: 008 | loss: 0.15129 - acc: 0.9449 -- iter: 14016/22500
Training Step: 2684  | total loss: [1m[32m0.15192[0m[0m | time: 80.340s
| Adam | epoch: 008 | loss: 0.15192 - acc: 0.9441 -- iter: 14080/22500
Training Step: 2685  | total loss: [1m[32m0.14957[0m[0m | time: 80.704s
| Adam | epoch: 008 | loss: 0.14957 - acc: 0.9482 -- iter: 14144/22500
Training Step: 2686  | total loss: [1m[32m0.14287[0m[0m | time: 81.075s
| Adam | epoch: 008 | loss: 0.14287 - acc: 0.9502 -- iter: 14208/22500
Training Step: 2687  | total loss: [1m[32m0.14868[0m[0m | time: 81.426s
| Adam | epoch: 008 | loss: 0.14868 - acc: 0.9474 -- iter: 14272/22500
Training Step: 2688  | total loss: [1m[32m0.14765[0m[0m | time: 81.776s
| Adam | epoch: 008 | loss: 0.14765 - acc: 0.9464 -- iter: 14336/22500
Training Step: 2689  | total loss: [1m[32m0.15670[0m[0m | time: 82.128s
| Adam | epoch: 008 | loss: 0.15670 - acc: 0.9424 -- iter: 14400/22500
Training Step: 2690  | total loss: [1m[32m0.15129[0m[0m | time: 82.480s
| Adam | epoch: 008 | loss: 0.15129 - acc: 0.9450 -- iter: 14464/22500
Training Step: 2691  | total loss: [1m[32m0.15137[0m[0m | time: 82.828s
| Adam | epoch: 008 | loss: 0.15137 - acc: 0.9474 -- iter: 14528/22500
Training Step: 2692  | total loss: [1m[32m0.15114[0m[0m | time: 83.183s
| Adam | epoch: 008 | loss: 0.15114 - acc: 0.9480 -- iter: 14592/22500
Training Step: 2693  | total loss: [1m[32m0.15806[0m[0m | time: 83.530s
| Adam | epoch: 008 | loss: 0.15806 - acc: 0.9485 -- iter: 14656/22500
Training Step: 2694  | total loss: [1m[32m0.15558[0m[0m | time: 83.872s
| Adam | epoch: 008 | loss: 0.15558 - acc: 0.9474 -- iter: 14720/22500
Training Step: 2695  | total loss: [1m[32m0.15333[0m[0m | time: 84.225s
| Adam | epoch: 008 | loss: 0.15333 - acc: 0.9495 -- iter: 14784/22500
Training Step: 2696  | total loss: [1m[32m0.15496[0m[0m | time: 84.594s
| Adam | epoch: 008 | loss: 0.15496 - acc: 0.9499 -- iter: 14848/22500
Training Step: 2697  | total loss: [1m[32m0.16015[0m[0m | time: 84.963s
| Adam | epoch: 008 | loss: 0.16015 - acc: 0.9455 -- iter: 14912/22500
Training Step: 2698  | total loss: [1m[32m0.16227[0m[0m | time: 85.314s
| Adam | epoch: 008 | loss: 0.16227 - acc: 0.9416 -- iter: 14976/22500
Training Step: 2699  | total loss: [1m[32m0.15883[0m[0m | time: 85.664s
| Adam | epoch: 008 | loss: 0.15883 - acc: 0.9427 -- iter: 15040/22500
Training Step: 2700  | total loss: [1m[32m0.15057[0m[0m | time: 86.011s
| Adam | epoch: 008 | loss: 0.15057 - acc: 0.9469 -- iter: 15104/22500
Training Step: 2701  | total loss: [1m[32m0.14896[0m[0m | time: 86.359s
| Adam | epoch: 008 | loss: 0.14896 - acc: 0.9491 -- iter: 15168/22500
Training Step: 2702  | total loss: [1m[32m0.14559[0m[0m | time: 86.717s
| Adam | epoch: 008 | loss: 0.14559 - acc: 0.9495 -- iter: 15232/22500
Training Step: 2703  | total loss: [1m[32m0.14500[0m[0m | time: 87.063s
| Adam | epoch: 008 | loss: 0.14500 - acc: 0.9499 -- iter: 15296/22500
Training Step: 2704  | total loss: [1m[32m0.14935[0m[0m | time: 87.408s
| Adam | epoch: 008 | loss: 0.14935 - acc: 0.9502 -- iter: 15360/22500
Training Step: 2705  | total loss: [1m[32m0.15291[0m[0m | time: 87.752s
| Adam | epoch: 008 | loss: 0.15291 - acc: 0.9458 -- iter: 15424/22500
Training Step: 2706  | total loss: [1m[32m0.16458[0m[0m | time: 88.105s
| Adam | epoch: 008 | loss: 0.16458 - acc: 0.9418 -- iter: 15488/22500
Training Step: 2707  | total loss: [1m[32m0.16212[0m[0m | time: 88.477s
| Adam | epoch: 008 | loss: 0.16212 - acc: 0.9430 -- iter: 15552/22500
Training Step: 2708  | total loss: [1m[32m0.15356[0m[0m | time: 88.845s
| Adam | epoch: 008 | loss: 0.15356 - acc: 0.9455 -- iter: 15616/22500
Training Step: 2709  | total loss: [1m[32m0.15593[0m[0m | time: 89.195s
| Adam | epoch: 008 | loss: 0.15593 - acc: 0.9432 -- iter: 15680/22500
Training Step: 2710  | total loss: [1m[32m0.15900[0m[0m | time: 89.544s
| Adam | epoch: 008 | loss: 0.15900 - acc: 0.9457 -- iter: 15744/22500
Training Step: 2711  | total loss: [1m[32m0.15862[0m[0m | time: 89.892s
| Adam | epoch: 008 | loss: 0.15862 - acc: 0.9480 -- iter: 15808/22500
Training Step: 2712  | total loss: [1m[32m0.15341[0m[0m | time: 90.243s
| Adam | epoch: 008 | loss: 0.15341 - acc: 0.9501 -- iter: 15872/22500
Training Step: 2713  | total loss: [1m[32m0.15508[0m[0m | time: 90.589s
| Adam | epoch: 008 | loss: 0.15508 - acc: 0.9504 -- iter: 15936/22500
Training Step: 2714  | total loss: [1m[32m0.14840[0m[0m | time: 90.935s
| Adam | epoch: 008 | loss: 0.14840 - acc: 0.9538 -- iter: 16000/22500
Training Step: 2715  | total loss: [1m[32m0.14384[0m[0m | time: 91.285s
| Adam | epoch: 008 | loss: 0.14384 - acc: 0.9569 -- iter: 16064/22500
Training Step: 2716  | total loss: [1m[32m0.14689[0m[0m | time: 91.638s
| Adam | epoch: 008 | loss: 0.14689 - acc: 0.9565 -- iter: 16128/22500
Training Step: 2717  | total loss: [1m[32m0.14565[0m[0m | time: 91.985s
| Adam | epoch: 008 | loss: 0.14565 - acc: 0.9593 -- iter: 16192/22500
Training Step: 2718  | total loss: [1m[32m0.14236[0m[0m | time: 92.357s
| Adam | epoch: 008 | loss: 0.14236 - acc: 0.9602 -- iter: 16256/22500
Training Step: 2719  | total loss: [1m[32m0.14565[0m[0m | time: 92.728s
| Adam | epoch: 008 | loss: 0.14565 - acc: 0.9595 -- iter: 16320/22500
Training Step: 2720  | total loss: [1m[32m0.13663[0m[0m | time: 93.079s
| Adam | epoch: 008 | loss: 0.13663 - acc: 0.9620 -- iter: 16384/22500
Training Step: 2721  | total loss: [1m[32m0.13404[0m[0m | time: 93.435s
| Adam | epoch: 008 | loss: 0.13404 - acc: 0.9611 -- iter: 16448/22500
Training Step: 2722  | total loss: [1m[32m0.13419[0m[0m | time: 93.783s
| Adam | epoch: 008 | loss: 0.13419 - acc: 0.9603 -- iter: 16512/22500
Training Step: 2723  | total loss: [1m[32m0.12862[0m[0m | time: 94.133s
| Adam | epoch: 008 | loss: 0.12862 - acc: 0.9643 -- iter: 16576/22500
Training Step: 2724  | total loss: [1m[32m0.13242[0m[0m | time: 94.501s
| Adam | epoch: 008 | loss: 0.13242 - acc: 0.9632 -- iter: 16640/22500
Training Step: 2725  | total loss: [1m[32m0.13163[0m[0m | time: 94.864s
| Adam | epoch: 008 | loss: 0.13163 - acc: 0.9606 -- iter: 16704/22500
Training Step: 2726  | total loss: [1m[32m0.14286[0m[0m | time: 95.220s
| Adam | epoch: 008 | loss: 0.14286 - acc: 0.9567 -- iter: 16768/22500
Training Step: 2727  | total loss: [1m[32m0.13805[0m[0m | time: 95.568s
| Adam | epoch: 008 | loss: 0.13805 - acc: 0.9564 -- iter: 16832/22500
Training Step: 2728  | total loss: [1m[32m0.13851[0m[0m | time: 95.918s
| Adam | epoch: 008 | loss: 0.13851 - acc: 0.9560 -- iter: 16896/22500
Training Step: 2729  | total loss: [1m[32m0.13998[0m[0m | time: 96.270s
| Adam | epoch: 008 | loss: 0.13998 - acc: 0.9573 -- iter: 16960/22500
Training Step: 2730  | total loss: [1m[32m0.15549[0m[0m | time: 96.638s
| Adam | epoch: 008 | loss: 0.15549 - acc: 0.9569 -- iter: 17024/22500
Training Step: 2731  | total loss: [1m[32m0.17367[0m[0m | time: 97.008s
| Adam | epoch: 008 | loss: 0.17367 - acc: 0.9503 -- iter: 17088/22500
Training Step: 2732  | total loss: [1m[32m0.17464[0m[0m | time: 97.359s
| Adam | epoch: 008 | loss: 0.17464 - acc: 0.9490 -- iter: 17152/22500
Training Step: 2733  | total loss: [1m[32m0.16263[0m[0m | time: 97.707s
| Adam | epoch: 008 | loss: 0.16263 - acc: 0.9541 -- iter: 17216/22500
Training Step: 2734  | total loss: [1m[32m0.15813[0m[0m | time: 98.059s
| Adam | epoch: 008 | loss: 0.15813 - acc: 0.9571 -- iter: 17280/22500
Training Step: 2735  | total loss: [1m[32m0.15664[0m[0m | time: 98.407s
| Adam | epoch: 008 | loss: 0.15664 - acc: 0.9536 -- iter: 17344/22500
Training Step: 2736  | total loss: [1m[32m0.14764[0m[0m | time: 98.753s
| Adam | epoch: 008 | loss: 0.14764 - acc: 0.9567 -- iter: 17408/22500
Training Step: 2737  | total loss: [1m[32m0.14562[0m[0m | time: 99.102s
| Adam | epoch: 008 | loss: 0.14562 - acc: 0.9579 -- iter: 17472/22500
Training Step: 2738  | total loss: [1m[32m0.14938[0m[0m | time: 99.451s
| Adam | epoch: 008 | loss: 0.14938 - acc: 0.9558 -- iter: 17536/22500
Training Step: 2739  | total loss: [1m[32m0.14749[0m[0m | time: 99.799s
| Adam | epoch: 008 | loss: 0.14749 - acc: 0.9556 -- iter: 17600/22500
Training Step: 2740  | total loss: [1m[32m0.14736[0m[0m | time: 100.144s
| Adam | epoch: 008 | loss: 0.14736 - acc: 0.9569 -- iter: 17664/22500
Training Step: 2741  | total loss: [1m[32m0.14480[0m[0m | time: 100.509s
| Adam | epoch: 008 | loss: 0.14480 - acc: 0.9581 -- iter: 17728/22500
Training Step: 2742  | total loss: [1m[32m0.13834[0m[0m | time: 100.892s
| Adam | epoch: 008 | loss: 0.13834 - acc: 0.9591 -- iter: 17792/22500
Training Step: 2743  | total loss: [1m[32m0.14739[0m[0m | time: 101.210s
| Adam | epoch: 008 | loss: 0.14739 - acc: 0.9570 -- iter: 17856/22500
Training Step: 2744  | total loss: [1m[32m0.15030[0m[0m | time: 101.529s
| Adam | epoch: 008 | loss: 0.15030 - acc: 0.9550 -- iter: 17920/22500
Training Step: 2745  | total loss: [1m[32m0.15393[0m[0m | time: 101.855s
| Adam | epoch: 008 | loss: 0.15393 - acc: 0.9533 -- iter: 17984/22500
Training Step: 2746  | total loss: [1m[32m0.15508[0m[0m | time: 102.182s
| Adam | epoch: 008 | loss: 0.15508 - acc: 0.9548 -- iter: 18048/22500
Training Step: 2747  | total loss: [1m[32m0.15194[0m[0m | time: 102.524s
| Adam | epoch: 008 | loss: 0.15194 - acc: 0.9531 -- iter: 18112/22500
Training Step: 2748  | total loss: [1m[32m0.15450[0m[0m | time: 102.871s
| Adam | epoch: 008 | loss: 0.15450 - acc: 0.9515 -- iter: 18176/22500
Training Step: 2749  | total loss: [1m[32m0.14916[0m[0m | time: 103.217s
| Adam | epoch: 008 | loss: 0.14916 - acc: 0.9517 -- iter: 18240/22500
Training Step: 2750  | total loss: [1m[32m0.15074[0m[0m | time: 103.567s
| Adam | epoch: 008 | loss: 0.15074 - acc: 0.9503 -- iter: 18304/22500
Training Step: 2751  | total loss: [1m[32m0.15430[0m[0m | time: 103.913s
| Adam | epoch: 008 | loss: 0.15430 - acc: 0.9474 -- iter: 18368/22500
Training Step: 2752  | total loss: [1m[32m0.15812[0m[0m | time: 104.277s
| Adam | epoch: 008 | loss: 0.15812 - acc: 0.9418 -- iter: 18432/22500
Training Step: 2753  | total loss: [1m[32m0.16310[0m[0m | time: 104.649s
| Adam | epoch: 008 | loss: 0.16310 - acc: 0.9366 -- iter: 18496/22500
Training Step: 2754  | total loss: [1m[32m0.15309[0m[0m | time: 104.996s
| Adam | epoch: 008 | loss: 0.15309 - acc: 0.9414 -- iter: 18560/22500
Training Step: 2755  | total loss: [1m[32m0.15332[0m[0m | time: 105.344s
| Adam | epoch: 008 | loss: 0.15332 - acc: 0.9426 -- iter: 18624/22500
Training Step: 2756  | total loss: [1m[32m0.14419[0m[0m | time: 105.692s
| Adam | epoch: 008 | loss: 0.14419 - acc: 0.9452 -- iter: 18688/22500
Training Step: 2757  | total loss: [1m[32m0.14279[0m[0m | time: 106.043s
| Adam | epoch: 008 | loss: 0.14279 - acc: 0.9476 -- iter: 18752/22500
Training Step: 2758  | total loss: [1m[32m0.13472[0m[0m | time: 106.391s
| Adam | epoch: 008 | loss: 0.13472 - acc: 0.9497 -- iter: 18816/22500
Training Step: 2759  | total loss: [1m[32m0.13330[0m[0m | time: 106.736s
| Adam | epoch: 008 | loss: 0.13330 - acc: 0.9485 -- iter: 18880/22500
Training Step: 2760  | total loss: [1m[32m0.13080[0m[0m | time: 107.087s
| Adam | epoch: 008 | loss: 0.13080 - acc: 0.9505 -- iter: 18944/22500
Training Step: 2761  | total loss: [1m[32m0.13226[0m[0m | time: 107.437s
| Adam | epoch: 008 | loss: 0.13226 - acc: 0.9508 -- iter: 19008/22500
Training Step: 2762  | total loss: [1m[32m0.13442[0m[0m | time: 107.785s
| Adam | epoch: 008 | loss: 0.13442 - acc: 0.9479 -- iter: 19072/22500
Training Step: 2763  | total loss: [1m[32m0.12797[0m[0m | time: 108.155s
| Adam | epoch: 008 | loss: 0.12797 - acc: 0.9515 -- iter: 19136/22500
Training Step: 2764  | total loss: [1m[32m0.11899[0m[0m | time: 108.524s
| Adam | epoch: 008 | loss: 0.11899 - acc: 0.9564 -- iter: 19200/22500
Training Step: 2765  | total loss: [1m[32m0.14411[0m[0m | time: 108.870s
| Adam | epoch: 008 | loss: 0.14411 - acc: 0.9514 -- iter: 19264/22500
Training Step: 2766  | total loss: [1m[32m0.14304[0m[0m | time: 109.218s
| Adam | epoch: 008 | loss: 0.14304 - acc: 0.9531 -- iter: 19328/22500
Training Step: 2767  | total loss: [1m[32m0.14591[0m[0m | time: 109.564s
| Adam | epoch: 008 | loss: 0.14591 - acc: 0.9547 -- iter: 19392/22500
Training Step: 2768  | total loss: [1m[32m0.14378[0m[0m | time: 109.914s
| Adam | epoch: 008 | loss: 0.14378 - acc: 0.9545 -- iter: 19456/22500
Training Step: 2769  | total loss: [1m[32m0.14296[0m[0m | time: 110.265s
| Adam | epoch: 008 | loss: 0.14296 - acc: 0.9559 -- iter: 19520/22500
Training Step: 2770  | total loss: [1m[32m0.13747[0m[0m | time: 110.613s
| Adam | epoch: 008 | loss: 0.13747 - acc: 0.9572 -- iter: 19584/22500
Training Step: 2771  | total loss: [1m[32m0.14563[0m[0m | time: 110.958s
| Adam | epoch: 008 | loss: 0.14563 - acc: 0.9552 -- iter: 19648/22500
Training Step: 2772  | total loss: [1m[32m0.14234[0m[0m | time: 111.308s
| Adam | epoch: 008 | loss: 0.14234 - acc: 0.9550 -- iter: 19712/22500
Training Step: 2773  | total loss: [1m[32m0.14200[0m[0m | time: 111.681s
| Adam | epoch: 008 | loss: 0.14200 - acc: 0.9564 -- iter: 19776/22500
Training Step: 2774  | total loss: [1m[32m0.14694[0m[0m | time: 112.029s
| Adam | epoch: 008 | loss: 0.14694 - acc: 0.9529 -- iter: 19840/22500
Training Step: 2775  | total loss: [1m[32m0.15399[0m[0m | time: 112.405s
| Adam | epoch: 008 | loss: 0.15399 - acc: 0.9514 -- iter: 19904/22500
Training Step: 2776  | total loss: [1m[32m0.15176[0m[0m | time: 112.777s
| Adam | epoch: 008 | loss: 0.15176 - acc: 0.9516 -- iter: 19968/22500
Training Step: 2777  | total loss: [1m[32m0.16315[0m[0m | time: 113.125s
| Adam | epoch: 008 | loss: 0.16315 - acc: 0.9455 -- iter: 20032/22500
Training Step: 2778  | total loss: [1m[32m0.15902[0m[0m | time: 113.477s
| Adam | epoch: 008 | loss: 0.15902 - acc: 0.9462 -- iter: 20096/22500
Training Step: 2779  | total loss: [1m[32m0.16494[0m[0m | time: 113.823s
| Adam | epoch: 008 | loss: 0.16494 - acc: 0.9454 -- iter: 20160/22500
Training Step: 2780  | total loss: [1m[32m0.15254[0m[0m | time: 114.168s
| Adam | epoch: 008 | loss: 0.15254 - acc: 0.9508 -- iter: 20224/22500
Training Step: 2781  | total loss: [1m[32m0.15070[0m[0m | time: 114.521s
| Adam | epoch: 008 | loss: 0.15070 - acc: 0.9511 -- iter: 20288/22500
Training Step: 2782  | total loss: [1m[32m0.15013[0m[0m | time: 114.869s
| Adam | epoch: 008 | loss: 0.15013 - acc: 0.9513 -- iter: 20352/22500
Training Step: 2783  | total loss: [1m[32m0.14687[0m[0m | time: 115.220s
| Adam | epoch: 008 | loss: 0.14687 - acc: 0.9499 -- iter: 20416/22500
Training Step: 2784  | total loss: [1m[32m0.14613[0m[0m | time: 115.573s
| Adam | epoch: 008 | loss: 0.14613 - acc: 0.9518 -- iter: 20480/22500
Training Step: 2785  | total loss: [1m[32m0.14742[0m[0m | time: 115.918s
| Adam | epoch: 008 | loss: 0.14742 - acc: 0.9503 -- iter: 20544/22500
Training Step: 2786  | total loss: [1m[32m0.14579[0m[0m | time: 116.282s
| Adam | epoch: 008 | loss: 0.14579 - acc: 0.9522 -- iter: 20608/22500
Training Step: 2787  | total loss: [1m[32m0.13897[0m[0m | time: 116.657s
| Adam | epoch: 008 | loss: 0.13897 - acc: 0.9554 -- iter: 20672/22500
Training Step: 2788  | total loss: [1m[32m0.14218[0m[0m | time: 117.003s
| Adam | epoch: 008 | loss: 0.14218 - acc: 0.9536 -- iter: 20736/22500
Training Step: 2789  | total loss: [1m[32m0.13509[0m[0m | time: 117.352s
| Adam | epoch: 008 | loss: 0.13509 - acc: 0.9536 -- iter: 20800/22500
Training Step: 2790  | total loss: [1m[32m0.12829[0m[0m | time: 117.702s
| Adam | epoch: 008 | loss: 0.12829 - acc: 0.9582 -- iter: 20864/22500
Training Step: 2791  | total loss: [1m[32m0.12728[0m[0m | time: 118.054s
| Adam | epoch: 008 | loss: 0.12728 - acc: 0.9577 -- iter: 20928/22500
Training Step: 2792  | total loss: [1m[32m0.33530[0m[0m | time: 118.404s
| Adam | epoch: 008 | loss: 0.33530 - acc: 0.9135 -- iter: 20992/22500
Training Step: 2793  | total loss: [1m[32m0.31217[0m[0m | time: 118.749s
| Adam | epoch: 008 | loss: 0.31217 - acc: 0.9190 -- iter: 21056/22500
Training Step: 2794  | total loss: [1m[32m0.29522[0m[0m | time: 119.097s
| Adam | epoch: 008 | loss: 0.29522 - acc: 0.9240 -- iter: 21120/22500
Training Step: 2795  | total loss: [1m[32m0.27482[0m[0m | time: 119.452s
| Adam | epoch: 008 | loss: 0.27482 - acc: 0.9316 -- iter: 21184/22500
Training Step: 2796  | total loss: [1m[32m0.25529[0m[0m | time: 119.800s
| Adam | epoch: 008 | loss: 0.25529 - acc: 0.9353 -- iter: 21248/22500
Training Step: 2797  | total loss: [1m[32m0.24021[0m[0m | time: 120.165s
| Adam | epoch: 008 | loss: 0.24021 - acc: 0.9402 -- iter: 21312/22500
Training Step: 2798  | total loss: [1m[32m0.22626[0m[0m | time: 120.534s
| Adam | epoch: 008 | loss: 0.22626 - acc: 0.9446 -- iter: 21376/22500
Training Step: 2799  | total loss: [1m[32m0.21294[0m[0m | time: 120.884s
| Adam | epoch: 008 | loss: 0.21294 - acc: 0.9502 -- iter: 21440/22500
Training Step: 2800  | total loss: [1m[32m0.19936[0m[0m | time: 124.478s
| Adam | epoch: 008 | loss: 0.19936 - acc: 0.9536 | val_loss: 0.58467 - val_acc: 0.7928 -- iter: 21504/22500
--
Training Step: 2801  | total loss: [1m[32m0.18990[0m[0m | time: 124.834s
| Adam | epoch: 008 | loss: 0.18990 - acc: 0.9567 -- iter: 21568/22500
Training Step: 2802  | total loss: [1m[32m0.18767[0m[0m | time: 125.179s
| Adam | epoch: 008 | loss: 0.18767 - acc: 0.9548 -- iter: 21632/22500
Training Step: 2803  | total loss: [1m[32m0.17517[0m[0m | time: 125.530s
| Adam | epoch: 008 | loss: 0.17517 - acc: 0.9593 -- iter: 21696/22500
Training Step: 2804  | total loss: [1m[32m0.16733[0m[0m | time: 125.880s
| Adam | epoch: 008 | loss: 0.16733 - acc: 0.9602 -- iter: 21760/22500
Training Step: 2805  | total loss: [1m[32m0.15917[0m[0m | time: 126.229s
| Adam | epoch: 008 | loss: 0.15917 - acc: 0.9642 -- iter: 21824/22500
Training Step: 2806  | total loss: [1m[32m0.15591[0m[0m | time: 126.584s
| Adam | epoch: 008 | loss: 0.15591 - acc: 0.9647 -- iter: 21888/22500
Training Step: 2807  | total loss: [1m[32m0.15756[0m[0m | time: 126.936s
| Adam | epoch: 008 | loss: 0.15756 - acc: 0.9635 -- iter: 21952/22500
Training Step: 2808  | total loss: [1m[32m0.15541[0m[0m | time: 127.285s
| Adam | epoch: 008 | loss: 0.15541 - acc: 0.9609 -- iter: 22016/22500
Training Step: 2809  | total loss: [1m[32m0.15428[0m[0m | time: 127.649s
| Adam | epoch: 008 | loss: 0.15428 - acc: 0.9601 -- iter: 22080/22500
Training Step: 2810  | total loss: [1m[32m0.14393[0m[0m | time: 128.021s
| Adam | epoch: 008 | loss: 0.14393 - acc: 0.9641 -- iter: 22144/22500
Training Step: 2811  | total loss: [1m[32m0.13907[0m[0m | time: 128.376s
| Adam | epoch: 008 | loss: 0.13907 - acc: 0.9661 -- iter: 22208/22500
Training Step: 2812  | total loss: [1m[32m0.13045[0m[0m | time: 128.721s
| Adam | epoch: 008 | loss: 0.13045 - acc: 0.9695 -- iter: 22272/22500
Training Step: 2813  | total loss: [1m[32m0.12813[0m[0m | time: 129.068s
| Adam | epoch: 008 | loss: 0.12813 - acc: 0.9694 -- iter: 22336/22500
Training Step: 2814  | total loss: [1m[32m0.12043[0m[0m | time: 129.418s
| Adam | epoch: 008 | loss: 0.12043 - acc: 0.9709 -- iter: 22400/22500
Training Step: 2815  | total loss: [1m[32m0.11370[0m[0m | time: 129.766s
| Adam | epoch: 008 | loss: 0.11370 - acc: 0.9738 -- iter: 22464/22500
Training Step: 2816  | total loss: [1m[32m0.10811[0m[0m | time: 133.274s
| Adam | epoch: 008 | loss: 0.10811 - acc: 0.9749 | val_loss: 0.65298 - val_acc: 0.7952 -- iter: 22500/22500
--
Training Step: 2817  | total loss: [1m[32m0.10308[0m[0m | time: 0.350s
| Adam | epoch: 009 | loss: 0.10308 - acc: 0.9743 -- iter: 00064/22500
Training Step: 2818  | total loss: [1m[32m0.09675[0m[0m | time: 0.699s
| Adam | epoch: 009 | loss: 0.09675 - acc: 0.9769 -- iter: 00128/22500
Training Step: 2819  | total loss: [1m[32m0.09586[0m[0m | time: 1.046s
| Adam | epoch: 009 | loss: 0.09586 - acc: 0.9776 -- iter: 00192/22500
Training Step: 2820  | total loss: [1m[32m0.09158[0m[0m | time: 1.396s
| Adam | epoch: 009 | loss: 0.09158 - acc: 0.9783 -- iter: 00256/22500
Training Step: 2821  | total loss: [1m[32m0.09376[0m[0m | time: 1.763s
| Adam | epoch: 009 | loss: 0.09376 - acc: 0.9789 -- iter: 00320/22500
Training Step: 2822  | total loss: [1m[32m0.09177[0m[0m | time: 2.135s
| Adam | epoch: 009 | loss: 0.09177 - acc: 0.9779 -- iter: 00384/22500
Training Step: 2823  | total loss: [1m[32m0.10110[0m[0m | time: 2.415s
| Adam | epoch: 009 | loss: 0.10110 - acc: 0.9754 -- iter: 00448/22500
Training Step: 2824  | total loss: [1m[32m0.09469[0m[0m | time: 2.689s
| Adam | epoch: 009 | loss: 0.09469 - acc: 0.9779 -- iter: 00512/22500
Training Step: 2825  | total loss: [1m[32m0.08773[0m[0m | time: 3.041s
| Adam | epoch: 009 | loss: 0.08773 - acc: 0.9801 -- iter: 00576/22500
Training Step: 2826  | total loss: [1m[32m0.09439[0m[0m | time: 3.393s
| Adam | epoch: 009 | loss: 0.09439 - acc: 0.9789 -- iter: 00640/22500
Training Step: 2827  | total loss: [1m[32m0.09222[0m[0m | time: 3.739s
| Adam | epoch: 009 | loss: 0.09222 - acc: 0.9779 -- iter: 00704/22500
Training Step: 2828  | total loss: [1m[32m0.10318[0m[0m | time: 4.085s
| Adam | epoch: 009 | loss: 0.10318 - acc: 0.9754 -- iter: 00768/22500
Training Step: 2829  | total loss: [1m[32m0.10666[0m[0m | time: 4.437s
| Adam | epoch: 009 | loss: 0.10666 - acc: 0.9748 -- iter: 00832/22500
Training Step: 2830  | total loss: [1m[32m0.10449[0m[0m | time: 4.785s
| Adam | epoch: 009 | loss: 0.10449 - acc: 0.9726 -- iter: 00896/22500
Training Step: 2831  | total loss: [1m[32m0.10349[0m[0m | time: 5.139s
| Adam | epoch: 009 | loss: 0.10349 - acc: 0.9738 -- iter: 00960/22500
Training Step: 2832  | total loss: [1m[32m0.10472[0m[0m | time: 5.507s
| Adam | epoch: 009 | loss: 0.10472 - acc: 0.9717 -- iter: 01024/22500
Training Step: 2833  | total loss: [1m[32m0.10754[0m[0m | time: 5.875s
| Adam | epoch: 009 | loss: 0.10754 - acc: 0.9714 -- iter: 01088/22500
Training Step: 2834  | total loss: [1m[32m0.10322[0m[0m | time: 6.210s
| Adam | epoch: 009 | loss: 0.10322 - acc: 0.9727 -- iter: 01152/22500
Training Step: 2835  | total loss: [1m[32m0.09709[0m[0m | time: 6.534s
| Adam | epoch: 009 | loss: 0.09709 - acc: 0.9739 -- iter: 01216/22500
Training Step: 2836  | total loss: [1m[32m0.10235[0m[0m | time: 6.855s
| Adam | epoch: 009 | loss: 0.10235 - acc: 0.9702 -- iter: 01280/22500
Training Step: 2837  | total loss: [1m[32m0.10534[0m[0m | time: 7.139s
| Adam | epoch: 009 | loss: 0.10534 - acc: 0.9685 -- iter: 01344/22500
Training Step: 2838  | total loss: [1m[32m0.10705[0m[0m | time: 7.421s
| Adam | epoch: 009 | loss: 0.10705 - acc: 0.9686 -- iter: 01408/22500
Training Step: 2839  | total loss: [1m[32m0.10364[0m[0m | time: 7.783s
| Adam | epoch: 009 | loss: 0.10364 - acc: 0.9670 -- iter: 01472/22500
Training Step: 2840  | total loss: [1m[32m0.10132[0m[0m | time: 8.132s
| Adam | epoch: 009 | loss: 0.10132 - acc: 0.9672 -- iter: 01536/22500
Training Step: 2841  | total loss: [1m[32m0.10028[0m[0m | time: 8.479s
| Adam | epoch: 009 | loss: 0.10028 - acc: 0.9658 -- iter: 01600/22500
Training Step: 2842  | total loss: [1m[32m0.09932[0m[0m | time: 8.829s
| Adam | epoch: 009 | loss: 0.09932 - acc: 0.9676 -- iter: 01664/22500
Training Step: 2843  | total loss: [1m[32m0.09309[0m[0m | time: 9.194s
| Adam | epoch: 009 | loss: 0.09309 - acc: 0.9693 -- iter: 01728/22500
Training Step: 2844  | total loss: [1m[32m0.08726[0m[0m | time: 9.559s
| Adam | epoch: 009 | loss: 0.08726 - acc: 0.9724 -- iter: 01792/22500
Training Step: 2845  | total loss: [1m[32m0.08432[0m[0m | time: 9.907s
| Adam | epoch: 009 | loss: 0.08432 - acc: 0.9736 -- iter: 01856/22500
Training Step: 2846  | total loss: [1m[32m0.08731[0m[0m | time: 10.257s
| Adam | epoch: 009 | loss: 0.08731 - acc: 0.9731 -- iter: 01920/22500
Training Step: 2847  | total loss: [1m[32m0.08452[0m[0m | time: 10.604s
| Adam | epoch: 009 | loss: 0.08452 - acc: 0.9727 -- iter: 01984/22500
Training Step: 2848  | total loss: [1m[32m0.07791[0m[0m | time: 10.956s
| Adam | epoch: 009 | loss: 0.07791 - acc: 0.9754 -- iter: 02048/22500
Training Step: 2849  | total loss: [1m[32m0.07703[0m[0m | time: 11.304s
| Adam | epoch: 009 | loss: 0.07703 - acc: 0.9763 -- iter: 02112/22500
Training Step: 2850  | total loss: [1m[32m0.07703[0m[0m | time: 11.652s
| Adam | epoch: 009 | loss: 0.07703 - acc: 0.9755 -- iter: 02176/22500
Training Step: 2851  | total loss: [1m[32m0.07817[0m[0m | time: 11.997s
| Adam | epoch: 009 | loss: 0.07817 - acc: 0.9764 -- iter: 02240/22500
Training Step: 2852  | total loss: [1m[32m0.09239[0m[0m | time: 12.350s
| Adam | epoch: 009 | loss: 0.09239 - acc: 0.9741 -- iter: 02304/22500
Training Step: 2853  | total loss: [1m[32m0.08536[0m[0m | time: 12.698s
| Adam | epoch: 009 | loss: 0.08536 - acc: 0.9767 -- iter: 02368/22500
Training Step: 2854  | total loss: [1m[32m0.08568[0m[0m | time: 13.062s
| Adam | epoch: 009 | loss: 0.08568 - acc: 0.9759 -- iter: 02432/22500
Training Step: 2855  | total loss: [1m[32m0.08454[0m[0m | time: 13.442s
| Adam | epoch: 009 | loss: 0.08454 - acc: 0.9767 -- iter: 02496/22500
Training Step: 2856  | total loss: [1m[32m0.08115[0m[0m | time: 13.792s
| Adam | epoch: 009 | loss: 0.08115 - acc: 0.9775 -- iter: 02560/22500
Training Step: 2857  | total loss: [1m[32m0.07611[0m[0m | time: 14.142s
| Adam | epoch: 009 | loss: 0.07611 - acc: 0.9798 -- iter: 02624/22500
Training Step: 2858  | total loss: [1m[32m0.07010[0m[0m | time: 14.487s
| Adam | epoch: 009 | loss: 0.07010 - acc: 0.9818 -- iter: 02688/22500
Training Step: 2859  | total loss: [1m[32m0.06776[0m[0m | time: 14.839s
| Adam | epoch: 009 | loss: 0.06776 - acc: 0.9820 -- iter: 02752/22500
Training Step: 2860  | total loss: [1m[32m0.07045[0m[0m | time: 15.192s
| Adam | epoch: 009 | loss: 0.07045 - acc: 0.9807 -- iter: 02816/22500
Training Step: 2861  | total loss: [1m[32m0.07362[0m[0m | time: 15.537s
| Adam | epoch: 009 | loss: 0.07362 - acc: 0.9811 -- iter: 02880/22500
Training Step: 2862  | total loss: [1m[32m0.06887[0m[0m | time: 15.884s
| Adam | epoch: 009 | loss: 0.06887 - acc: 0.9830 -- iter: 02944/22500
Training Step: 2863  | total loss: [1m[32m0.08975[0m[0m | time: 16.234s
| Adam | epoch: 009 | loss: 0.08975 - acc: 0.9753 -- iter: 03008/22500
Training Step: 2864  | total loss: [1m[32m0.09163[0m[0m | time: 16.583s
| Adam | epoch: 009 | loss: 0.09163 - acc: 0.9762 -- iter: 03072/22500
Training Step: 2865  | total loss: [1m[32m0.08832[0m[0m | time: 16.934s
| Adam | epoch: 009 | loss: 0.08832 - acc: 0.9770 -- iter: 03136/22500
Training Step: 2866  | total loss: [1m[32m0.09515[0m[0m | time: 17.297s
| Adam | epoch: 009 | loss: 0.09515 - acc: 0.9746 -- iter: 03200/22500
Training Step: 2867  | total loss: [1m[32m0.09055[0m[0m | time: 17.664s
| Adam | epoch: 009 | loss: 0.09055 - acc: 0.9756 -- iter: 03264/22500
Training Step: 2868  | total loss: [1m[32m0.09044[0m[0m | time: 18.013s
| Adam | epoch: 009 | loss: 0.09044 - acc: 0.9765 -- iter: 03328/22500
Training Step: 2869  | total loss: [1m[32m0.09296[0m[0m | time: 18.362s
| Adam | epoch: 009 | loss: 0.09296 - acc: 0.9773 -- iter: 03392/22500
Training Step: 2870  | total loss: [1m[32m0.08786[0m[0m | time: 18.713s
| Adam | epoch: 009 | loss: 0.08786 - acc: 0.9795 -- iter: 03456/22500
Training Step: 2871  | total loss: [1m[32m0.08813[0m[0m | time: 19.060s
| Adam | epoch: 009 | loss: 0.08813 - acc: 0.9800 -- iter: 03520/22500
Training Step: 2872  | total loss: [1m[32m0.08522[0m[0m | time: 19.410s
| Adam | epoch: 009 | loss: 0.08522 - acc: 0.9789 -- iter: 03584/22500
Training Step: 2873  | total loss: [1m[32m0.09173[0m[0m | time: 19.758s
| Adam | epoch: 009 | loss: 0.09173 - acc: 0.9779 -- iter: 03648/22500
Training Step: 2874  | total loss: [1m[32m0.09281[0m[0m | time: 20.106s
| Adam | epoch: 009 | loss: 0.09281 - acc: 0.9770 -- iter: 03712/22500
Training Step: 2875  | total loss: [1m[32m0.08761[0m[0m | time: 20.451s
| Adam | epoch: 009 | loss: 0.08761 - acc: 0.9777 -- iter: 03776/22500
Training Step: 2876  | total loss: [1m[32m0.08571[0m[0m | time: 20.802s
| Adam | epoch: 009 | loss: 0.08571 - acc: 0.9784 -- iter: 03840/22500
Training Step: 2877  | total loss: [1m[32m0.09339[0m[0m | time: 21.163s
| Adam | epoch: 009 | loss: 0.09339 - acc: 0.9759 -- iter: 03904/22500
Training Step: 2878  | total loss: [1m[32m0.09837[0m[0m | time: 21.532s
| Adam | epoch: 009 | loss: 0.09837 - acc: 0.9720 -- iter: 03968/22500
Training Step: 2879  | total loss: [1m[32m0.09240[0m[0m | time: 21.882s
| Adam | epoch: 009 | loss: 0.09240 - acc: 0.9733 -- iter: 04032/22500
Training Step: 2880  | total loss: [1m[32m0.09028[0m[0m | time: 22.228s
| Adam | epoch: 009 | loss: 0.09028 - acc: 0.9744 -- iter: 04096/22500
Training Step: 2881  | total loss: [1m[32m0.08573[0m[0m | time: 22.575s
| Adam | epoch: 009 | loss: 0.08573 - acc: 0.9769 -- iter: 04160/22500
Training Step: 2882  | total loss: [1m[32m0.07922[0m[0m | time: 22.927s
| Adam | epoch: 009 | loss: 0.07922 - acc: 0.9792 -- iter: 04224/22500
Training Step: 2883  | total loss: [1m[32m0.07846[0m[0m | time: 23.274s
| Adam | epoch: 009 | loss: 0.07846 - acc: 0.9782 -- iter: 04288/22500
Training Step: 2884  | total loss: [1m[32m0.08257[0m[0m | time: 23.624s
| Adam | epoch: 009 | loss: 0.08257 - acc: 0.9741 -- iter: 04352/22500
Training Step: 2885  | total loss: [1m[32m0.07829[0m[0m | time: 23.973s
| Adam | epoch: 009 | loss: 0.07829 - acc: 0.9751 -- iter: 04416/22500
Training Step: 2886  | total loss: [1m[32m0.07360[0m[0m | time: 24.319s
| Adam | epoch: 009 | loss: 0.07360 - acc: 0.9776 -- iter: 04480/22500
Training Step: 2887  | total loss: [1m[32m0.07857[0m[0m | time: 24.671s
| Adam | epoch: 009 | loss: 0.07857 - acc: 0.9783 -- iter: 04544/22500
Training Step: 2888  | total loss: [1m[32m0.08239[0m[0m | time: 25.045s
| Adam | epoch: 009 | loss: 0.08239 - acc: 0.9742 -- iter: 04608/22500
Training Step: 2889  | total loss: [1m[32m0.08115[0m[0m | time: 25.416s
| Adam | epoch: 009 | loss: 0.08115 - acc: 0.9737 -- iter: 04672/22500
Training Step: 2890  | total loss: [1m[32m0.08532[0m[0m | time: 25.766s
| Adam | epoch: 009 | loss: 0.08532 - acc: 0.9716 -- iter: 04736/22500
Training Step: 2891  | total loss: [1m[32m0.08701[0m[0m | time: 26.111s
| Adam | epoch: 009 | loss: 0.08701 - acc: 0.9713 -- iter: 04800/22500
Training Step: 2892  | total loss: [1m[32m0.08040[0m[0m | time: 26.459s
| Adam | epoch: 009 | loss: 0.08040 - acc: 0.9742 -- iter: 04864/22500
Training Step: 2893  | total loss: [1m[32m0.07382[0m[0m | time: 26.806s
| Adam | epoch: 009 | loss: 0.07382 - acc: 0.9768 -- iter: 04928/22500
Training Step: 2894  | total loss: [1m[32m0.07542[0m[0m | time: 27.151s
| Adam | epoch: 009 | loss: 0.07542 - acc: 0.9760 -- iter: 04992/22500
Training Step: 2895  | total loss: [1m[32m0.07128[0m[0m | time: 27.498s
| Adam | epoch: 009 | loss: 0.07128 - acc: 0.9784 -- iter: 05056/22500
Training Step: 2896  | total loss: [1m[32m0.07823[0m[0m | time: 27.846s
| Adam | epoch: 009 | loss: 0.07823 - acc: 0.9774 -- iter: 05120/22500
Training Step: 2897  | total loss: [1m[32m0.08627[0m[0m | time: 28.191s
| Adam | epoch: 009 | loss: 0.08627 - acc: 0.9719 -- iter: 05184/22500
Training Step: 2898  | total loss: [1m[32m0.08590[0m[0m | time: 28.545s
| Adam | epoch: 009 | loss: 0.08590 - acc: 0.9731 -- iter: 05248/22500
Training Step: 2899  | total loss: [1m[32m0.09399[0m[0m | time: 28.911s
| Adam | epoch: 009 | loss: 0.09399 - acc: 0.9727 -- iter: 05312/22500
Training Step: 2900  | total loss: [1m[32m0.08936[0m[0m | time: 29.282s
| Adam | epoch: 009 | loss: 0.08936 - acc: 0.9723 -- iter: 05376/22500
Training Step: 2901  | total loss: [1m[32m0.08974[0m[0m | time: 29.628s
| Adam | epoch: 009 | loss: 0.08974 - acc: 0.9735 -- iter: 05440/22500
Training Step: 2902  | total loss: [1m[32m0.09480[0m[0m | time: 29.978s
| Adam | epoch: 009 | loss: 0.09480 - acc: 0.9715 -- iter: 05504/22500
Training Step: 2903  | total loss: [1m[32m0.10183[0m[0m | time: 30.332s
| Adam | epoch: 009 | loss: 0.10183 - acc: 0.9649 -- iter: 05568/22500
Training Step: 2904  | total loss: [1m[32m0.10021[0m[0m | time: 30.679s
| Adam | epoch: 009 | loss: 0.10021 - acc: 0.9669 -- iter: 05632/22500
Training Step: 2905  | total loss: [1m[32m0.09597[0m[0m | time: 31.024s
| Adam | epoch: 009 | loss: 0.09597 - acc: 0.9671 -- iter: 05696/22500
Training Step: 2906  | total loss: [1m[32m0.09317[0m[0m | time: 31.389s
| Adam | epoch: 009 | loss: 0.09317 - acc: 0.9672 -- iter: 05760/22500
Training Step: 2907  | total loss: [1m[32m0.09112[0m[0m | time: 31.739s
| Adam | epoch: 009 | loss: 0.09112 - acc: 0.9674 -- iter: 05824/22500
Training Step: 2908  | total loss: [1m[32m0.09041[0m[0m | time: 32.090s
| Adam | epoch: 009 | loss: 0.09041 - acc: 0.9675 -- iter: 05888/22500
Training Step: 2909  | total loss: [1m[32m0.09449[0m[0m | time: 32.437s
| Adam | epoch: 009 | loss: 0.09449 - acc: 0.9661 -- iter: 05952/22500
Training Step: 2910  | total loss: [1m[32m0.10091[0m[0m | time: 32.781s
| Adam | epoch: 009 | loss: 0.10091 - acc: 0.9664 -- iter: 06016/22500
Training Step: 2911  | total loss: [1m[32m0.10685[0m[0m | time: 33.149s
| Adam | epoch: 009 | loss: 0.10685 - acc: 0.9666 -- iter: 06080/22500
Training Step: 2912  | total loss: [1m[32m0.10303[0m[0m | time: 33.521s
| Adam | epoch: 009 | loss: 0.10303 - acc: 0.9652 -- iter: 06144/22500
Training Step: 2913  | total loss: [1m[32m0.10735[0m[0m | time: 33.874s
| Adam | epoch: 009 | loss: 0.10735 - acc: 0.9625 -- iter: 06208/22500
Training Step: 2914  | total loss: [1m[32m0.09917[0m[0m | time: 34.222s
| Adam | epoch: 009 | loss: 0.09917 - acc: 0.9662 -- iter: 06272/22500
Training Step: 2915  | total loss: [1m[32m0.11306[0m[0m | time: 34.572s
| Adam | epoch: 009 | loss: 0.11306 - acc: 0.9602 -- iter: 06336/22500
Training Step: 2916  | total loss: [1m[32m0.12490[0m[0m | time: 34.919s
| Adam | epoch: 009 | loss: 0.12490 - acc: 0.9564 -- iter: 06400/22500
Training Step: 2917  | total loss: [1m[32m0.12004[0m[0m | time: 35.264s
| Adam | epoch: 009 | loss: 0.12004 - acc: 0.9592 -- iter: 06464/22500
Training Step: 2918  | total loss: [1m[32m0.11497[0m[0m | time: 35.612s
| Adam | epoch: 009 | loss: 0.11497 - acc: 0.9601 -- iter: 06528/22500
Training Step: 2919  | total loss: [1m[32m0.12370[0m[0m | time: 35.962s
| Adam | epoch: 009 | loss: 0.12370 - acc: 0.9579 -- iter: 06592/22500
Training Step: 2920  | total loss: [1m[32m0.12617[0m[0m | time: 36.311s
| Adam | epoch: 009 | loss: 0.12617 - acc: 0.9558 -- iter: 06656/22500
Training Step: 2921  | total loss: [1m[32m0.12624[0m[0m | time: 36.662s
| Adam | epoch: 009 | loss: 0.12624 - acc: 0.9540 -- iter: 06720/22500
Training Step: 2922  | total loss: [1m[32m0.12792[0m[0m | time: 37.024s
| Adam | epoch: 009 | loss: 0.12792 - acc: 0.9539 -- iter: 06784/22500
Training Step: 2923  | total loss: [1m[32m0.13020[0m[0m | time: 37.393s
| Adam | epoch: 009 | loss: 0.13020 - acc: 0.9538 -- iter: 06848/22500
Training Step: 2924  | total loss: [1m[32m0.12459[0m[0m | time: 37.746s
| Adam | epoch: 009 | loss: 0.12459 - acc: 0.9569 -- iter: 06912/22500
Training Step: 2925  | total loss: [1m[32m0.11991[0m[0m | time: 38.096s
| Adam | epoch: 009 | loss: 0.11991 - acc: 0.9581 -- iter: 06976/22500
Training Step: 2926  | total loss: [1m[32m0.11963[0m[0m | time: 38.467s
| Adam | epoch: 009 | loss: 0.11963 - acc: 0.9576 -- iter: 07040/22500
Training Step: 2927  | total loss: [1m[32m0.11288[0m[0m | time: 38.814s
| Adam | epoch: 009 | loss: 0.11288 - acc: 0.9603 -- iter: 07104/22500
Training Step: 2928  | total loss: [1m[32m0.10907[0m[0m | time: 39.174s
| Adam | epoch: 009 | loss: 0.10907 - acc: 0.9611 -- iter: 07168/22500
Training Step: 2929  | total loss: [1m[32m0.10935[0m[0m | time: 39.522s
| Adam | epoch: 009 | loss: 0.10935 - acc: 0.9619 -- iter: 07232/22500
Training Step: 2930  | total loss: [1m[32m0.10613[0m[0m | time: 39.884s
| Adam | epoch: 009 | loss: 0.10613 - acc: 0.9626 -- iter: 07296/22500
Training Step: 2931  | total loss: [1m[32m0.11088[0m[0m | time: 40.213s
| Adam | epoch: 009 | loss: 0.11088 - acc: 0.9616 -- iter: 07360/22500
Training Step: 2932  | total loss: [1m[32m0.10716[0m[0m | time: 40.570s
| Adam | epoch: 009 | loss: 0.10716 - acc: 0.9623 -- iter: 07424/22500
Training Step: 2933  | total loss: [1m[32m0.11602[0m[0m | time: 40.871s
| Adam | epoch: 009 | loss: 0.11602 - acc: 0.9583 -- iter: 07488/22500
Training Step: 2934  | total loss: [1m[32m0.12283[0m[0m | time: 41.173s
| Adam | epoch: 009 | loss: 0.12283 - acc: 0.9578 -- iter: 07552/22500
Training Step: 2935  | total loss: [1m[32m0.12030[0m[0m | time: 41.457s
| Adam | epoch: 009 | loss: 0.12030 - acc: 0.9573 -- iter: 07616/22500
Training Step: 2936  | total loss: [1m[32m0.11307[0m[0m | time: 41.880s
| Adam | epoch: 009 | loss: 0.11307 - acc: 0.9616 -- iter: 07680/22500
Training Step: 2937  | total loss: [1m[32m0.11327[0m[0m | time: 42.226s
| Adam | epoch: 009 | loss: 0.11327 - acc: 0.9639 -- iter: 07744/22500
Training Step: 2938  | total loss: [1m[32m0.11715[0m[0m | time: 42.571s
| Adam | epoch: 009 | loss: 0.11715 - acc: 0.9628 -- iter: 07808/22500
Training Step: 2939  | total loss: [1m[32m0.11673[0m[0m | time: 42.919s
| Adam | epoch: 009 | loss: 0.11673 - acc: 0.9634 -- iter: 07872/22500
Training Step: 2940  | total loss: [1m[32m0.12276[0m[0m | time: 43.269s
| Adam | epoch: 009 | loss: 0.12276 - acc: 0.9624 -- iter: 07936/22500
Training Step: 2941  | total loss: [1m[32m0.13554[0m[0m | time: 43.621s
| Adam | epoch: 009 | loss: 0.13554 - acc: 0.9599 -- iter: 08000/22500
Training Step: 2942  | total loss: [1m[32m0.12745[0m[0m | time: 43.983s
| Adam | epoch: 009 | loss: 0.12745 - acc: 0.9639 -- iter: 08064/22500
Training Step: 2943  | total loss: [1m[32m0.12092[0m[0m | time: 44.332s
| Adam | epoch: 009 | loss: 0.12092 - acc: 0.9659 -- iter: 08128/22500
Training Step: 2944  | total loss: [1m[32m0.11586[0m[0m | time: 44.678s
| Adam | epoch: 009 | loss: 0.11586 - acc: 0.9678 -- iter: 08192/22500
Training Step: 2945  | total loss: [1m[32m0.11014[0m[0m | time: 45.034s
| Adam | epoch: 009 | loss: 0.11014 - acc: 0.9694 -- iter: 08256/22500
Training Step: 2946  | total loss: [1m[32m0.11161[0m[0m | time: 45.382s
| Adam | epoch: 009 | loss: 0.11161 - acc: 0.9678 -- iter: 08320/22500
Training Step: 2947  | total loss: [1m[32m0.10744[0m[0m | time: 45.731s
| Adam | epoch: 009 | loss: 0.10744 - acc: 0.9695 -- iter: 08384/22500
Training Step: 2948  | total loss: [1m[32m0.10037[0m[0m | time: 46.077s
| Adam | epoch: 009 | loss: 0.10037 - acc: 0.9725 -- iter: 08448/22500
Training Step: 2949  | total loss: [1m[32m0.10336[0m[0m | time: 46.425s
| Adam | epoch: 009 | loss: 0.10336 - acc: 0.9721 -- iter: 08512/22500
Training Step: 2950  | total loss: [1m[32m0.10413[0m[0m | time: 46.775s
| Adam | epoch: 009 | loss: 0.10413 - acc: 0.9702 -- iter: 08576/22500
Training Step: 2951  | total loss: [1m[32m0.11640[0m[0m | time: 47.121s
| Adam | epoch: 009 | loss: 0.11640 - acc: 0.9654 -- iter: 08640/22500
Training Step: 2952  | total loss: [1m[32m0.11758[0m[0m | time: 47.472s
| Adam | epoch: 009 | loss: 0.11758 - acc: 0.9657 -- iter: 08704/22500
Training Step: 2953  | total loss: [1m[32m0.11094[0m[0m | time: 47.824s
| Adam | epoch: 009 | loss: 0.11094 - acc: 0.9676 -- iter: 08768/22500
Training Step: 2954  | total loss: [1m[32m0.11399[0m[0m | time: 48.171s
| Adam | epoch: 009 | loss: 0.11399 - acc: 0.9662 -- iter: 08832/22500
Training Step: 2955  | total loss: [1m[32m0.10877[0m[0m | time: 48.518s
| Adam | epoch: 009 | loss: 0.10877 - acc: 0.9664 -- iter: 08896/22500
Training Step: 2956  | total loss: [1m[32m0.10159[0m[0m | time: 48.900s
| Adam | epoch: 009 | loss: 0.10159 - acc: 0.9682 -- iter: 08960/22500
Training Step: 2957  | total loss: [1m[32m0.09518[0m[0m | time: 49.272s
| Adam | epoch: 009 | loss: 0.09518 - acc: 0.9698 -- iter: 09024/22500
Training Step: 2958  | total loss: [1m[32m0.10341[0m[0m | time: 49.621s
| Adam | epoch: 009 | loss: 0.10341 - acc: 0.9619 -- iter: 09088/22500
Training Step: 2959  | total loss: [1m[32m0.10138[0m[0m | time: 49.974s
| Adam | epoch: 009 | loss: 0.10138 - acc: 0.9642 -- iter: 09152/22500
Training Step: 2960  | total loss: [1m[32m0.10294[0m[0m | time: 50.337s
| Adam | epoch: 009 | loss: 0.10294 - acc: 0.9630 -- iter: 09216/22500
Training Step: 2961  | total loss: [1m[32m0.11029[0m[0m | time: 50.686s
| Adam | epoch: 009 | loss: 0.11029 - acc: 0.9605 -- iter: 09280/22500
Training Step: 2962  | total loss: [1m[32m0.11513[0m[0m | time: 51.033s
| Adam | epoch: 009 | loss: 0.11513 - acc: 0.9598 -- iter: 09344/22500
Training Step: 2963  | total loss: [1m[32m0.11136[0m[0m | time: 51.384s
| Adam | epoch: 009 | loss: 0.11136 - acc: 0.9622 -- iter: 09408/22500
Training Step: 2964  | total loss: [1m[32m0.10735[0m[0m | time: 51.736s
| Adam | epoch: 009 | loss: 0.10735 - acc: 0.9644 -- iter: 09472/22500
Training Step: 2965  | total loss: [1m[32m0.10476[0m[0m | time: 52.082s
| Adam | epoch: 009 | loss: 0.10476 - acc: 0.9664 -- iter: 09536/22500
Training Step: 2966  | total loss: [1m[32m0.09792[0m[0m | time: 52.431s
| Adam | epoch: 009 | loss: 0.09792 - acc: 0.9698 -- iter: 09600/22500
Training Step: 2967  | total loss: [1m[32m0.09613[0m[0m | time: 52.796s
| Adam | epoch: 009 | loss: 0.09613 - acc: 0.9681 -- iter: 09664/22500
Training Step: 2968  | total loss: [1m[32m0.11142[0m[0m | time: 53.163s
| Adam | epoch: 009 | loss: 0.11142 - acc: 0.9619 -- iter: 09728/22500
Training Step: 2969  | total loss: [1m[32m0.11238[0m[0m | time: 53.508s
| Adam | epoch: 009 | loss: 0.11238 - acc: 0.9611 -- iter: 09792/22500
Training Step: 2970  | total loss: [1m[32m0.11188[0m[0m | time: 53.860s
| Adam | epoch: 009 | loss: 0.11188 - acc: 0.9634 -- iter: 09856/22500
Training Step: 2971  | total loss: [1m[32m0.10860[0m[0m | time: 54.208s
| Adam | epoch: 009 | loss: 0.10860 - acc: 0.9639 -- iter: 09920/22500
Training Step: 2972  | total loss: [1m[32m0.11425[0m[0m | time: 54.630s
| Adam | epoch: 009 | loss: 0.11425 - acc: 0.9644 -- iter: 09984/22500
Training Step: 2973  | total loss: [1m[32m0.12442[0m[0m | time: 54.961s
| Adam | epoch: 009 | loss: 0.12442 - acc: 0.9633 -- iter: 10048/22500
Training Step: 2974  | total loss: [1m[32m0.12338[0m[0m | time: 55.309s
| Adam | epoch: 009 | loss: 0.12338 - acc: 0.9623 -- iter: 10112/22500
Training Step: 2975  | total loss: [1m[32m0.11709[0m[0m | time: 55.664s
| Adam | epoch: 009 | loss: 0.11709 - acc: 0.9660 -- iter: 10176/22500
Training Step: 2976  | total loss: [1m[32m0.11675[0m[0m | time: 56.009s
| Adam | epoch: 009 | loss: 0.11675 - acc: 0.9663 -- iter: 10240/22500
Training Step: 2977  | total loss: [1m[32m0.11897[0m[0m | time: 56.370s
| Adam | epoch: 009 | loss: 0.11897 - acc: 0.9666 -- iter: 10304/22500
Training Step: 2978  | total loss: [1m[32m0.11380[0m[0m | time: 56.742s
| Adam | epoch: 009 | loss: 0.11380 - acc: 0.9668 -- iter: 10368/22500
Training Step: 2979  | total loss: [1m[32m0.10673[0m[0m | time: 57.106s
| Adam | epoch: 009 | loss: 0.10673 - acc: 0.9701 -- iter: 10432/22500
Training Step: 2980  | total loss: [1m[32m0.10818[0m[0m | time: 57.457s
| Adam | epoch: 009 | loss: 0.10818 - acc: 0.9700 -- iter: 10496/22500
Training Step: 2981  | total loss: [1m[32m0.10008[0m[0m | time: 57.810s
| Adam | epoch: 009 | loss: 0.10008 - acc: 0.9730 -- iter: 10560/22500
Training Step: 2982  | total loss: [1m[32m0.10377[0m[0m | time: 58.162s
| Adam | epoch: 009 | loss: 0.10377 - acc: 0.9725 -- iter: 10624/22500
Training Step: 2983  | total loss: [1m[32m0.10336[0m[0m | time: 58.520s
| Adam | epoch: 009 | loss: 0.10336 - acc: 0.9737 -- iter: 10688/22500
Training Step: 2984  | total loss: [1m[32m0.09943[0m[0m | time: 58.874s
| Adam | epoch: 009 | loss: 0.09943 - acc: 0.9748 -- iter: 10752/22500
Training Step: 2985  | total loss: [1m[32m0.10564[0m[0m | time: 59.221s
| Adam | epoch: 009 | loss: 0.10564 - acc: 0.9711 -- iter: 10816/22500
Training Step: 2986  | total loss: [1m[32m0.10759[0m[0m | time: 59.571s
| Adam | epoch: 009 | loss: 0.10759 - acc: 0.9724 -- iter: 10880/22500
Training Step: 2987  | total loss: [1m[32m0.13383[0m[0m | time: 59.921s
| Adam | epoch: 009 | loss: 0.13383 - acc: 0.9658 -- iter: 10944/22500
Training Step: 2988  | total loss: [1m[32m0.12998[0m[0m | time: 60.266s
| Adam | epoch: 009 | loss: 0.12998 - acc: 0.9661 -- iter: 11008/22500
Training Step: 2989  | total loss: [1m[32m0.13360[0m[0m | time: 60.618s
| Adam | epoch: 009 | loss: 0.13360 - acc: 0.9632 -- iter: 11072/22500
Training Step: 2990  | total loss: [1m[32m0.12448[0m[0m | time: 60.984s
| Adam | epoch: 009 | loss: 0.12448 - acc: 0.9653 -- iter: 11136/22500
Training Step: 2991  | total loss: [1m[32m0.11700[0m[0m | time: 61.351s
| Adam | epoch: 009 | loss: 0.11700 - acc: 0.9672 -- iter: 11200/22500
Training Step: 2992  | total loss: [1m[32m0.10889[0m[0m | time: 61.697s
| Adam | epoch: 009 | loss: 0.10889 - acc: 0.9705 -- iter: 11264/22500
Training Step: 2993  | total loss: [1m[32m0.10798[0m[0m | time: 62.042s
| Adam | epoch: 009 | loss: 0.10798 - acc: 0.9688 -- iter: 11328/22500
Training Step: 2994  | total loss: [1m[32m0.10185[0m[0m | time: 62.399s
| Adam | epoch: 009 | loss: 0.10185 - acc: 0.9688 -- iter: 11392/22500
Training Step: 2995  | total loss: [1m[32m0.10584[0m[0m | time: 62.751s
| Adam | epoch: 009 | loss: 0.10584 - acc: 0.9656 -- iter: 11456/22500
Training Step: 2996  | total loss: [1m[32m0.10472[0m[0m | time: 63.099s
| Adam | epoch: 009 | loss: 0.10472 - acc: 0.9660 -- iter: 11520/22500
Training Step: 2997  | total loss: [1m[32m0.10898[0m[0m | time: 63.446s
| Adam | epoch: 009 | loss: 0.10898 - acc: 0.9615 -- iter: 11584/22500
Training Step: 2998  | total loss: [1m[32m0.10881[0m[0m | time: 63.798s
| Adam | epoch: 009 | loss: 0.10881 - acc: 0.9607 -- iter: 11648/22500
Training Step: 2999  | total loss: [1m[32m0.10114[0m[0m | time: 64.153s
| Adam | epoch: 009 | loss: 0.10114 - acc: 0.9646 -- iter: 11712/22500
Training Step: 3000  | total loss: [1m[32m0.10305[0m[0m | time: 67.675s
| Adam | epoch: 009 | loss: 0.10305 - acc: 0.9635 | val_loss: 0.72130 - val_acc: 0.8044 -- iter: 11776/22500
--
Training Step: 3001  | total loss: [1m[32m0.10271[0m[0m | time: 68.041s
| Adam | epoch: 009 | loss: 0.10271 - acc: 0.9609 -- iter: 11840/22500
Training Step: 3002  | total loss: [1m[32m0.10342[0m[0m | time: 68.413s
| Adam | epoch: 009 | loss: 0.10342 - acc: 0.9601 -- iter: 11904/22500
Training Step: 3003  | total loss: [1m[32m0.10174[0m[0m | time: 68.761s
| Adam | epoch: 009 | loss: 0.10174 - acc: 0.9610 -- iter: 11968/22500
Training Step: 3004  | total loss: [1m[32m0.10921[0m[0m | time: 69.108s
| Adam | epoch: 009 | loss: 0.10921 - acc: 0.9602 -- iter: 12032/22500
Training Step: 3005  | total loss: [1m[32m0.10627[0m[0m | time: 69.458s
| Adam | epoch: 009 | loss: 0.10627 - acc: 0.9610 -- iter: 12096/22500
Training Step: 3006  | total loss: [1m[32m0.10272[0m[0m | time: 69.804s
| Adam | epoch: 009 | loss: 0.10272 - acc: 0.9618 -- iter: 12160/22500
Training Step: 3007  | total loss: [1m[32m0.10273[0m[0m | time: 70.154s
| Adam | epoch: 009 | loss: 0.10273 - acc: 0.9625 -- iter: 12224/22500
Training Step: 3008  | total loss: [1m[32m0.10445[0m[0m | time: 70.505s
| Adam | epoch: 009 | loss: 0.10445 - acc: 0.9647 -- iter: 12288/22500
Training Step: 3009  | total loss: [1m[32m0.11277[0m[0m | time: 70.856s
| Adam | epoch: 009 | loss: 0.11277 - acc: 0.9635 -- iter: 12352/22500
Training Step: 3010  | total loss: [1m[32m0.10740[0m[0m | time: 71.203s
| Adam | epoch: 009 | loss: 0.10740 - acc: 0.9656 -- iter: 12416/22500
Training Step: 3011  | total loss: [1m[32m0.10694[0m[0m | time: 71.550s
| Adam | epoch: 009 | loss: 0.10694 - acc: 0.9659 -- iter: 12480/22500
Training Step: 3012  | total loss: [1m[32m0.10249[0m[0m | time: 71.904s
| Adam | epoch: 009 | loss: 0.10249 - acc: 0.9678 -- iter: 12544/22500
Training Step: 3013  | total loss: [1m[32m0.10792[0m[0m | time: 72.255s
| Adam | epoch: 009 | loss: 0.10792 - acc: 0.9632 -- iter: 12608/22500
Training Step: 3014  | total loss: [1m[32m0.10751[0m[0m | time: 72.599s
| Adam | epoch: 009 | loss: 0.10751 - acc: 0.9622 -- iter: 12672/22500
Training Step: 3015  | total loss: [1m[32m0.10202[0m[0m | time: 72.951s
| Adam | epoch: 009 | loss: 0.10202 - acc: 0.9660 -- iter: 12736/22500
Training Step: 3016  | total loss: [1m[32m0.10042[0m[0m | time: 73.305s
| Adam | epoch: 009 | loss: 0.10042 - acc: 0.9647 -- iter: 12800/22500
Training Step: 3017  | total loss: [1m[32m0.09949[0m[0m | time: 73.654s
| Adam | epoch: 009 | loss: 0.09949 - acc: 0.9635 -- iter: 12864/22500
Training Step: 3018  | total loss: [1m[32m0.10402[0m[0m | time: 74.002s
| Adam | epoch: 009 | loss: 0.10402 - acc: 0.9609 -- iter: 12928/22500
Training Step: 3019  | total loss: [1m[32m0.10486[0m[0m | time: 74.351s
| Adam | epoch: 009 | loss: 0.10486 - acc: 0.9601 -- iter: 12992/22500
Training Step: 3020  | total loss: [1m[32m0.11212[0m[0m | time: 74.698s
| Adam | epoch: 009 | loss: 0.11212 - acc: 0.9579 -- iter: 13056/22500
Training Step: 3021  | total loss: [1m[32m0.10840[0m[0m | time: 75.049s
| Adam | epoch: 009 | loss: 0.10840 - acc: 0.9605 -- iter: 13120/22500
Training Step: 3022  | total loss: [1m[32m0.11205[0m[0m | time: 75.399s
| Adam | epoch: 009 | loss: 0.11205 - acc: 0.9582 -- iter: 13184/22500
Training Step: 3023  | total loss: [1m[32m0.11652[0m[0m | time: 75.750s
| Adam | epoch: 009 | loss: 0.11652 - acc: 0.9577 -- iter: 13248/22500
Training Step: 3024  | total loss: [1m[32m0.11041[0m[0m | time: 76.135s
| Adam | epoch: 009 | loss: 0.11041 - acc: 0.9604 -- iter: 13312/22500
Training Step: 3025  | total loss: [1m[32m0.11162[0m[0m | time: 76.507s
| Adam | epoch: 009 | loss: 0.11162 - acc: 0.9597 -- iter: 13376/22500
Training Step: 3026  | total loss: [1m[32m0.11894[0m[0m | time: 76.857s
| Adam | epoch: 009 | loss: 0.11894 - acc: 0.9574 -- iter: 13440/22500
Training Step: 3027  | total loss: [1m[32m0.11974[0m[0m | time: 77.201s
| Adam | epoch: 009 | loss: 0.11974 - acc: 0.9586 -- iter: 13504/22500
Training Step: 3028  | total loss: [1m[32m0.12104[0m[0m | time: 77.550s
| Adam | epoch: 009 | loss: 0.12104 - acc: 0.9580 -- iter: 13568/22500
Training Step: 3029  | total loss: [1m[32m0.11382[0m[0m | time: 77.899s
| Adam | epoch: 009 | loss: 0.11382 - acc: 0.9622 -- iter: 13632/22500
Training Step: 3030  | total loss: [1m[32m0.12599[0m[0m | time: 78.252s
| Adam | epoch: 009 | loss: 0.12599 - acc: 0.9582 -- iter: 13696/22500
Training Step: 3031  | total loss: [1m[32m0.12750[0m[0m | time: 78.599s
| Adam | epoch: 009 | loss: 0.12750 - acc: 0.9592 -- iter: 13760/22500
Training Step: 3032  | total loss: [1m[32m0.12065[0m[0m | time: 78.953s
| Adam | epoch: 009 | loss: 0.12065 - acc: 0.9618 -- iter: 13824/22500
Training Step: 3033  | total loss: [1m[32m0.11255[0m[0m | time: 79.301s
| Adam | epoch: 009 | loss: 0.11255 - acc: 0.9656 -- iter: 13888/22500
Training Step: 3034  | total loss: [1m[32m0.11561[0m[0m | time: 79.647s
| Adam | epoch: 009 | loss: 0.11561 - acc: 0.9612 -- iter: 13952/22500
Training Step: 3035  | total loss: [1m[32m0.11214[0m[0m | time: 80.015s
| Adam | epoch: 009 | loss: 0.11214 - acc: 0.9620 -- iter: 14016/22500
Training Step: 3036  | total loss: [1m[32m0.10808[0m[0m | time: 80.383s
| Adam | epoch: 009 | loss: 0.10808 - acc: 0.9642 -- iter: 14080/22500
Training Step: 3037  | total loss: [1m[32m0.10990[0m[0m | time: 80.735s
| Adam | epoch: 009 | loss: 0.10990 - acc: 0.9615 -- iter: 14144/22500
Training Step: 3038  | total loss: [1m[32m0.12498[0m[0m | time: 81.082s
| Adam | epoch: 009 | loss: 0.12498 - acc: 0.9560 -- iter: 14208/22500
Training Step: 3039  | total loss: [1m[32m0.13326[0m[0m | time: 81.429s
| Adam | epoch: 009 | loss: 0.13326 - acc: 0.9495 -- iter: 14272/22500
Training Step: 3040  | total loss: [1m[32m0.12701[0m[0m | time: 81.777s
| Adam | epoch: 009 | loss: 0.12701 - acc: 0.9514 -- iter: 14336/22500
Training Step: 3041  | total loss: [1m[32m0.12491[0m[0m | time: 82.127s
| Adam | epoch: 009 | loss: 0.12491 - acc: 0.9516 -- iter: 14400/22500
Training Step: 3042  | total loss: [1m[32m0.11925[0m[0m | time: 82.474s
| Adam | epoch: 009 | loss: 0.11925 - acc: 0.9533 -- iter: 14464/22500
Training Step: 3043  | total loss: [1m[32m0.12677[0m[0m | time: 82.823s
| Adam | epoch: 009 | loss: 0.12677 - acc: 0.9533 -- iter: 14528/22500
Training Step: 3044  | total loss: [1m[32m0.13976[0m[0m | time: 83.175s
| Adam | epoch: 009 | loss: 0.13976 - acc: 0.9454 -- iter: 14592/22500
Training Step: 3045  | total loss: [1m[32m0.15278[0m[0m | time: 83.523s
| Adam | epoch: 009 | loss: 0.15278 - acc: 0.9431 -- iter: 14656/22500
Training Step: 3046  | total loss: [1m[32m0.14951[0m[0m | time: 83.893s
| Adam | epoch: 009 | loss: 0.14951 - acc: 0.9441 -- iter: 14720/22500
Training Step: 3047  | total loss: [1m[32m0.15169[0m[0m | time: 84.262s
| Adam | epoch: 009 | loss: 0.15169 - acc: 0.9419 -- iter: 14784/22500
Training Step: 3048  | total loss: [1m[32m0.15183[0m[0m | time: 84.604s
| Adam | epoch: 009 | loss: 0.15183 - acc: 0.9399 -- iter: 14848/22500
Training Step: 3049  | total loss: [1m[32m0.15791[0m[0m | time: 84.957s
| Adam | epoch: 009 | loss: 0.15791 - acc: 0.9396 -- iter: 14912/22500
Training Step: 3050  | total loss: [1m[32m0.16030[0m[0m | time: 85.307s
| Adam | epoch: 009 | loss: 0.16030 - acc: 0.9379 -- iter: 14976/22500
Training Step: 3051  | total loss: [1m[32m0.15345[0m[0m | time: 85.653s
| Adam | epoch: 009 | loss: 0.15345 - acc: 0.9394 -- iter: 15040/22500
Training Step: 3052  | total loss: [1m[32m0.14902[0m[0m | time: 86.004s
| Adam | epoch: 009 | loss: 0.14902 - acc: 0.9408 -- iter: 15104/22500
Training Step: 3053  | total loss: [1m[32m0.15453[0m[0m | time: 86.352s
| Adam | epoch: 009 | loss: 0.15453 - acc: 0.9404 -- iter: 15168/22500
Training Step: 3054  | total loss: [1m[32m0.16356[0m[0m | time: 86.698s
| Adam | epoch: 009 | loss: 0.16356 - acc: 0.9370 -- iter: 15232/22500
Training Step: 3055  | total loss: [1m[32m0.17202[0m[0m | time: 87.050s
| Adam | epoch: 009 | loss: 0.17202 - acc: 0.9355 -- iter: 15296/22500
Training Step: 3056  | total loss: [1m[32m0.16318[0m[0m | time: 87.396s
| Adam | epoch: 009 | loss: 0.16318 - acc: 0.9388 -- iter: 15360/22500
Training Step: 3057  | total loss: [1m[32m0.15364[0m[0m | time: 87.739s
| Adam | epoch: 009 | loss: 0.15364 - acc: 0.9434 -- iter: 15424/22500
Training Step: 3058  | total loss: [1m[32m0.17474[0m[0m | time: 88.106s
| Adam | epoch: 009 | loss: 0.17474 - acc: 0.9334 -- iter: 15488/22500
Training Step: 3059  | total loss: [1m[32m0.17717[0m[0m | time: 88.482s
| Adam | epoch: 009 | loss: 0.17717 - acc: 0.9338 -- iter: 15552/22500
Training Step: 3060  | total loss: [1m[32m0.16802[0m[0m | time: 88.832s
| Adam | epoch: 009 | loss: 0.16802 - acc: 0.9373 -- iter: 15616/22500
Training Step: 3061  | total loss: [1m[32m0.15805[0m[0m | time: 89.187s
| Adam | epoch: 009 | loss: 0.15805 - acc: 0.9420 -- iter: 15680/22500
Training Step: 3062  | total loss: [1m[32m0.15384[0m[0m | time: 89.535s
| Adam | epoch: 009 | loss: 0.15384 - acc: 0.9447 -- iter: 15744/22500
Training Step: 3063  | total loss: [1m[32m0.14598[0m[0m | time: 89.879s
| Adam | epoch: 009 | loss: 0.14598 - acc: 0.9487 -- iter: 15808/22500
Training Step: 3064  | total loss: [1m[32m0.13950[0m[0m | time: 90.228s
| Adam | epoch: 009 | loss: 0.13950 - acc: 0.9522 -- iter: 15872/22500
Training Step: 3065  | total loss: [1m[32m0.14016[0m[0m | time: 90.572s
| Adam | epoch: 009 | loss: 0.14016 - acc: 0.9523 -- iter: 15936/22500
Training Step: 3066  | total loss: [1m[32m0.13600[0m[0m | time: 90.920s
| Adam | epoch: 009 | loss: 0.13600 - acc: 0.9540 -- iter: 16000/22500
Training Step: 3067  | total loss: [1m[32m0.14289[0m[0m | time: 91.272s
| Adam | epoch: 009 | loss: 0.14289 - acc: 0.9523 -- iter: 16064/22500
Training Step: 3068  | total loss: [1m[32m0.15026[0m[0m | time: 91.621s
| Adam | epoch: 009 | loss: 0.15026 - acc: 0.9493 -- iter: 16128/22500
Training Step: 3069  | total loss: [1m[32m0.15627[0m[0m | time: 91.989s
| Adam | epoch: 009 | loss: 0.15627 - acc: 0.9465 -- iter: 16192/22500
Training Step: 3070  | total loss: [1m[32m0.15529[0m[0m | time: 92.357s
| Adam | epoch: 009 | loss: 0.15529 - acc: 0.9472 -- iter: 16256/22500
Training Step: 3071  | total loss: [1m[32m0.15339[0m[0m | time: 92.702s
| Adam | epoch: 009 | loss: 0.15339 - acc: 0.9493 -- iter: 16320/22500
Training Step: 3072  | total loss: [1m[32m0.15231[0m[0m | time: 93.051s
| Adam | epoch: 009 | loss: 0.15231 - acc: 0.9513 -- iter: 16384/22500
Training Step: 3073  | total loss: [1m[32m0.15573[0m[0m | time: 93.398s
| Adam | epoch: 009 | loss: 0.15573 - acc: 0.9515 -- iter: 16448/22500
Training Step: 3074  | total loss: [1m[32m0.14566[0m[0m | time: 93.748s
| Adam | epoch: 009 | loss: 0.14566 - acc: 0.9548 -- iter: 16512/22500
Training Step: 3075  | total loss: [1m[32m0.14359[0m[0m | time: 94.097s
| Adam | epoch: 009 | loss: 0.14359 - acc: 0.9546 -- iter: 16576/22500
Training Step: 3076  | total loss: [1m[32m0.14311[0m[0m | time: 94.445s
| Adam | epoch: 009 | loss: 0.14311 - acc: 0.9529 -- iter: 16640/22500
Training Step: 3077  | total loss: [1m[32m0.14068[0m[0m | time: 94.794s
| Adam | epoch: 009 | loss: 0.14068 - acc: 0.9529 -- iter: 16704/22500
Training Step: 3078  | total loss: [1m[32m0.14152[0m[0m | time: 95.141s
| Adam | epoch: 009 | loss: 0.14152 - acc: 0.9514 -- iter: 16768/22500
Training Step: 3079  | total loss: [1m[32m0.13515[0m[0m | time: 95.493s
| Adam | epoch: 009 | loss: 0.13515 - acc: 0.9531 -- iter: 16832/22500
Training Step: 3080  | total loss: [1m[32m0.13140[0m[0m | time: 95.858s
| Adam | epoch: 009 | loss: 0.13140 - acc: 0.9531 -- iter: 16896/22500
Training Step: 3081  | total loss: [1m[32m0.12989[0m[0m | time: 96.227s
| Adam | epoch: 009 | loss: 0.12989 - acc: 0.9500 -- iter: 16960/22500
Training Step: 3082  | total loss: [1m[32m0.13130[0m[0m | time: 96.578s
| Adam | epoch: 009 | loss: 0.13130 - acc: 0.9487 -- iter: 17024/22500
Training Step: 3083  | total loss: [1m[32m0.12876[0m[0m | time: 96.930s
| Adam | epoch: 009 | loss: 0.12876 - acc: 0.9461 -- iter: 17088/22500
Training Step: 3084  | total loss: [1m[32m0.13367[0m[0m | time: 97.279s
| Adam | epoch: 009 | loss: 0.13367 - acc: 0.9436 -- iter: 17152/22500
Training Step: 3085  | total loss: [1m[32m0.12914[0m[0m | time: 97.625s
| Adam | epoch: 009 | loss: 0.12914 - acc: 0.9461 -- iter: 17216/22500
Training Step: 3086  | total loss: [1m[32m0.12678[0m[0m | time: 97.979s
| Adam | epoch: 009 | loss: 0.12678 - acc: 0.9453 -- iter: 17280/22500
Training Step: 3087  | total loss: [1m[32m0.11969[0m[0m | time: 98.356s
| Adam | epoch: 009 | loss: 0.11969 - acc: 0.9492 -- iter: 17344/22500
Training Step: 3088  | total loss: [1m[32m0.12563[0m[0m | time: 98.707s
| Adam | epoch: 009 | loss: 0.12563 - acc: 0.9480 -- iter: 17408/22500
Training Step: 3089  | total loss: [1m[32m0.12385[0m[0m | time: 99.063s
| Adam | epoch: 009 | loss: 0.12385 - acc: 0.9501 -- iter: 17472/22500
Training Step: 3090  | total loss: [1m[32m0.12235[0m[0m | time: 99.412s
| Adam | epoch: 009 | loss: 0.12235 - acc: 0.9520 -- iter: 17536/22500
Training Step: 3091  | total loss: [1m[32m0.12616[0m[0m | time: 99.782s
| Adam | epoch: 009 | loss: 0.12616 - acc: 0.9536 -- iter: 17600/22500
Training Step: 3092  | total loss: [1m[32m0.12915[0m[0m | time: 100.155s
| Adam | epoch: 009 | loss: 0.12915 - acc: 0.9520 -- iter: 17664/22500
Training Step: 3093  | total loss: [1m[32m0.12621[0m[0m | time: 100.504s
| Adam | epoch: 009 | loss: 0.12621 - acc: 0.9537 -- iter: 17728/22500
Training Step: 3094  | total loss: [1m[32m0.12155[0m[0m | time: 100.853s
| Adam | epoch: 009 | loss: 0.12155 - acc: 0.9552 -- iter: 17792/22500
Training Step: 3095  | total loss: [1m[32m0.13837[0m[0m | time: 101.200s
| Adam | epoch: 009 | loss: 0.13837 - acc: 0.9519 -- iter: 17856/22500
Training Step: 3096  | total loss: [1m[32m0.12834[0m[0m | time: 101.548s
| Adam | epoch: 009 | loss: 0.12834 - acc: 0.9567 -- iter: 17920/22500
Training Step: 3097  | total loss: [1m[32m0.13241[0m[0m | time: 101.899s
| Adam | epoch: 009 | loss: 0.13241 - acc: 0.9548 -- iter: 17984/22500
Training Step: 3098  | total loss: [1m[32m0.13207[0m[0m | time: 102.249s
| Adam | epoch: 009 | loss: 0.13207 - acc: 0.9546 -- iter: 18048/22500
Training Step: 3099  | total loss: [1m[32m0.13498[0m[0m | time: 102.597s
| Adam | epoch: 009 | loss: 0.13498 - acc: 0.9545 -- iter: 18112/22500
Training Step: 3100  | total loss: [1m[32m0.14011[0m[0m | time: 102.948s
| Adam | epoch: 009 | loss: 0.14011 - acc: 0.9528 -- iter: 18176/22500
Training Step: 3101  | total loss: [1m[32m0.14783[0m[0m | time: 103.303s
| Adam | epoch: 009 | loss: 0.14783 - acc: 0.9497 -- iter: 18240/22500
Training Step: 3102  | total loss: [1m[32m0.14212[0m[0m | time: 103.653s
| Adam | epoch: 009 | loss: 0.14212 - acc: 0.9531 -- iter: 18304/22500
Training Step: 3103  | total loss: [1m[32m0.13870[0m[0m | time: 104.018s
| Adam | epoch: 009 | loss: 0.13870 - acc: 0.9531 -- iter: 18368/22500
Training Step: 3104  | total loss: [1m[32m0.14061[0m[0m | time: 104.388s
| Adam | epoch: 009 | loss: 0.14061 - acc: 0.9500 -- iter: 18432/22500
Training Step: 3105  | total loss: [1m[32m0.12928[0m[0m | time: 104.736s
| Adam | epoch: 009 | loss: 0.12928 - acc: 0.9534 -- iter: 18496/22500
Training Step: 3106  | total loss: [1m[32m0.13592[0m[0m | time: 105.081s
| Adam | epoch: 009 | loss: 0.13592 - acc: 0.9519 -- iter: 18560/22500
Training Step: 3107  | total loss: [1m[32m0.12780[0m[0m | time: 105.431s
| Adam | epoch: 009 | loss: 0.12780 - acc: 0.9551 -- iter: 18624/22500
Training Step: 3108  | total loss: [1m[32m0.13458[0m[0m | time: 105.779s
| Adam | epoch: 009 | loss: 0.13458 - acc: 0.9533 -- iter: 18688/22500
Training Step: 3109  | total loss: [1m[32m0.12667[0m[0m | time: 106.128s
| Adam | epoch: 009 | loss: 0.12667 - acc: 0.9564 -- iter: 18752/22500
Training Step: 3110  | total loss: [1m[32m0.12685[0m[0m | time: 106.479s
| Adam | epoch: 009 | loss: 0.12685 - acc: 0.9577 -- iter: 18816/22500
Training Step: 3111  | total loss: [1m[32m0.12439[0m[0m | time: 106.830s
| Adam | epoch: 009 | loss: 0.12439 - acc: 0.9603 -- iter: 18880/22500
Training Step: 3112  | total loss: [1m[32m0.13275[0m[0m | time: 107.182s
| Adam | epoch: 009 | loss: 0.13275 - acc: 0.9596 -- iter: 18944/22500
Training Step: 3113  | total loss: [1m[32m0.12920[0m[0m | time: 107.530s
| Adam | epoch: 009 | loss: 0.12920 - acc: 0.9605 -- iter: 19008/22500
Training Step: 3114  | total loss: [1m[32m0.12556[0m[0m | time: 107.891s
| Adam | epoch: 009 | loss: 0.12556 - acc: 0.9629 -- iter: 19072/22500
Training Step: 3115  | total loss: [1m[32m0.12173[0m[0m | time: 108.240s
| Adam | epoch: 009 | loss: 0.12173 - acc: 0.9619 -- iter: 19136/22500
Training Step: 3116  | total loss: [1m[32m0.12509[0m[0m | time: 108.553s
| Adam | epoch: 009 | loss: 0.12509 - acc: 0.9626 -- iter: 19200/22500
Training Step: 3117  | total loss: [1m[32m0.12318[0m[0m | time: 108.893s
| Adam | epoch: 009 | loss: 0.12318 - acc: 0.9648 -- iter: 19264/22500
Training Step: 3118  | total loss: [1m[32m0.13446[0m[0m | time: 109.176s
| Adam | epoch: 009 | loss: 0.13446 - acc: 0.9605 -- iter: 19328/22500
Training Step: 3119  | total loss: [1m[32m0.13852[0m[0m | time: 109.459s
| Adam | epoch: 009 | loss: 0.13852 - acc: 0.9582 -- iter: 19392/22500
Training Step: 3120  | total loss: [1m[32m0.13893[0m[0m | time: 109.837s
| Adam | epoch: 009 | loss: 0.13893 - acc: 0.9561 -- iter: 19456/22500
Training Step: 3121  | total loss: [1m[32m0.13499[0m[0m | time: 110.184s
| Adam | epoch: 009 | loss: 0.13499 - acc: 0.9574 -- iter: 19520/22500
Training Step: 3122  | total loss: [1m[32m0.13733[0m[0m | time: 110.535s
| Adam | epoch: 009 | loss: 0.13733 - acc: 0.9570 -- iter: 19584/22500
Training Step: 3123  | total loss: [1m[32m0.13468[0m[0m | time: 110.884s
| Adam | epoch: 009 | loss: 0.13468 - acc: 0.9566 -- iter: 19648/22500
Training Step: 3124  | total loss: [1m[32m0.13130[0m[0m | time: 111.230s
| Adam | epoch: 009 | loss: 0.13130 - acc: 0.9578 -- iter: 19712/22500
Training Step: 3125  | total loss: [1m[32m0.13361[0m[0m | time: 111.601s
| Adam | epoch: 009 | loss: 0.13361 - acc: 0.9542 -- iter: 19776/22500
Training Step: 3126  | total loss: [1m[32m0.12256[0m[0m | time: 111.973s
| Adam | epoch: 009 | loss: 0.12256 - acc: 0.9588 -- iter: 19840/22500
Training Step: 3127  | total loss: [1m[32m0.12994[0m[0m | time: 112.322s
| Adam | epoch: 009 | loss: 0.12994 - acc: 0.9567 -- iter: 19904/22500
Training Step: 3128  | total loss: [1m[32m0.13555[0m[0m | time: 112.673s
| Adam | epoch: 009 | loss: 0.13555 - acc: 0.9532 -- iter: 19968/22500
Training Step: 3129  | total loss: [1m[32m0.13301[0m[0m | time: 113.018s
| Adam | epoch: 009 | loss: 0.13301 - acc: 0.9547 -- iter: 20032/22500
Training Step: 3130  | total loss: [1m[32m0.13003[0m[0m | time: 113.366s
| Adam | epoch: 009 | loss: 0.13003 - acc: 0.9577 -- iter: 20096/22500
Training Step: 3131  | total loss: [1m[32m0.12263[0m[0m | time: 113.716s
| Adam | epoch: 009 | loss: 0.12263 - acc: 0.9619 -- iter: 20160/22500
Training Step: 3132  | total loss: [1m[32m0.11639[0m[0m | time: 114.079s
| Adam | epoch: 009 | loss: 0.11639 - acc: 0.9642 -- iter: 20224/22500
Training Step: 3133  | total loss: [1m[32m0.11468[0m[0m | time: 114.428s
| Adam | epoch: 009 | loss: 0.11468 - acc: 0.9662 -- iter: 20288/22500
Training Step: 3134  | total loss: [1m[32m0.11700[0m[0m | time: 114.777s
| Adam | epoch: 009 | loss: 0.11700 - acc: 0.9633 -- iter: 20352/22500
Training Step: 3135  | total loss: [1m[32m0.11636[0m[0m | time: 115.129s
| Adam | epoch: 009 | loss: 0.11636 - acc: 0.9623 -- iter: 20416/22500
Training Step: 3136  | total loss: [1m[32m0.12188[0m[0m | time: 115.503s
| Adam | epoch: 009 | loss: 0.12188 - acc: 0.9598 -- iter: 20480/22500
Training Step: 3137  | total loss: [1m[32m0.11815[0m[0m | time: 115.878s
| Adam | epoch: 009 | loss: 0.11815 - acc: 0.9607 -- iter: 20544/22500
Training Step: 3138  | total loss: [1m[32m0.12085[0m[0m | time: 116.227s
| Adam | epoch: 009 | loss: 0.12085 - acc: 0.9631 -- iter: 20608/22500
Training Step: 3139  | total loss: [1m[32m0.12099[0m[0m | time: 116.583s
| Adam | epoch: 009 | loss: 0.12099 - acc: 0.9621 -- iter: 20672/22500
Training Step: 3140  | total loss: [1m[32m0.11644[0m[0m | time: 116.930s
| Adam | epoch: 009 | loss: 0.11644 - acc: 0.9628 -- iter: 20736/22500
Training Step: 3141  | total loss: [1m[32m0.11852[0m[0m | time: 117.277s
| Adam | epoch: 009 | loss: 0.11852 - acc: 0.9618 -- iter: 20800/22500
Training Step: 3142  | total loss: [1m[32m0.12084[0m[0m | time: 117.632s
| Adam | epoch: 009 | loss: 0.12084 - acc: 0.9609 -- iter: 20864/22500
Training Step: 3143  | total loss: [1m[32m0.11823[0m[0m | time: 117.984s
| Adam | epoch: 009 | loss: 0.11823 - acc: 0.9617 -- iter: 20928/22500
Training Step: 3144  | total loss: [1m[32m0.11633[0m[0m | time: 118.334s
| Adam | epoch: 009 | loss: 0.11633 - acc: 0.9608 -- iter: 20992/22500
Training Step: 3145  | total loss: [1m[32m0.11340[0m[0m | time: 118.683s
| Adam | epoch: 009 | loss: 0.11340 - acc: 0.9616 -- iter: 21056/22500
Training Step: 3146  | total loss: [1m[32m0.10503[0m[0m | time: 119.037s
| Adam | epoch: 009 | loss: 0.10503 - acc: 0.9655 -- iter: 21120/22500
Training Step: 3147  | total loss: [1m[32m0.09941[0m[0m | time: 119.385s
| Adam | epoch: 009 | loss: 0.09941 - acc: 0.9689 -- iter: 21184/22500
Training Step: 3148  | total loss: [1m[32m0.09637[0m[0m | time: 119.750s
| Adam | epoch: 009 | loss: 0.09637 - acc: 0.9689 -- iter: 21248/22500
Training Step: 3149  | total loss: [1m[32m0.09177[0m[0m | time: 120.123s
| Adam | epoch: 009 | loss: 0.09177 - acc: 0.9705 -- iter: 21312/22500
Training Step: 3150  | total loss: [1m[32m0.11088[0m[0m | time: 120.472s
| Adam | epoch: 009 | loss: 0.11088 - acc: 0.9656 -- iter: 21376/22500
Training Step: 3151  | total loss: [1m[32m0.10429[0m[0m | time: 120.821s
| Adam | epoch: 009 | loss: 0.10429 - acc: 0.9675 -- iter: 21440/22500
Training Step: 3152  | total loss: [1m[32m0.10262[0m[0m | time: 121.173s
| Adam | epoch: 009 | loss: 0.10262 - acc: 0.9676 -- iter: 21504/22500
Training Step: 3153  | total loss: [1m[32m0.09453[0m[0m | time: 121.527s
| Adam | epoch: 009 | loss: 0.09453 - acc: 0.9708 -- iter: 21568/22500
Training Step: 3154  | total loss: [1m[32m0.09231[0m[0m | time: 121.878s
| Adam | epoch: 009 | loss: 0.09231 - acc: 0.9722 -- iter: 21632/22500
Training Step: 3155  | total loss: [1m[32m0.08501[0m[0m | time: 122.227s
| Adam | epoch: 009 | loss: 0.08501 - acc: 0.9750 -- iter: 21696/22500
Training Step: 3156  | total loss: [1m[32m0.07998[0m[0m | time: 122.577s
| Adam | epoch: 009 | loss: 0.07998 - acc: 0.9759 -- iter: 21760/22500
Training Step: 3157  | total loss: [1m[32m0.07397[0m[0m | time: 122.928s
| Adam | epoch: 009 | loss: 0.07397 - acc: 0.9783 -- iter: 21824/22500
Training Step: 3158  | total loss: [1m[32m0.07089[0m[0m | time: 123.283s
| Adam | epoch: 009 | loss: 0.07089 - acc: 0.9789 -- iter: 21888/22500
Training Step: 3159  | total loss: [1m[32m0.06967[0m[0m | time: 123.649s
| Adam | epoch: 009 | loss: 0.06967 - acc: 0.9795 -- iter: 21952/22500
Training Step: 3160  | total loss: [1m[32m0.06461[0m[0m | time: 124.024s
| Adam | epoch: 009 | loss: 0.06461 - acc: 0.9815 -- iter: 22016/22500
Training Step: 3161  | total loss: [1m[32m0.06637[0m[0m | time: 124.374s
| Adam | epoch: 009 | loss: 0.06637 - acc: 0.9818 -- iter: 22080/22500
Training Step: 3162  | total loss: [1m[32m0.06331[0m[0m | time: 124.727s
| Adam | epoch: 009 | loss: 0.06331 - acc: 0.9821 -- iter: 22144/22500
Training Step: 3163  | total loss: [1m[32m0.05851[0m[0m | time: 125.082s
| Adam | epoch: 009 | loss: 0.05851 - acc: 0.9839 -- iter: 22208/22500
Training Step: 3164  | total loss: [1m[32m0.05509[0m[0m | time: 125.436s
| Adam | epoch: 009 | loss: 0.05509 - acc: 0.9839 -- iter: 22272/22500
Training Step: 3165  | total loss: [1m[32m0.05269[0m[0m | time: 125.787s
| Adam | epoch: 009 | loss: 0.05269 - acc: 0.9840 -- iter: 22336/22500
Training Step: 3166  | total loss: [1m[32m0.06336[0m[0m | time: 126.137s
| Adam | epoch: 009 | loss: 0.06336 - acc: 0.9809 -- iter: 22400/22500
Training Step: 3167  | total loss: [1m[32m0.06047[0m[0m | time: 126.486s
| Adam | epoch: 009 | loss: 0.06047 - acc: 0.9812 -- iter: 22464/22500
Training Step: 3168  | total loss: [1m[32m0.05834[0m[0m | time: 129.995s
| Adam | epoch: 009 | loss: 0.05834 - acc: 0.9815 | val_loss: 0.74447 - val_acc: 0.7964 -- iter: 22500/22500
--
Training Step: 3169  | total loss: [1m[32m0.05702[0m[0m | time: 0.354s
| Adam | epoch: 010 | loss: 0.05702 - acc: 0.9818 -- iter: 00064/22500
Training Step: 3170  | total loss: [1m[32m0.06482[0m[0m | time: 0.710s
| Adam | epoch: 010 | loss: 0.06482 - acc: 0.9805 -- iter: 00128/22500
Training Step: 3171  | total loss: [1m[32m0.06430[0m[0m | time: 1.167s
| Adam | epoch: 010 | loss: 0.06430 - acc: 0.9809 -- iter: 00192/22500
Training Step: 3172  | total loss: [1m[32m0.07482[0m[0m | time: 1.516s
| Adam | epoch: 010 | loss: 0.07482 - acc: 0.9766 -- iter: 00256/22500
Training Step: 3173  | total loss: [1m[32m0.07234[0m[0m | time: 1.932s
| Adam | epoch: 010 | loss: 0.07234 - acc: 0.9773 -- iter: 00320/22500
Training Step: 3174  | total loss: [1m[32m0.08029[0m[0m | time: 2.266s
| Adam | epoch: 010 | loss: 0.08029 - acc: 0.9749 -- iter: 00384/22500
Training Step: 3175  | total loss: [1m[32m0.09196[0m[0m | time: 2.573s
| Adam | epoch: 010 | loss: 0.09196 - acc: 0.9696 -- iter: 00448/22500
Training Step: 3176  | total loss: [1m[32m0.08911[0m[0m | time: 2.904s
| Adam | epoch: 010 | loss: 0.08911 - acc: 0.9695 -- iter: 00512/22500
Training Step: 3177  | total loss: [1m[32m0.09472[0m[0m | time: 3.205s
| Adam | epoch: 010 | loss: 0.09472 - acc: 0.9642 -- iter: 00576/22500
Training Step: 3178  | total loss: [1m[32m0.09280[0m[0m | time: 3.599s
| Adam | epoch: 010 | loss: 0.09280 - acc: 0.9650 -- iter: 00640/22500
Training Step: 3179  | total loss: [1m[32m0.09671[0m[0m | time: 3.984s
| Adam | epoch: 010 | loss: 0.09671 - acc: 0.9623 -- iter: 00704/22500
Training Step: 3180  | total loss: [1m[32m0.10085[0m[0m | time: 4.363s
| Adam | epoch: 010 | loss: 0.10085 - acc: 0.9629 -- iter: 00768/22500
Training Step: 3181  | total loss: [1m[32m0.09847[0m[0m | time: 4.717s
| Adam | epoch: 010 | loss: 0.09847 - acc: 0.9620 -- iter: 00832/22500
Training Step: 3182  | total loss: [1m[32m0.09133[0m[0m | time: 5.086s
| Adam | epoch: 010 | loss: 0.09133 - acc: 0.9658 -- iter: 00896/22500
Training Step: 3183  | total loss: [1m[32m0.08937[0m[0m | time: 5.460s
| Adam | epoch: 010 | loss: 0.08937 - acc: 0.9661 -- iter: 00960/22500
Training Step: 3184  | total loss: [1m[32m0.10285[0m[0m | time: 5.818s
| Adam | epoch: 010 | loss: 0.10285 - acc: 0.9648 -- iter: 01024/22500
Training Step: 3185  | total loss: [1m[32m0.10331[0m[0m | time: 6.172s
| Adam | epoch: 010 | loss: 0.10331 - acc: 0.9620 -- iter: 01088/22500
Training Step: 3186  | total loss: [1m[32m0.10582[0m[0m | time: 6.530s
| Adam | epoch: 010 | loss: 0.10582 - acc: 0.9611 -- iter: 01152/22500
Training Step: 3187  | total loss: [1m[32m0.11170[0m[0m | time: 6.883s
| Adam | epoch: 010 | loss: 0.11170 - acc: 0.9619 -- iter: 01216/22500
Training Step: 3188  | total loss: [1m[32m0.10733[0m[0m | time: 7.233s
| Adam | epoch: 010 | loss: 0.10733 - acc: 0.9610 -- iter: 01280/22500
Training Step: 3189  | total loss: [1m[32m0.10423[0m[0m | time: 7.585s
| Adam | epoch: 010 | loss: 0.10423 - acc: 0.9634 -- iter: 01344/22500
Training Step: 3190  | total loss: [1m[32m0.10557[0m[0m | time: 7.938s
| Adam | epoch: 010 | loss: 0.10557 - acc: 0.9639 -- iter: 01408/22500
Training Step: 3191  | total loss: [1m[32m0.09736[0m[0m | time: 8.295s
| Adam | epoch: 010 | loss: 0.09736 - acc: 0.9675 -- iter: 01472/22500
Training Step: 3192  | total loss: [1m[32m0.09065[0m[0m | time: 8.645s
| Adam | epoch: 010 | loss: 0.09065 - acc: 0.9708 -- iter: 01536/22500
Training Step: 3193  | total loss: [1m[32m0.08802[0m[0m | time: 9.011s
| Adam | epoch: 010 | loss: 0.08802 - acc: 0.9721 -- iter: 01600/22500
Training Step: 3194  | total loss: [1m[32m0.09025[0m[0m | time: 9.386s
| Adam | epoch: 010 | loss: 0.09025 - acc: 0.9718 -- iter: 01664/22500
Training Step: 3195  | total loss: [1m[32m0.09031[0m[0m | time: 9.742s
| Adam | epoch: 010 | loss: 0.09031 - acc: 0.9715 -- iter: 01728/22500
Training Step: 3196  | total loss: [1m[32m0.08772[0m[0m | time: 10.088s
| Adam | epoch: 010 | loss: 0.08772 - acc: 0.9728 -- iter: 01792/22500
Training Step: 3197  | total loss: [1m[32m0.08447[0m[0m | time: 10.440s
| Adam | epoch: 010 | loss: 0.08447 - acc: 0.9724 -- iter: 01856/22500
Training Step: 3198  | total loss: [1m[32m0.08607[0m[0m | time: 10.789s
| Adam | epoch: 010 | loss: 0.08607 - acc: 0.9704 -- iter: 01920/22500
Training Step: 3199  | total loss: [1m[32m0.08786[0m[0m | time: 11.136s
| Adam | epoch: 010 | loss: 0.08786 - acc: 0.9703 -- iter: 01984/22500
Training Step: 3200  | total loss: [1m[32m0.08653[0m[0m | time: 14.680s
| Adam | epoch: 010 | loss: 0.08653 - acc: 0.9717 | val_loss: 0.69206 - val_acc: 0.7988 -- iter: 02048/22500
--
Training Step: 3201  | total loss: [1m[32m0.08336[0m[0m | time: 15.033s
| Adam | epoch: 010 | loss: 0.08336 - acc: 0.9714 -- iter: 02112/22500
Training Step: 3202  | total loss: [1m[32m0.09546[0m[0m | time: 15.391s
| Adam | epoch: 010 | loss: 0.09546 - acc: 0.9680 -- iter: 02176/22500
Training Step: 3203  | total loss: [1m[32m0.10012[0m[0m | time: 15.741s
| Adam | epoch: 010 | loss: 0.10012 - acc: 0.9634 -- iter: 02240/22500
Training Step: 3204  | total loss: [1m[32m0.09924[0m[0m | time: 16.090s
| Adam | epoch: 010 | loss: 0.09924 - acc: 0.9655 -- iter: 02304/22500
Training Step: 3205  | total loss: [1m[32m0.09490[0m[0m | time: 16.463s
| Adam | epoch: 010 | loss: 0.09490 - acc: 0.9674 -- iter: 02368/22500
Training Step: 3206  | total loss: [1m[32m0.09114[0m[0m | time: 16.832s
| Adam | epoch: 010 | loss: 0.09114 - acc: 0.9706 -- iter: 02432/22500
Training Step: 3207  | total loss: [1m[32m0.08863[0m[0m | time: 17.184s
| Adam | epoch: 010 | loss: 0.08863 - acc: 0.9720 -- iter: 02496/22500
Training Step: 3208  | total loss: [1m[32m0.08631[0m[0m | time: 17.534s
| Adam | epoch: 010 | loss: 0.08631 - acc: 0.9717 -- iter: 02560/22500
Training Step: 3209  | total loss: [1m[32m0.08272[0m[0m | time: 17.887s
| Adam | epoch: 010 | loss: 0.08272 - acc: 0.9730 -- iter: 02624/22500
Training Step: 3210  | total loss: [1m[32m0.10497[0m[0m | time: 18.236s
| Adam | epoch: 010 | loss: 0.10497 - acc: 0.9663 -- iter: 02688/22500
Training Step: 3211  | total loss: [1m[32m0.09794[0m[0m | time: 18.590s
| Adam | epoch: 010 | loss: 0.09794 - acc: 0.9697 -- iter: 02752/22500
Training Step: 3212  | total loss: [1m[32m0.09381[0m[0m | time: 18.942s
| Adam | epoch: 010 | loss: 0.09381 - acc: 0.9711 -- iter: 02816/22500
Training Step: 3213  | total loss: [1m[32m0.09656[0m[0m | time: 19.299s
| Adam | epoch: 010 | loss: 0.09656 - acc: 0.9709 -- iter: 02880/22500
Training Step: 3214  | total loss: [1m[32m0.09771[0m[0m | time: 19.653s
| Adam | epoch: 010 | loss: 0.09771 - acc: 0.9707 -- iter: 02944/22500
Training Step: 3215  | total loss: [1m[32m0.09380[0m[0m | time: 20.007s
| Adam | epoch: 010 | loss: 0.09380 - acc: 0.9705 -- iter: 03008/22500
Training Step: 3216  | total loss: [1m[32m0.09989[0m[0m | time: 20.385s
| Adam | epoch: 010 | loss: 0.09989 - acc: 0.9687 -- iter: 03072/22500
Training Step: 3217  | total loss: [1m[32m0.09294[0m[0m | time: 20.757s
| Adam | epoch: 010 | loss: 0.09294 - acc: 0.9719 -- iter: 03136/22500
Training Step: 3218  | total loss: [1m[32m0.09056[0m[0m | time: 21.121s
| Adam | epoch: 010 | loss: 0.09056 - acc: 0.9731 -- iter: 03200/22500
Training Step: 3219  | total loss: [1m[32m0.08318[0m[0m | time: 21.494s
| Adam | epoch: 010 | loss: 0.08318 - acc: 0.9758 -- iter: 03264/22500
Training Step: 3220  | total loss: [1m[32m0.08088[0m[0m | time: 21.857s
| Adam | epoch: 010 | loss: 0.08088 - acc: 0.9767 -- iter: 03328/22500
Training Step: 3221  | total loss: [1m[32m0.08229[0m[0m | time: 22.213s
| Adam | epoch: 010 | loss: 0.08229 - acc: 0.9759 -- iter: 03392/22500
Training Step: 3222  | total loss: [1m[32m0.07923[0m[0m | time: 22.563s
| Adam | epoch: 010 | loss: 0.07923 - acc: 0.9767 -- iter: 03456/22500
Training Step: 3223  | total loss: [1m[32m0.07860[0m[0m | time: 22.911s
| Adam | epoch: 010 | loss: 0.07860 - acc: 0.9744 -- iter: 03520/22500
Training Step: 3224  | total loss: [1m[32m0.07483[0m[0m | time: 23.259s
| Adam | epoch: 010 | loss: 0.07483 - acc: 0.9754 -- iter: 03584/22500
Training Step: 3225  | total loss: [1m[32m0.06959[0m[0m | time: 23.609s
| Adam | epoch: 010 | loss: 0.06959 - acc: 0.9778 -- iter: 03648/22500
Training Step: 3226  | total loss: [1m[32m0.06474[0m[0m | time: 23.956s
| Adam | epoch: 010 | loss: 0.06474 - acc: 0.9800 -- iter: 03712/22500
Training Step: 3227  | total loss: [1m[32m0.06315[0m[0m | time: 24.320s
| Adam | epoch: 010 | loss: 0.06315 - acc: 0.9805 -- iter: 03776/22500
Training Step: 3228  | total loss: [1m[32m0.06180[0m[0m | time: 24.693s
| Adam | epoch: 010 | loss: 0.06180 - acc: 0.9809 -- iter: 03840/22500
Training Step: 3229  | total loss: [1m[32m0.06750[0m[0m | time: 25.044s
| Adam | epoch: 010 | loss: 0.06750 - acc: 0.9812 -- iter: 03904/22500
Training Step: 3230  | total loss: [1m[32m0.06864[0m[0m | time: 25.396s
| Adam | epoch: 010 | loss: 0.06864 - acc: 0.9800 -- iter: 03968/22500
Training Step: 3231  | total loss: [1m[32m0.07256[0m[0m | time: 25.742s
| Adam | epoch: 010 | loss: 0.07256 - acc: 0.9789 -- iter: 04032/22500
Training Step: 3232  | total loss: [1m[32m0.06968[0m[0m | time: 26.093s
| Adam | epoch: 010 | loss: 0.06968 - acc: 0.9810 -- iter: 04096/22500
Training Step: 3233  | total loss: [1m[32m0.07857[0m[0m | time: 26.444s
| Adam | epoch: 010 | loss: 0.07857 - acc: 0.9766 -- iter: 04160/22500
Training Step: 3234  | total loss: [1m[32m0.08381[0m[0m | time: 26.792s
| Adam | epoch: 010 | loss: 0.08381 - acc: 0.9758 -- iter: 04224/22500
Training Step: 3235  | total loss: [1m[32m0.07782[0m[0m | time: 27.142s
| Adam | epoch: 010 | loss: 0.07782 - acc: 0.9782 -- iter: 04288/22500
Training Step: 3236  | total loss: [1m[32m0.08064[0m[0m | time: 27.490s
| Adam | epoch: 010 | loss: 0.08064 - acc: 0.9789 -- iter: 04352/22500
Training Step: 3237  | total loss: [1m[32m0.07604[0m[0m | time: 27.839s
| Adam | epoch: 010 | loss: 0.07604 - acc: 0.9810 -- iter: 04416/22500
Training Step: 3238  | total loss: [1m[32m0.07751[0m[0m | time: 28.211s
| Adam | epoch: 010 | loss: 0.07751 - acc: 0.9782 -- iter: 04480/22500
Training Step: 3239  | total loss: [1m[32m0.08000[0m[0m | time: 28.588s
| Adam | epoch: 010 | loss: 0.08000 - acc: 0.9757 -- iter: 04544/22500
Training Step: 3240  | total loss: [1m[32m0.08139[0m[0m | time: 28.965s
| Adam | epoch: 010 | loss: 0.08139 - acc: 0.9750 -- iter: 04608/22500
Training Step: 3241  | total loss: [1m[32m0.08017[0m[0m | time: 29.315s
| Adam | epoch: 010 | loss: 0.08017 - acc: 0.9744 -- iter: 04672/22500
Training Step: 3242  | total loss: [1m[32m0.07503[0m[0m | time: 29.670s
| Adam | epoch: 010 | loss: 0.07503 - acc: 0.9769 -- iter: 04736/22500
Training Step: 3243  | total loss: [1m[32m0.07655[0m[0m | time: 30.022s
| Adam | epoch: 010 | loss: 0.07655 - acc: 0.9761 -- iter: 04800/22500
Training Step: 3244  | total loss: [1m[32m0.07392[0m[0m | time: 30.411s
| Adam | epoch: 010 | loss: 0.07392 - acc: 0.9769 -- iter: 04864/22500
Training Step: 3245  | total loss: [1m[32m0.07567[0m[0m | time: 30.740s
| Adam | epoch: 010 | loss: 0.07567 - acc: 0.9761 -- iter: 04928/22500
Training Step: 3246  | total loss: [1m[32m0.07910[0m[0m | time: 31.027s
| Adam | epoch: 010 | loss: 0.07910 - acc: 0.9754 -- iter: 04992/22500
Training Step: 3247  | total loss: [1m[32m0.07967[0m[0m | time: 31.310s
| Adam | epoch: 010 | loss: 0.07967 - acc: 0.9747 -- iter: 05056/22500
Training Step: 3248  | total loss: [1m[32m0.08142[0m[0m | time: 31.596s
| Adam | epoch: 010 | loss: 0.08142 - acc: 0.9741 -- iter: 05120/22500
Training Step: 3249  | total loss: [1m[32m0.08878[0m[0m | time: 31.884s
| Adam | epoch: 010 | loss: 0.08878 - acc: 0.9705 -- iter: 05184/22500
Training Step: 3250  | total loss: [1m[32m0.08509[0m[0m | time: 32.261s
| Adam | epoch: 010 | loss: 0.08509 - acc: 0.9719 -- iter: 05248/22500
Training Step: 3251  | total loss: [1m[32m0.07923[0m[0m | time: 32.627s
| Adam | epoch: 010 | loss: 0.07923 - acc: 0.9747 -- iter: 05312/22500
Training Step: 3252  | total loss: [1m[32m0.07773[0m[0m | time: 32.976s
| Adam | epoch: 010 | loss: 0.07773 - acc: 0.9741 -- iter: 05376/22500
Training Step: 3253  | total loss: [1m[32m0.08847[0m[0m | time: 33.334s
| Adam | epoch: 010 | loss: 0.08847 - acc: 0.9735 -- iter: 05440/22500
Training Step: 3254  | total loss: [1m[32m0.09609[0m[0m | time: 33.687s
| Adam | epoch: 010 | loss: 0.09609 - acc: 0.9684 -- iter: 05504/22500
Training Step: 3255  | total loss: [1m[32m0.09250[0m[0m | time: 34.038s
| Adam | epoch: 010 | loss: 0.09250 - acc: 0.9700 -- iter: 05568/22500
Training Step: 3256  | total loss: [1m[32m0.10472[0m[0m | time: 34.389s
| Adam | epoch: 010 | loss: 0.10472 - acc: 0.9652 -- iter: 05632/22500
Training Step: 3257  | total loss: [1m[32m0.10092[0m[0m | time: 34.737s
| Adam | epoch: 010 | loss: 0.10092 - acc: 0.9655 -- iter: 05696/22500
Training Step: 3258  | total loss: [1m[32m0.10096[0m[0m | time: 35.091s
| Adam | epoch: 010 | loss: 0.10096 - acc: 0.9674 -- iter: 05760/22500
Training Step: 3259  | total loss: [1m[32m0.09580[0m[0m | time: 35.441s
| Adam | epoch: 010 | loss: 0.09580 - acc: 0.9691 -- iter: 05824/22500
Training Step: 3260  | total loss: [1m[32m0.09012[0m[0m | time: 35.789s
| Adam | epoch: 010 | loss: 0.09012 - acc: 0.9722 -- iter: 05888/22500
Training Step: 3261  | total loss: [1m[32m0.08434[0m[0m | time: 36.160s
| Adam | epoch: 010 | loss: 0.08434 - acc: 0.9750 -- iter: 05952/22500
Training Step: 3262  | total loss: [1m[32m0.08762[0m[0m | time: 36.545s
| Adam | epoch: 010 | loss: 0.08762 - acc: 0.9744 -- iter: 06016/22500
Training Step: 3263  | total loss: [1m[32m0.08160[0m[0m | time: 36.894s
| Adam | epoch: 010 | loss: 0.08160 - acc: 0.9769 -- iter: 06080/22500
Training Step: 3264  | total loss: [1m[32m0.08178[0m[0m | time: 37.260s
| Adam | epoch: 010 | loss: 0.08178 - acc: 0.9745 -- iter: 06144/22500
Training Step: 3265  | total loss: [1m[32m0.07562[0m[0m | time: 37.609s
| Adam | epoch: 010 | loss: 0.07562 - acc: 0.9771 -- iter: 06208/22500
Training Step: 3266  | total loss: [1m[32m0.07570[0m[0m | time: 37.957s
| Adam | epoch: 010 | loss: 0.07570 - acc: 0.9763 -- iter: 06272/22500
Training Step: 3267  | total loss: [1m[32m0.07914[0m[0m | time: 38.305s
| Adam | epoch: 010 | loss: 0.07914 - acc: 0.9739 -- iter: 06336/22500
Training Step: 3268  | total loss: [1m[32m0.07647[0m[0m | time: 38.655s
| Adam | epoch: 010 | loss: 0.07647 - acc: 0.9750 -- iter: 06400/22500
Training Step: 3269  | total loss: [1m[32m0.08651[0m[0m | time: 39.004s
| Adam | epoch: 010 | loss: 0.08651 - acc: 0.9712 -- iter: 06464/22500
Training Step: 3270  | total loss: [1m[32m0.08101[0m[0m | time: 39.352s
| Adam | epoch: 010 | loss: 0.08101 - acc: 0.9741 -- iter: 06528/22500
Training Step: 3271  | total loss: [1m[32m0.07541[0m[0m | time: 39.700s
| Adam | epoch: 010 | loss: 0.07541 - acc: 0.9767 -- iter: 06592/22500
Training Step: 3272  | total loss: [1m[32m0.07219[0m[0m | time: 40.050s
| Adam | epoch: 010 | loss: 0.07219 - acc: 0.9775 -- iter: 06656/22500
Training Step: 3273  | total loss: [1m[32m0.07005[0m[0m | time: 40.402s
| Adam | epoch: 010 | loss: 0.07005 - acc: 0.9782 -- iter: 06720/22500
Training Step: 3274  | total loss: [1m[32m0.06983[0m[0m | time: 40.756s
| Adam | epoch: 010 | loss: 0.06983 - acc: 0.9772 -- iter: 06784/22500
Training Step: 3275  | total loss: [1m[32m0.07262[0m[0m | time: 41.106s
| Adam | epoch: 010 | loss: 0.07262 - acc: 0.9748 -- iter: 06848/22500
Training Step: 3276  | total loss: [1m[32m0.08446[0m[0m | time: 41.456s
| Adam | epoch: 010 | loss: 0.08446 - acc: 0.9695 -- iter: 06912/22500
Training Step: 3277  | total loss: [1m[32m0.09182[0m[0m | time: 41.806s
| Adam | epoch: 010 | loss: 0.09182 - acc: 0.9679 -- iter: 06976/22500
Training Step: 3278  | total loss: [1m[32m0.08493[0m[0m | time: 42.156s
| Adam | epoch: 010 | loss: 0.08493 - acc: 0.9711 -- iter: 07040/22500
Training Step: 3279  | total loss: [1m[32m0.08559[0m[0m | time: 42.505s
| Adam | epoch: 010 | loss: 0.08559 - acc: 0.9693 -- iter: 07104/22500
Training Step: 3280  | total loss: [1m[32m0.08048[0m[0m | time: 42.855s
| Adam | epoch: 010 | loss: 0.08048 - acc: 0.9708 -- iter: 07168/22500
Training Step: 3281  | total loss: [1m[32m0.08245[0m[0m | time: 43.202s
| Adam | epoch: 010 | loss: 0.08245 - acc: 0.9722 -- iter: 07232/22500
Training Step: 3282  | total loss: [1m[32m0.07748[0m[0m | time: 43.552s
| Adam | epoch: 010 | loss: 0.07748 - acc: 0.9749 -- iter: 07296/22500
Training Step: 3283  | total loss: [1m[32m0.08029[0m[0m | time: 43.904s
| Adam | epoch: 010 | loss: 0.08029 - acc: 0.9759 -- iter: 07360/22500
Training Step: 3284  | total loss: [1m[32m0.07545[0m[0m | time: 44.289s
| Adam | epoch: 010 | loss: 0.07545 - acc: 0.9783 -- iter: 07424/22500
Training Step: 3285  | total loss: [1m[32m0.07239[0m[0m | time: 44.726s
| Adam | epoch: 010 | loss: 0.07239 - acc: 0.9789 -- iter: 07488/22500
Training Step: 3286  | total loss: [1m[32m0.06790[0m[0m | time: 45.054s
| Adam | epoch: 010 | loss: 0.06790 - acc: 0.9810 -- iter: 07552/22500
Training Step: 3287  | total loss: [1m[32m0.06571[0m[0m | time: 45.416s
| Adam | epoch: 010 | loss: 0.06571 - acc: 0.9814 -- iter: 07616/22500
Training Step: 3288  | total loss: [1m[32m0.07463[0m[0m | time: 45.778s
| Adam | epoch: 010 | loss: 0.07463 - acc: 0.9770 -- iter: 07680/22500
Training Step: 3289  | total loss: [1m[32m0.07397[0m[0m | time: 46.138s
| Adam | epoch: 010 | loss: 0.07397 - acc: 0.9777 -- iter: 07744/22500
Training Step: 3290  | total loss: [1m[32m0.07903[0m[0m | time: 46.491s
| Adam | epoch: 010 | loss: 0.07903 - acc: 0.9752 -- iter: 07808/22500
Training Step: 3291  | total loss: [1m[32m0.08841[0m[0m | time: 46.844s
| Adam | epoch: 010 | loss: 0.08841 - acc: 0.9730 -- iter: 07872/22500
Training Step: 3292  | total loss: [1m[32m0.08176[0m[0m | time: 47.192s
| Adam | epoch: 010 | loss: 0.08176 - acc: 0.9757 -- iter: 07936/22500
Training Step: 3293  | total loss: [1m[32m0.07571[0m[0m | time: 47.546s
| Adam | epoch: 010 | loss: 0.07571 - acc: 0.9782 -- iter: 08000/22500
Training Step: 3294  | total loss: [1m[32m0.07022[0m[0m | time: 47.907s
| Adam | epoch: 010 | loss: 0.07022 - acc: 0.9803 -- iter: 08064/22500
Training Step: 3295  | total loss: [1m[32m0.07610[0m[0m | time: 48.276s
| Adam | epoch: 010 | loss: 0.07610 - acc: 0.9776 -- iter: 08128/22500
Training Step: 3296  | total loss: [1m[32m0.07903[0m[0m | time: 48.652s
| Adam | epoch: 010 | loss: 0.07903 - acc: 0.9736 -- iter: 08192/22500
Training Step: 3297  | total loss: [1m[32m0.07628[0m[0m | time: 49.038s
| Adam | epoch: 010 | loss: 0.07628 - acc: 0.9747 -- iter: 08256/22500
Training Step: 3298  | total loss: [1m[32m0.07833[0m[0m | time: 49.322s
| Adam | epoch: 010 | loss: 0.07833 - acc: 0.9757 -- iter: 08320/22500
Training Step: 3299  | total loss: [1m[32m0.08264[0m[0m | time: 49.606s
| Adam | epoch: 010 | loss: 0.08264 - acc: 0.9750 -- iter: 08384/22500
Training Step: 3300  | total loss: [1m[32m0.08031[0m[0m | time: 49.892s
| Adam | epoch: 010 | loss: 0.08031 - acc: 0.9759 -- iter: 08448/22500
Training Step: 3301  | total loss: [1m[32m0.08208[0m[0m | time: 50.176s
| Adam | epoch: 010 | loss: 0.08208 - acc: 0.9752 -- iter: 08512/22500
Training Step: 3302  | total loss: [1m[32m0.08328[0m[0m | time: 50.469s
| Adam | epoch: 010 | loss: 0.08328 - acc: 0.9745 -- iter: 08576/22500
Training Step: 3303  | total loss: [1m[32m0.07783[0m[0m | time: 50.836s
| Adam | epoch: 010 | loss: 0.07783 - acc: 0.9755 -- iter: 08640/22500
Training Step: 3304  | total loss: [1m[32m0.07448[0m[0m | time: 51.183s
| Adam | epoch: 010 | loss: 0.07448 - acc: 0.9764 -- iter: 08704/22500
Training Step: 3305  | total loss: [1m[32m0.07061[0m[0m | time: 51.533s
| Adam | epoch: 010 | loss: 0.07061 - acc: 0.9756 -- iter: 08768/22500
Training Step: 3306  | total loss: [1m[32m0.06483[0m[0m | time: 51.910s
| Adam | epoch: 010 | loss: 0.06483 - acc: 0.9781 -- iter: 08832/22500
Training Step: 3307  | total loss: [1m[32m0.06443[0m[0m | time: 52.281s
| Adam | epoch: 010 | loss: 0.06443 - acc: 0.9771 -- iter: 08896/22500
Training Step: 3308  | total loss: [1m[32m0.06532[0m[0m | time: 52.633s
| Adam | epoch: 010 | loss: 0.06532 - acc: 0.9763 -- iter: 08960/22500
Training Step: 3309  | total loss: [1m[32m0.06832[0m[0m | time: 52.985s
| Adam | epoch: 010 | loss: 0.06832 - acc: 0.9740 -- iter: 09024/22500
Training Step: 3310  | total loss: [1m[32m0.07009[0m[0m | time: 53.333s
| Adam | epoch: 010 | loss: 0.07009 - acc: 0.9750 -- iter: 09088/22500
Training Step: 3311  | total loss: [1m[32m0.06545[0m[0m | time: 53.685s
| Adam | epoch: 010 | loss: 0.06545 - acc: 0.9760 -- iter: 09152/22500
Training Step: 3312  | total loss: [1m[32m0.07058[0m[0m | time: 54.038s
| Adam | epoch: 010 | loss: 0.07058 - acc: 0.9752 -- iter: 09216/22500
Training Step: 3313  | total loss: [1m[32m0.06781[0m[0m | time: 54.388s
| Adam | epoch: 010 | loss: 0.06781 - acc: 0.9762 -- iter: 09280/22500
Training Step: 3314  | total loss: [1m[32m0.06964[0m[0m | time: 54.743s
| Adam | epoch: 010 | loss: 0.06964 - acc: 0.9739 -- iter: 09344/22500
Training Step: 3315  | total loss: [1m[32m0.07756[0m[0m | time: 55.097s
| Adam | epoch: 010 | loss: 0.07756 - acc: 0.9702 -- iter: 09408/22500
Training Step: 3316  | total loss: [1m[32m0.07156[0m[0m | time: 55.449s
| Adam | epoch: 010 | loss: 0.07156 - acc: 0.9732 -- iter: 09472/22500
Training Step: 3317  | total loss: [1m[32m0.06702[0m[0m | time: 55.798s
| Adam | epoch: 010 | loss: 0.06702 - acc: 0.9743 -- iter: 09536/22500
Training Step: 3318  | total loss: [1m[32m0.06928[0m[0m | time: 56.147s
| Adam | epoch: 010 | loss: 0.06928 - acc: 0.9722 -- iter: 09600/22500
Training Step: 3319  | total loss: [1m[32m0.06555[0m[0m | time: 56.502s
| Adam | epoch: 010 | loss: 0.06555 - acc: 0.9734 -- iter: 09664/22500
Training Step: 3320  | total loss: [1m[32m0.06421[0m[0m | time: 56.847s
| Adam | epoch: 010 | loss: 0.06421 - acc: 0.9729 -- iter: 09728/22500
Training Step: 3321  | total loss: [1m[32m0.05896[0m[0m | time: 57.203s
| Adam | epoch: 010 | loss: 0.05896 - acc: 0.9757 -- iter: 09792/22500
Training Step: 3322  | total loss: [1m[32m0.06379[0m[0m | time: 57.556s
| Adam | epoch: 010 | loss: 0.06379 - acc: 0.9734 -- iter: 09856/22500
Training Step: 3323  | total loss: [1m[32m0.08644[0m[0m | time: 57.943s
| Adam | epoch: 010 | loss: 0.08644 - acc: 0.9682 -- iter: 09920/22500
Training Step: 3324  | total loss: [1m[32m0.08717[0m[0m | time: 58.289s
| Adam | epoch: 010 | loss: 0.08717 - acc: 0.9699 -- iter: 09984/22500
Training Step: 3325  | total loss: [1m[32m0.08799[0m[0m | time: 58.685s
| Adam | epoch: 010 | loss: 0.08799 - acc: 0.9697 -- iter: 10048/22500
Training Step: 3326  | total loss: [1m[32m0.08198[0m[0m | time: 59.038s
| Adam | epoch: 010 | loss: 0.08198 - acc: 0.9728 -- iter: 10112/22500
Training Step: 3327  | total loss: [1m[32m0.08011[0m[0m | time: 59.386s
| Adam | epoch: 010 | loss: 0.08011 - acc: 0.9724 -- iter: 10176/22500
Training Step: 3328  | total loss: [1m[32m0.07358[0m[0m | time: 59.747s
| Adam | epoch: 010 | loss: 0.07358 - acc: 0.9751 -- iter: 10240/22500
Training Step: 3329  | total loss: [1m[32m0.07111[0m[0m | time: 60.140s
| Adam | epoch: 010 | loss: 0.07111 - acc: 0.9745 -- iter: 10304/22500
Training Step: 3330  | total loss: [1m[32m0.07122[0m[0m | time: 60.510s
| Adam | epoch: 010 | loss: 0.07122 - acc: 0.9739 -- iter: 10368/22500
Training Step: 3331  | total loss: [1m[32m0.07186[0m[0m | time: 60.861s
| Adam | epoch: 010 | loss: 0.07186 - acc: 0.9734 -- iter: 10432/22500
Training Step: 3332  | total loss: [1m[32m0.08470[0m[0m | time: 61.216s
| Adam | epoch: 010 | loss: 0.08470 - acc: 0.9698 -- iter: 10496/22500
Training Step: 3333  | total loss: [1m[32m0.08473[0m[0m | time: 61.566s
| Adam | epoch: 010 | loss: 0.08473 - acc: 0.9681 -- iter: 10560/22500
Training Step: 3334  | total loss: [1m[32m0.08303[0m[0m | time: 61.920s
| Adam | epoch: 010 | loss: 0.08303 - acc: 0.9698 -- iter: 10624/22500
Training Step: 3335  | total loss: [1m[32m0.08128[0m[0m | time: 62.276s
| Adam | epoch: 010 | loss: 0.08128 - acc: 0.9712 -- iter: 10688/22500
Training Step: 3336  | total loss: [1m[32m0.07763[0m[0m | time: 62.629s
| Adam | epoch: 010 | loss: 0.07763 - acc: 0.9725 -- iter: 10752/22500
Training Step: 3337  | total loss: [1m[32m0.07364[0m[0m | time: 62.982s
| Adam | epoch: 010 | loss: 0.07364 - acc: 0.9737 -- iter: 10816/22500
Training Step: 3338  | total loss: [1m[32m0.06920[0m[0m | time: 63.344s
| Adam | epoch: 010 | loss: 0.06920 - acc: 0.9764 -- iter: 10880/22500
Training Step: 3339  | total loss: [1m[32m0.06863[0m[0m | time: 63.700s
| Adam | epoch: 010 | loss: 0.06863 - acc: 0.9756 -- iter: 10944/22500
Training Step: 3340  | total loss: [1m[32m0.07140[0m[0m | time: 64.066s
| Adam | epoch: 010 | loss: 0.07140 - acc: 0.9733 -- iter: 11008/22500
Training Step: 3341  | total loss: [1m[32m0.07249[0m[0m | time: 64.477s
| Adam | epoch: 010 | loss: 0.07249 - acc: 0.9713 -- iter: 11072/22500
Training Step: 3342  | total loss: [1m[32m0.07586[0m[0m | time: 64.768s
| Adam | epoch: 010 | loss: 0.07586 - acc: 0.9726 -- iter: 11136/22500
Training Step: 3343  | total loss: [1m[32m0.07744[0m[0m | time: 65.054s
| Adam | epoch: 010 | loss: 0.07744 - acc: 0.9691 -- iter: 11200/22500
Training Step: 3344  | total loss: [1m[32m0.07607[0m[0m | time: 65.342s
| Adam | epoch: 010 | loss: 0.07607 - acc: 0.9706 -- iter: 11264/22500
Training Step: 3345  | total loss: [1m[32m0.07214[0m[0m | time: 65.629s
| Adam | epoch: 010 | loss: 0.07214 - acc: 0.9720 -- iter: 11328/22500
Training Step: 3346  | total loss: [1m[32m0.08078[0m[0m | time: 65.915s
| Adam | epoch: 010 | loss: 0.08078 - acc: 0.9717 -- iter: 11392/22500
Training Step: 3347  | total loss: [1m[32m0.07746[0m[0m | time: 66.281s
| Adam | epoch: 010 | loss: 0.07746 - acc: 0.9730 -- iter: 11456/22500
Training Step: 3348  | total loss: [1m[32m0.07298[0m[0m | time: 66.632s
| Adam | epoch: 010 | loss: 0.07298 - acc: 0.9757 -- iter: 11520/22500
Training Step: 3349  | total loss: [1m[32m0.07680[0m[0m | time: 66.985s
| Adam | epoch: 010 | loss: 0.07680 - acc: 0.9750 -- iter: 11584/22500
Training Step: 3350  | total loss: [1m[32m0.07657[0m[0m | time: 67.340s
| Adam | epoch: 010 | loss: 0.07657 - acc: 0.9743 -- iter: 11648/22500
Training Step: 3351  | total loss: [1m[32m0.07801[0m[0m | time: 67.709s
| Adam | epoch: 010 | loss: 0.07801 - acc: 0.9754 -- iter: 11712/22500
Training Step: 3352  | total loss: [1m[32m0.07606[0m[0m | time: 68.078s
| Adam | epoch: 010 | loss: 0.07606 - acc: 0.9747 -- iter: 11776/22500
Training Step: 3353  | total loss: [1m[32m0.08031[0m[0m | time: 68.433s
| Adam | epoch: 010 | loss: 0.08031 - acc: 0.9741 -- iter: 11840/22500
Training Step: 3354  | total loss: [1m[32m0.08091[0m[0m | time: 68.786s
| Adam | epoch: 010 | loss: 0.08091 - acc: 0.9720 -- iter: 11904/22500
Training Step: 3355  | total loss: [1m[32m0.07596[0m[0m | time: 69.137s
| Adam | epoch: 010 | loss: 0.07596 - acc: 0.9732 -- iter: 11968/22500
Training Step: 3356  | total loss: [1m[32m0.07654[0m[0m | time: 69.488s
| Adam | epoch: 010 | loss: 0.07654 - acc: 0.9728 -- iter: 12032/22500
Training Step: 3357  | total loss: [1m[32m0.07423[0m[0m | time: 69.841s
| Adam | epoch: 010 | loss: 0.07423 - acc: 0.9708 -- iter: 12096/22500
Training Step: 3358  | total loss: [1m[32m0.07405[0m[0m | time: 70.194s
| Adam | epoch: 010 | loss: 0.07405 - acc: 0.9706 -- iter: 12160/22500
Training Step: 3359  | total loss: [1m[32m0.08186[0m[0m | time: 70.544s
| Adam | epoch: 010 | loss: 0.08186 - acc: 0.9689 -- iter: 12224/22500
Training Step: 3360  | total loss: [1m[32m0.09161[0m[0m | time: 70.938s
| Adam | epoch: 010 | loss: 0.09161 - acc: 0.9673 -- iter: 12288/22500
Training Step: 3361  | total loss: [1m[32m0.09098[0m[0m | time: 71.359s
| Adam | epoch: 010 | loss: 0.09098 - acc: 0.9690 -- iter: 12352/22500
Training Step: 3362  | total loss: [1m[32m0.08769[0m[0m | time: 71.720s
| Adam | epoch: 010 | loss: 0.08769 - acc: 0.9721 -- iter: 12416/22500
Training Step: 3363  | total loss: [1m[32m0.08528[0m[0m | time: 72.090s
| Adam | epoch: 010 | loss: 0.08528 - acc: 0.9718 -- iter: 12480/22500
Training Step: 3364  | total loss: [1m[32m0.08726[0m[0m | time: 72.480s
| Adam | epoch: 010 | loss: 0.08726 - acc: 0.9699 -- iter: 12544/22500
Training Step: 3365  | total loss: [1m[32m0.08467[0m[0m | time: 72.759s
| Adam | epoch: 010 | loss: 0.08467 - acc: 0.9713 -- iter: 12608/22500
Training Step: 3366  | total loss: [1m[32m0.09333[0m[0m | time: 73.041s
| Adam | epoch: 010 | loss: 0.09333 - acc: 0.9695 -- iter: 12672/22500
Training Step: 3367  | total loss: [1m[32m0.09122[0m[0m | time: 73.326s
| Adam | epoch: 010 | loss: 0.09122 - acc: 0.9694 -- iter: 12736/22500
Training Step: 3368  | total loss: [1m[32m0.08649[0m[0m | time: 73.611s
| Adam | epoch: 010 | loss: 0.08649 - acc: 0.9709 -- iter: 12800/22500
Training Step: 3369  | total loss: [1m[32m0.08087[0m[0m | time: 73.974s
| Adam | epoch: 010 | loss: 0.08087 - acc: 0.9738 -- iter: 12864/22500
Training Step: 3370  | total loss: [1m[32m0.08506[0m[0m | time: 74.324s
| Adam | epoch: 010 | loss: 0.08506 - acc: 0.9718 -- iter: 12928/22500
Training Step: 3371  | total loss: [1m[32m0.07838[0m[0m | time: 74.675s
| Adam | epoch: 010 | loss: 0.07838 - acc: 0.9746 -- iter: 12992/22500
Training Step: 3372  | total loss: [1m[32m0.07817[0m[0m | time: 75.027s
| Adam | epoch: 010 | loss: 0.07817 - acc: 0.9756 -- iter: 13056/22500
Training Step: 3373  | total loss: [1m[32m0.09290[0m[0m | time: 75.422s
| Adam | epoch: 010 | loss: 0.09290 - acc: 0.9702 -- iter: 13120/22500
Training Step: 3374  | total loss: [1m[32m0.09854[0m[0m | time: 75.817s
| Adam | epoch: 010 | loss: 0.09854 - acc: 0.9669 -- iter: 13184/22500
Training Step: 3375  | total loss: [1m[32m0.09096[0m[0m | time: 76.207s
| Adam | epoch: 010 | loss: 0.09096 - acc: 0.9702 -- iter: 13248/22500
Training Step: 3376  | total loss: [1m[32m0.09359[0m[0m | time: 76.563s
| Adam | epoch: 010 | loss: 0.09359 - acc: 0.9701 -- iter: 13312/22500
Training Step: 3377  | total loss: [1m[32m0.08907[0m[0m | time: 76.915s
| Adam | epoch: 010 | loss: 0.08907 - acc: 0.9715 -- iter: 13376/22500
Training Step: 3378  | total loss: [1m[32m0.08581[0m[0m | time: 77.266s
| Adam | epoch: 010 | loss: 0.08581 - acc: 0.9712 -- iter: 13440/22500
Training Step: 3379  | total loss: [1m[32m0.08136[0m[0m | time: 77.628s
| Adam | epoch: 010 | loss: 0.08136 - acc: 0.9726 -- iter: 13504/22500
Training Step: 3380  | total loss: [1m[32m0.08937[0m[0m | time: 77.978s
| Adam | epoch: 010 | loss: 0.08937 - acc: 0.9706 -- iter: 13568/22500
Training Step: 3381  | total loss: [1m[32m0.08446[0m[0m | time: 78.332s
| Adam | epoch: 010 | loss: 0.08446 - acc: 0.9736 -- iter: 13632/22500
Training Step: 3382  | total loss: [1m[32m0.08638[0m[0m | time: 78.693s
| Adam | epoch: 010 | loss: 0.08638 - acc: 0.9746 -- iter: 13696/22500
Training Step: 3383  | total loss: [1m[32m0.08889[0m[0m | time: 79.044s
| Adam | epoch: 010 | loss: 0.08889 - acc: 0.9740 -- iter: 13760/22500
Training Step: 3384  | total loss: [1m[32m0.08637[0m[0m | time: 79.395s
| Adam | epoch: 010 | loss: 0.08637 - acc: 0.9720 -- iter: 13824/22500
Training Step: 3385  | total loss: [1m[32m0.08902[0m[0m | time: 79.760s
| Adam | epoch: 010 | loss: 0.08902 - acc: 0.9716 -- iter: 13888/22500
Training Step: 3386  | total loss: [1m[32m0.10082[0m[0m | time: 80.190s
| Adam | epoch: 010 | loss: 0.10082 - acc: 0.9698 -- iter: 13952/22500
Training Step: 3387  | total loss: [1m[32m0.09587[0m[0m | time: 80.650s
| Adam | epoch: 010 | loss: 0.09587 - acc: 0.9697 -- iter: 14016/22500
Training Step: 3388  | total loss: [1m[32m0.09341[0m[0m | time: 81.031s
| Adam | epoch: 010 | loss: 0.09341 - acc: 0.9711 -- iter: 14080/22500
Training Step: 3389  | total loss: [1m[32m0.09197[0m[0m | time: 81.549s
| Adam | epoch: 010 | loss: 0.09197 - acc: 0.9709 -- iter: 14144/22500
Training Step: 3390  | total loss: [1m[32m0.08625[0m[0m | time: 81.935s
| Adam | epoch: 010 | loss: 0.08625 - acc: 0.9738 -- iter: 14208/22500
Training Step: 3391  | total loss: [1m[32m0.08192[0m[0m | time: 82.352s
| Adam | epoch: 010 | loss: 0.08192 - acc: 0.9749 -- iter: 14272/22500
Training Step: 3392  | total loss: [1m[32m0.08514[0m[0m | time: 82.744s
| Adam | epoch: 010 | loss: 0.08514 - acc: 0.9743 -- iter: 14336/22500
Training Step: 3393  | total loss: [1m[32m0.09795[0m[0m | time: 83.135s
| Adam | epoch: 010 | loss: 0.09795 - acc: 0.9706 -- iter: 14400/22500
Training Step: 3394  | total loss: [1m[32m0.10202[0m[0m | time: 83.530s
| Adam | epoch: 010 | loss: 0.10202 - acc: 0.9673 -- iter: 14464/22500
Training Step: 3395  | total loss: [1m[32m0.10431[0m[0m | time: 83.904s
| Adam | epoch: 010 | loss: 0.10431 - acc: 0.9643 -- iter: 14528/22500
Training Step: 3396  | total loss: [1m[32m0.10399[0m[0m | time: 84.244s
| Adam | epoch: 010 | loss: 0.10399 - acc: 0.9647 -- iter: 14592/22500
Training Step: 3397  | total loss: [1m[32m0.10494[0m[0m | time: 84.670s
| Adam | epoch: 010 | loss: 0.10494 - acc: 0.9651 -- iter: 14656/22500
Training Step: 3398  | total loss: [1m[32m0.09813[0m[0m | time: 85.176s
| Adam | epoch: 010 | loss: 0.09813 - acc: 0.9686 -- iter: 14720/22500
Training Step: 3399  | total loss: [1m[32m0.09280[0m[0m | time: 85.574s
| Adam | epoch: 010 | loss: 0.09280 - acc: 0.9702 -- iter: 14784/22500
Training Step: 3400  | total loss: [1m[32m0.08819[0m[0m | time: 89.943s
| Adam | epoch: 010 | loss: 0.08819 - acc: 0.9732 | val_loss: 0.73790 - val_acc: 0.7888 -- iter: 14848/22500
--
Training Step: 3401  | total loss: [1m[32m0.09049[0m[0m | time: 90.399s
| Adam | epoch: 010 | loss: 0.09049 - acc: 0.9743 -- iter: 14912/22500
Training Step: 3402  | total loss: [1m[32m0.09381[0m[0m | time: 90.877s
| Adam | epoch: 010 | loss: 0.09381 - acc: 0.9737 -- iter: 14976/22500
Training Step: 3403  | total loss: [1m[32m0.09356[0m[0m | time: 91.429s
| Adam | epoch: 010 | loss: 0.09356 - acc: 0.9732 -- iter: 15040/22500
Training Step: 3404  | total loss: [1m[32m0.09743[0m[0m | time: 91.815s
| Adam | epoch: 010 | loss: 0.09743 - acc: 0.9728 -- iter: 15104/22500
Training Step: 3405  | total loss: [1m[32m0.09881[0m[0m | time: 92.184s
| Adam | epoch: 010 | loss: 0.09881 - acc: 0.9740 -- iter: 15168/22500
Training Step: 3406  | total loss: [1m[32m0.10220[0m[0m | time: 92.586s
| Adam | epoch: 010 | loss: 0.10220 - acc: 0.9703 -- iter: 15232/22500
Training Step: 3407  | total loss: [1m[32m0.10015[0m[0m | time: 92.953s
| Adam | epoch: 010 | loss: 0.10015 - acc: 0.9686 -- iter: 15296/22500
Training Step: 3408  | total loss: [1m[32m0.10021[0m[0m | time: 93.343s
| Adam | epoch: 010 | loss: 0.10021 - acc: 0.9686 -- iter: 15360/22500
Training Step: 3409  | total loss: [1m[32m0.09925[0m[0m | time: 93.753s
| Adam | epoch: 010 | loss: 0.09925 - acc: 0.9686 -- iter: 15424/22500
Training Step: 3410  | total loss: [1m[32m0.10583[0m[0m | time: 94.194s
| Adam | epoch: 010 | loss: 0.10583 - acc: 0.9686 -- iter: 15488/22500
Training Step: 3411  | total loss: [1m[32m0.10633[0m[0m | time: 94.618s
| Adam | epoch: 010 | loss: 0.10633 - acc: 0.9671 -- iter: 15552/22500
Training Step: 3412  | total loss: [1m[32m0.10314[0m[0m | time: 95.003s
| Adam | epoch: 010 | loss: 0.10314 - acc: 0.9688 -- iter: 15616/22500
Training Step: 3413  | total loss: [1m[32m0.09809[0m[0m | time: 95.343s
| Adam | epoch: 010 | loss: 0.09809 - acc: 0.9704 -- iter: 15680/22500
Training Step: 3414  | total loss: [1m[32m0.09248[0m[0m | time: 95.683s
| Adam | epoch: 010 | loss: 0.09248 - acc: 0.9702 -- iter: 15744/22500
Training Step: 3415  | total loss: [1m[32m0.09884[0m[0m | time: 96.154s
| Adam | epoch: 010 | loss: 0.09884 - acc: 0.9669 -- iter: 15808/22500
Training Step: 3416  | total loss: [1m[32m0.09566[0m[0m | time: 96.563s
| Adam | epoch: 010 | loss: 0.09566 - acc: 0.9671 -- iter: 15872/22500
Training Step: 3417  | total loss: [1m[32m0.09984[0m[0m | time: 97.027s
| Adam | epoch: 010 | loss: 0.09984 - acc: 0.9657 -- iter: 15936/22500
Training Step: 3418  | total loss: [1m[32m0.11269[0m[0m | time: 97.448s
| Adam | epoch: 010 | loss: 0.11269 - acc: 0.9613 -- iter: 16000/22500
Training Step: 3419  | total loss: [1m[32m0.11719[0m[0m | time: 97.833s
| Adam | epoch: 010 | loss: 0.11719 - acc: 0.9605 -- iter: 16064/22500
Training Step: 3420  | total loss: [1m[32m0.11720[0m[0m | time: 98.288s
| Adam | epoch: 010 | loss: 0.11720 - acc: 0.9613 -- iter: 16128/22500
Training Step: 3421  | total loss: [1m[32m0.12045[0m[0m | time: 98.707s
| Adam | epoch: 010 | loss: 0.12045 - acc: 0.9590 -- iter: 16192/22500
Training Step: 3422  | total loss: [1m[32m0.12437[0m[0m | time: 99.090s
| Adam | epoch: 010 | loss: 0.12437 - acc: 0.9584 -- iter: 16256/22500
Training Step: 3423  | total loss: [1m[32m0.13574[0m[0m | time: 99.446s
| Adam | epoch: 010 | loss: 0.13574 - acc: 0.9532 -- iter: 16320/22500
Training Step: 3424  | total loss: [1m[32m0.13513[0m[0m | time: 99.740s
| Adam | epoch: 010 | loss: 0.13513 - acc: 0.9547 -- iter: 16384/22500
Training Step: 3425  | total loss: [1m[32m0.12329[0m[0m | time: 100.024s
| Adam | epoch: 010 | loss: 0.12329 - acc: 0.9592 -- iter: 16448/22500
Training Step: 3426  | total loss: [1m[32m0.12237[0m[0m | time: 100.406s
| Adam | epoch: 010 | loss: 0.12237 - acc: 0.9602 -- iter: 16512/22500
Training Step: 3427  | total loss: [1m[32m0.14955[0m[0m | time: 100.753s
| Adam | epoch: 010 | loss: 0.14955 - acc: 0.9532 -- iter: 16576/22500
Training Step: 3428  | total loss: [1m[32m0.13886[0m[0m | time: 101.098s
| Adam | epoch: 010 | loss: 0.13886 - acc: 0.9564 -- iter: 16640/22500
Training Step: 3429  | total loss: [1m[32m0.13223[0m[0m | time: 101.450s
| Adam | epoch: 010 | loss: 0.13223 - acc: 0.9576 -- iter: 16704/22500
Training Step: 3430  | total loss: [1m[32m0.12295[0m[0m | time: 101.802s
| Adam | epoch: 010 | loss: 0.12295 - acc: 0.9603 -- iter: 16768/22500
Training Step: 3431  | total loss: [1m[32m0.12110[0m[0m | time: 102.168s
| Adam | epoch: 010 | loss: 0.12110 - acc: 0.9611 -- iter: 16832/22500
Training Step: 3432  | total loss: [1m[32m0.11153[0m[0m | time: 102.535s
| Adam | epoch: 010 | loss: 0.11153 - acc: 0.9650 -- iter: 16896/22500
Training Step: 3433  | total loss: [1m[32m0.11266[0m[0m | time: 102.990s
| Adam | epoch: 010 | loss: 0.11266 - acc: 0.9638 -- iter: 16960/22500
Training Step: 3434  | total loss: [1m[32m0.11602[0m[0m | time: 103.486s
| Adam | epoch: 010 | loss: 0.11602 - acc: 0.9627 -- iter: 17024/22500
Training Step: 3435  | total loss: [1m[32m0.11699[0m[0m | time: 103.884s
| Adam | epoch: 010 | loss: 0.11699 - acc: 0.9633 -- iter: 17088/22500
Training Step: 3436  | total loss: [1m[32m0.10795[0m[0m | time: 104.309s
| Adam | epoch: 010 | loss: 0.10795 - acc: 0.9670 -- iter: 17152/22500
Training Step: 3437  | total loss: [1m[32m0.11771[0m[0m | time: 104.733s
| Adam | epoch: 010 | loss: 0.11771 - acc: 0.9641 -- iter: 17216/22500
Training Step: 3438  | total loss: [1m[32m0.11751[0m[0m | time: 105.155s
| Adam | epoch: 010 | loss: 0.11751 - acc: 0.9645 -- iter: 17280/22500
Training Step: 3439  | total loss: [1m[32m0.12345[0m[0m | time: 105.627s
| Adam | epoch: 010 | loss: 0.12345 - acc: 0.9603 -- iter: 17344/22500
Training Step: 3440  | total loss: [1m[32m0.11877[0m[0m | time: 105.997s
| Adam | epoch: 010 | loss: 0.11877 - acc: 0.9627 -- iter: 17408/22500
Training Step: 3441  | total loss: [1m[32m0.11160[0m[0m | time: 106.356s
| Adam | epoch: 010 | loss: 0.11160 - acc: 0.9648 -- iter: 17472/22500
Training Step: 3442  | total loss: [1m[32m0.10785[0m[0m | time: 106.655s
| Adam | epoch: 010 | loss: 0.10785 - acc: 0.9668 -- iter: 17536/22500
Training Step: 3443  | total loss: [1m[32m0.10223[0m[0m | time: 106.959s
| Adam | epoch: 010 | loss: 0.10223 - acc: 0.9686 -- iter: 17600/22500
Training Step: 3444  | total loss: [1m[32m0.09567[0m[0m | time: 107.246s
| Adam | epoch: 010 | loss: 0.09567 - acc: 0.9717 -- iter: 17664/22500
Training Step: 3445  | total loss: [1m[32m0.09790[0m[0m | time: 107.532s
| Adam | epoch: 010 | loss: 0.09790 - acc: 0.9714 -- iter: 17728/22500
Training Step: 3446  | total loss: [1m[32m0.09503[0m[0m | time: 107.864s
| Adam | epoch: 010 | loss: 0.09503 - acc: 0.9727 -- iter: 17792/22500
Training Step: 3447  | total loss: [1m[32m0.09491[0m[0m | time: 108.219s
| Adam | epoch: 010 | loss: 0.09491 - acc: 0.9707 -- iter: 17856/22500
Training Step: 3448  | total loss: [1m[32m0.09734[0m[0m | time: 108.570s
| Adam | epoch: 010 | loss: 0.09734 - acc: 0.9690 -- iter: 17920/22500
Training Step: 3449  | total loss: [1m[32m0.10853[0m[0m | time: 108.918s
| Adam | epoch: 010 | loss: 0.10853 - acc: 0.9658 -- iter: 17984/22500
Training Step: 3450  | total loss: [1m[32m0.11477[0m[0m | time: 109.268s
| Adam | epoch: 010 | loss: 0.11477 - acc: 0.9630 -- iter: 18048/22500
Training Step: 3451  | total loss: [1m[32m0.12006[0m[0m | time: 109.597s
| Adam | epoch: 010 | loss: 0.12006 - acc: 0.9573 -- iter: 18112/22500
Training Step: 3452  | total loss: [1m[32m0.11104[0m[0m | time: 109.880s
| Adam | epoch: 010 | loss: 0.11104 - acc: 0.9616 -- iter: 18176/22500
Training Step: 3453  | total loss: [1m[32m0.10752[0m[0m | time: 110.165s
| Adam | epoch: 010 | loss: 0.10752 - acc: 0.9623 -- iter: 18240/22500
Training Step: 3454  | total loss: [1m[32m0.10084[0m[0m | time: 110.463s
| Adam | epoch: 010 | loss: 0.10084 - acc: 0.9645 -- iter: 18304/22500
Training Step: 3455  | total loss: [1m[32m0.11154[0m[0m | time: 110.769s
| Adam | epoch: 010 | loss: 0.11154 - acc: 0.9603 -- iter: 18368/22500
Training Step: 3456  | total loss: [1m[32m0.10610[0m[0m | time: 111.138s
| Adam | epoch: 010 | loss: 0.10610 - acc: 0.9642 -- iter: 18432/22500
Training Step: 3457  | total loss: [1m[32m0.11182[0m[0m | time: 111.494s
| Adam | epoch: 010 | loss: 0.11182 - acc: 0.9631 -- iter: 18496/22500
Training Step: 3458  | total loss: [1m[32m0.11228[0m[0m | time: 111.845s
| Adam | epoch: 010 | loss: 0.11228 - acc: 0.9621 -- iter: 18560/22500
Training Step: 3459  | total loss: [1m[32m0.11015[0m[0m | time: 112.196s
| Adam | epoch: 010 | loss: 0.11015 - acc: 0.9628 -- iter: 18624/22500
Training Step: 3460  | total loss: [1m[32m0.10985[0m[0m | time: 112.567s
| Adam | epoch: 010 | loss: 0.10985 - acc: 0.9649 -- iter: 18688/22500
Training Step: 3461  | total loss: [1m[32m0.10848[0m[0m | time: 112.917s
| Adam | epoch: 010 | loss: 0.10848 - acc: 0.9638 -- iter: 18752/22500
Training Step: 3462  | total loss: [1m[32m0.10395[0m[0m | time: 113.286s
| Adam | epoch: 010 | loss: 0.10395 - acc: 0.9658 -- iter: 18816/22500
Training Step: 3463  | total loss: [1m[32m0.10283[0m[0m | time: 113.638s
| Adam | epoch: 010 | loss: 0.10283 - acc: 0.9646 -- iter: 18880/22500
Training Step: 3464  | total loss: [1m[32m0.11427[0m[0m | time: 113.992s
| Adam | epoch: 010 | loss: 0.11427 - acc: 0.9634 -- iter: 18944/22500
Training Step: 3465  | total loss: [1m[32m0.10901[0m[0m | time: 114.363s
| Adam | epoch: 010 | loss: 0.10901 - acc: 0.9655 -- iter: 19008/22500
Training Step: 3466  | total loss: [1m[32m0.10235[0m[0m | time: 114.746s
| Adam | epoch: 010 | loss: 0.10235 - acc: 0.9674 -- iter: 19072/22500
Training Step: 3467  | total loss: [1m[32m0.10023[0m[0m | time: 115.097s
| Adam | epoch: 010 | loss: 0.10023 - acc: 0.9675 -- iter: 19136/22500
Training Step: 3468  | total loss: [1m[32m0.09647[0m[0m | time: 115.451s
| Adam | epoch: 010 | loss: 0.09647 - acc: 0.9692 -- iter: 19200/22500
Training Step: 3469  | total loss: [1m[32m0.09907[0m[0m | time: 115.781s
| Adam | epoch: 010 | loss: 0.09907 - acc: 0.9676 -- iter: 19264/22500
Training Step: 3470  | total loss: [1m[32m0.09959[0m[0m | time: 116.064s
| Adam | epoch: 010 | loss: 0.09959 - acc: 0.9677 -- iter: 19328/22500
Training Step: 3471  | total loss: [1m[32m0.11303[0m[0m | time: 116.346s
| Adam | epoch: 010 | loss: 0.11303 - acc: 0.9647 -- iter: 19392/22500
Training Step: 3472  | total loss: [1m[32m0.10815[0m[0m | time: 116.643s
| Adam | epoch: 010 | loss: 0.10815 - acc: 0.9651 -- iter: 19456/22500
Training Step: 3473  | total loss: [1m[32m0.10158[0m[0m | time: 116.928s
| Adam | epoch: 010 | loss: 0.10158 - acc: 0.9670 -- iter: 19520/22500
Training Step: 3474  | total loss: [1m[32m0.09839[0m[0m | time: 117.285s
| Adam | epoch: 010 | loss: 0.09839 - acc: 0.9688 -- iter: 19584/22500
Training Step: 3475  | total loss: [1m[32m0.09482[0m[0m | time: 117.639s
| Adam | epoch: 010 | loss: 0.09482 - acc: 0.9703 -- iter: 19648/22500
Training Step: 3476  | total loss: [1m[32m0.09471[0m[0m | time: 118.005s
| Adam | epoch: 010 | loss: 0.09471 - acc: 0.9717 -- iter: 19712/22500
Training Step: 3477  | total loss: [1m[32m0.10063[0m[0m | time: 118.374s
| Adam | epoch: 010 | loss: 0.10063 - acc: 0.9699 -- iter: 19776/22500
Training Step: 3478  | total loss: [1m[32m0.10653[0m[0m | time: 118.729s
| Adam | epoch: 010 | loss: 0.10653 - acc: 0.9698 -- iter: 19840/22500
Training Step: 3479  | total loss: [1m[32m0.10166[0m[0m | time: 119.080s
| Adam | epoch: 010 | loss: 0.10166 - acc: 0.9712 -- iter: 19904/22500
Training Step: 3480  | total loss: [1m[32m0.09625[0m[0m | time: 119.432s
| Adam | epoch: 010 | loss: 0.09625 - acc: 0.9725 -- iter: 19968/22500
Training Step: 3481  | total loss: [1m[32m0.09500[0m[0m | time: 119.787s
| Adam | epoch: 010 | loss: 0.09500 - acc: 0.9706 -- iter: 20032/22500
Training Step: 3482  | total loss: [1m[32m0.10181[0m[0m | time: 120.171s
| Adam | epoch: 010 | loss: 0.10181 - acc: 0.9688 -- iter: 20096/22500
Training Step: 3483  | total loss: [1m[32m0.11030[0m[0m | time: 120.457s
| Adam | epoch: 010 | loss: 0.11030 - acc: 0.9688 -- iter: 20160/22500
Training Step: 3484  | total loss: [1m[32m0.10863[0m[0m | time: 120.741s
| Adam | epoch: 010 | loss: 0.10863 - acc: 0.9688 -- iter: 20224/22500
Training Step: 3485  | total loss: [1m[32m0.10402[0m[0m | time: 121.024s
| Adam | epoch: 010 | loss: 0.10402 - acc: 0.9688 -- iter: 20288/22500
Training Step: 3486  | total loss: [1m[32m0.09946[0m[0m | time: 121.310s
| Adam | epoch: 010 | loss: 0.09946 - acc: 0.9688 -- iter: 20352/22500
Training Step: 3487  | total loss: [1m[32m0.09283[0m[0m | time: 121.605s
| Adam | epoch: 010 | loss: 0.09283 - acc: 0.9719 -- iter: 20416/22500
Training Step: 3488  | total loss: [1m[32m0.09069[0m[0m | time: 121.909s
| Adam | epoch: 010 | loss: 0.09069 - acc: 0.9716 -- iter: 20480/22500
Training Step: 3489  | total loss: [1m[32m0.08941[0m[0m | time: 122.193s
| Adam | epoch: 010 | loss: 0.08941 - acc: 0.9698 -- iter: 20544/22500
Training Step: 3490  | total loss: [1m[32m0.08374[0m[0m | time: 122.524s
| Adam | epoch: 010 | loss: 0.08374 - acc: 0.9712 -- iter: 20608/22500
Training Step: 3491  | total loss: [1m[32m0.08272[0m[0m | time: 122.843s
| Adam | epoch: 010 | loss: 0.08272 - acc: 0.9710 -- iter: 20672/22500
Training Step: 3492  | total loss: [1m[32m0.08938[0m[0m | time: 123.156s
| Adam | epoch: 010 | loss: 0.08938 - acc: 0.9692 -- iter: 20736/22500
Training Step: 3493  | total loss: [1m[32m0.08791[0m[0m | time: 123.591s
| Adam | epoch: 010 | loss: 0.08791 - acc: 0.9676 -- iter: 20800/22500
Training Step: 3494  | total loss: [1m[32m0.08183[0m[0m | time: 124.028s
| Adam | epoch: 010 | loss: 0.08183 - acc: 0.9708 -- iter: 20864/22500
Training Step: 3495  | total loss: [1m[32m0.08930[0m[0m | time: 124.439s
| Adam | epoch: 010 | loss: 0.08930 - acc: 0.9706 -- iter: 20928/22500
Training Step: 3496  | total loss: [1m[32m0.08427[0m[0m | time: 124.842s
| Adam | epoch: 010 | loss: 0.08427 - acc: 0.9720 -- iter: 20992/22500
Training Step: 3497  | total loss: [1m[32m0.08207[0m[0m | time: 125.301s
| Adam | epoch: 010 | loss: 0.08207 - acc: 0.9717 -- iter: 21056/22500
Training Step: 3498  | total loss: [1m[32m0.33627[0m[0m | time: 125.744s
| Adam | epoch: 010 | loss: 0.33627 - acc: 0.9198 -- iter: 21120/22500
Training Step: 3499  | total loss: [1m[32m0.31095[0m[0m | time: 126.174s
| Adam | epoch: 010 | loss: 0.31095 - acc: 0.9231 -- iter: 21184/22500
Training Step: 3500  | total loss: [1m[32m0.29171[0m[0m | time: 126.551s
| Adam | epoch: 010 | loss: 0.29171 - acc: 0.9246 -- iter: 21248/22500
Training Step: 3501  | total loss: [1m[32m0.27425[0m[0m | time: 126.844s
| Adam | epoch: 010 | loss: 0.27425 - acc: 0.9290 -- iter: 21312/22500
Training Step: 3502  | total loss: [1m[32m0.26010[0m[0m | time: 127.155s
| Adam | epoch: 010 | loss: 0.26010 - acc: 0.9330 -- iter: 21376/22500
Training Step: 3503  | total loss: [1m[32m0.24260[0m[0m | time: 127.466s
| Adam | epoch: 010 | loss: 0.24260 - acc: 0.9381 -- iter: 21440/22500
Training Step: 3504  | total loss: [1m[32m0.23109[0m[0m | time: 127.763s
| Adam | epoch: 010 | loss: 0.23109 - acc: 0.9412 -- iter: 21504/22500
Training Step: 3505  | total loss: [1m[32m0.21941[0m[0m | time: 128.074s
| Adam | epoch: 010 | loss: 0.21941 - acc: 0.9455 -- iter: 21568/22500
Training Step: 3506  | total loss: [1m[32m0.21029[0m[0m | time: 128.370s
| Adam | epoch: 010 | loss: 0.21029 - acc: 0.9478 -- iter: 21632/22500
Training Step: 3507  | total loss: [1m[32m0.20515[0m[0m | time: 128.655s
| Adam | epoch: 010 | loss: 0.20515 - acc: 0.9499 -- iter: 21696/22500
Training Step: 3508  | total loss: [1m[32m0.19035[0m[0m | time: 128.949s
| Adam | epoch: 010 | loss: 0.19035 - acc: 0.9549 -- iter: 21760/22500
Training Step: 3509  | total loss: [1m[32m0.17907[0m[0m | time: 129.247s
| Adam | epoch: 010 | loss: 0.17907 - acc: 0.9579 -- iter: 21824/22500
Training Step: 3510  | total loss: [1m[32m0.16555[0m[0m | time: 129.627s
| Adam | epoch: 010 | loss: 0.16555 - acc: 0.9621 -- iter: 21888/22500
Training Step: 3511  | total loss: [1m[32m0.15873[0m[0m | time: 130.012s
| Adam | epoch: 010 | loss: 0.15873 - acc: 0.9643 -- iter: 21952/22500
Training Step: 3512  | total loss: [1m[32m0.14592[0m[0m | time: 130.371s
| Adam | epoch: 010 | loss: 0.14592 - acc: 0.9679 -- iter: 22016/22500
Training Step: 3513  | total loss: [1m[32m0.14682[0m[0m | time: 130.734s
| Adam | epoch: 010 | loss: 0.14682 - acc: 0.9680 -- iter: 22080/22500
Training Step: 3514  | total loss: [1m[32m0.13621[0m[0m | time: 131.113s
| Adam | epoch: 010 | loss: 0.13621 - acc: 0.9696 -- iter: 22144/22500
Training Step: 3515  | total loss: [1m[32m0.13792[0m[0m | time: 131.535s
| Adam | epoch: 010 | loss: 0.13792 - acc: 0.9680 -- iter: 22208/22500
Training Step: 3516  | total loss: [1m[32m0.14167[0m[0m | time: 131.903s
| Adam | epoch: 010 | loss: 0.14167 - acc: 0.9634 -- iter: 22272/22500
Training Step: 3517  | total loss: [1m[32m0.13785[0m[0m | time: 132.263s
| Adam | epoch: 010 | loss: 0.13785 - acc: 0.9639 -- iter: 22336/22500
Training Step: 3518  | total loss: [1m[32m0.13272[0m[0m | time: 132.621s
| Adam | epoch: 010 | loss: 0.13272 - acc: 0.9644 -- iter: 22400/22500
Training Step: 3519  | total loss: [1m[32m0.13125[0m[0m | time: 133.000s
| Adam | epoch: 010 | loss: 0.13125 - acc: 0.9664 -- iter: 22464/22500
Training Step: 3520  | total loss: [1m[32m0.12991[0m[0m | time: 136.605s
| Adam | epoch: 010 | loss: 0.12991 - acc: 0.9651 | val_loss: 0.66163 - val_acc: 0.7908 -- iter: 22500/22500
--
Training Completed...
Model Saved...